{
  "pytest-dev__pytest-7490": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "pytest-dev__pytest-7490",
    "model_patch": ""
  },
  "django__django-11179": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-11179",
    "model_patch": "diff --git a/django/db/models/deletion.py b/django/db/models/deletion.py\nindex 92fe4affb9..9bb9e2667d 100644\n--- a/django/db/models/deletion.py\n+++ b/django/db/models/deletion.py\n@@ -271,14 +271,6 @@ class Collector:\n         # number of objects deleted for each model label\n         deleted_counter = Counter()\n \n-        # Optimize for the case with a single obj and no dependencies\n-        if len(self.data) == 1 and len(instances) == 1:\n-            instance = list(instances)[0]\n-            if self.can_fast_delete(instance):\n-                with transaction.mark_for_rollback_on_error():\n-                    count = sql.DeleteQuery(model).delete_batch([instance.pk], self.using)\n-                return count, {model._meta.label: count}\n-\n         with transaction.atomic(using=self.using, savepoint=False):\n             # send pre_delete signals\n             for model, obj in self.instances_with_model():\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..9bb9e2667d\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,319 @@\n+from collections import Counter\n+from operator import attrgetter\n+\n+from django.db import IntegrityError, connections, transaction\n+from django.db.models import signals, sql\n+\n+\n+class ProtectedError(IntegrityError):\n+    def __init__(self, msg, protected_objects):\n+        self.protected_objects = protected_objects\n+        super().__init__(msg, protected_objects)\n+\n+\n+def CASCADE(collector, field, sub_objs, using):\n+    collector.collect(sub_objs, source=field.remote_field.model,\n+                      source_attr=field.name, nullable=field.null)\n+    if field.null and not connections[using].features.can_defer_constraint_checks:\n+        collector.add_field_update(field, None, sub_objs)\n+\n+\n+def PROTECT(collector, field, sub_objs, using):\n+    raise ProtectedError(\n+        \"Cannot delete some instances of model '%s' because they are \"\n+        \"referenced through a protected foreign key: '%s.%s'\" % (\n+            field.remote_field.model.__name__, sub_objs[0].__class__.__name__, field.name\n+        ),\n+        sub_objs\n+    )\n+\n+\n+def SET(value):\n+    if callable(value):\n+        def set_on_delete(collector, field, sub_objs, using):\n+            collector.add_field_update(field, value(), sub_objs)\n+    else:\n+        def set_on_delete(collector, field, sub_objs, using):\n+            collector.add_field_update(field, value, sub_objs)\n+    set_on_delete.deconstruct = lambda: ('django.db.models.SET', (value,), {})\n+    return set_on_delete\n+\n+\n+def SET_NULL(collector, field, sub_objs, using):\n+    collector.add_field_update(field, None, sub_objs)\n+\n+\n+def SET_DEFAULT(collector, field, sub_objs, using):\n+    collector.add_field_update(field, field.get_default(), sub_objs)\n+\n+\n+def DO_NOTHING(collector, field, sub_objs, using):\n+    pass\n+\n+\n+def get_candidate_relations_to_delete(opts):\n+    # The candidate relations are the ones that come from N-1 and 1-1 relations.\n+    # N-N  (i.e., many-to-many) relations aren't candidates for deletion.\n+    return (\n+        f for f in opts.get_fields(include_hidden=True)\n+        if f.auto_created and not f.concrete and (f.one_to_one or f.one_to_many)\n+    )\n+\n+\n+class Collector:\n+    def __init__(self, using):\n+        self.using = using\n+        # Initially, {model: {instances}}, later values become lists.\n+        self.data = {}\n+        self.field_updates = {}  # {model: {(field, value): {instances}}}\n+        # fast_deletes is a list of queryset-likes that can be deleted without\n+        # fetching the objects into memory.\n+        self.fast_deletes = []\n+\n+        # Tracks deletion-order dependency for databases without transactions\n+        # or ability to defer constraint checks. Only concrete model classes\n+        # should be included, as the dependencies exist only between actual\n+        # database tables; proxy models are represented here by their concrete\n+        # parent.\n+        self.dependencies = {}  # {model: {models}}\n+\n+    def add(self, objs, source=None, nullable=False, reverse_dependency=False):\n+        \"\"\"\n+        Add 'objs' to the collection of objects to be deleted.  If the call is\n+        the result of a cascade, 'source' should be the model that caused it,\n+        and 'nullable' should be set to True if the relation can be null.\n+\n+        Return a list of all objects that were not already collected.\n+        \"\"\"\n+        if not objs:\n+            return []\n+        new_objs = []\n+        model = objs[0].__class__\n+        instances = self.data.setdefault(model, set())\n+        for obj in objs:\n+            if obj not in instances:\n+                new_objs.append(obj)\n+        instances.update(new_objs)\n+        # Nullable relationships can be ignored -- they are nulled out before\n+        # deleting, and therefore do not affect the order in which objects have\n+        # to be deleted.\n+        if source is not None and not nullable:\n+            if reverse_dependency:\n+                source, model = model, source\n+            self.dependencies.setdefault(\n+                source._meta.concrete_model, set()).add(model._meta.concrete_model)\n+        return new_objs\n+\n+    def add_field_update(self, field, value, objs):\n+        \"\"\"\n+        Schedule a field update. 'objs' must be a homogeneous iterable\n+        collection of model instances (e.g. a QuerySet).\n+        \"\"\"\n+        if not objs:\n+            return\n+        model = objs[0].__class__\n+        self.field_updates.setdefault(\n+            model, {}).setdefault(\n+            (field, value), set()).update(objs)\n+\n+    def can_fast_delete(self, objs, from_field=None):\n+        \"\"\"\n+        Determine if the objects in the given queryset-like or single object\n+        can be fast-deleted. This can be done if there are no cascades, no\n+        parents and no signal listeners for the object class.\n+\n+        The 'from_field' tells where we are coming from - we need this to\n+        determine if the objects are in fact to be deleted. Allow also\n+        skipping parent -> child -> parent chain preventing fast delete of\n+        the child.\n+        \"\"\"\n+        if from_field and from_field.remote_field.on_delete is not CASCADE:\n+            return False\n+        if hasattr(objs, '_meta'):\n+            model = type(objs)\n+        elif hasattr(objs, 'model') and hasattr(objs, '_raw_delete'):\n+            model = objs.model\n+        else:\n+            return False\n+        if (signals.pre_delete.has_listeners(model) or\n+                signals.post_delete.has_listeners(model) or\n+                signals.m2m_changed.has_listeners(model)):\n+            return False\n+        # The use of from_field comes from the need to avoid cascade back to\n+        # parent when parent delete is cascading to child.\n+        opts = model._meta\n+        return (\n+            all(link == from_field for link in opts.concrete_model._meta.parents.values()) and\n+            # Foreign keys pointing to this model.\n+            all(\n+                related.field.remote_field.on_delete is DO_NOTHING\n+                for related in get_candidate_relations_to_delete(opts)\n+            ) and (\n+                # Something like generic foreign key.\n+                not any(hasattr(field, 'bulk_related_objects') for field in opts.private_fields)\n+            )\n+        )\n+\n+    def get_del_batches(self, objs, field):\n+        \"\"\"\n+        Return the objs in suitably sized batches for the used connection.\n+        \"\"\"\n+        conn_batch_size = max(\n+            connections[self.using].ops.bulk_batch_size([field.name], objs), 1)\n+        if len(objs) > conn_batch_size:\n+            return [objs[i:i + conn_batch_size]\n+                    for i in range(0, len(objs), conn_batch_size)]\n+        else:\n+            return [objs]\n+\n+    def collect(self, objs, source=None, nullable=False, collect_related=True,\n+                source_attr=None, reverse_dependency=False, keep_parents=False):\n+        \"\"\"\n+        Add 'objs' to the collection of objects to be deleted as well as all\n+        parent instances.  'objs' must be a homogeneous iterable collection of\n+        model instances (e.g. a QuerySet).  If 'collect_related' is True,\n+        related objects will be handled by their respective on_delete handler.\n+\n+        If the call is the result of a cascade, 'source' should be the model\n+        that caused it and 'nullable' should be set to True, if the relation\n+        can be null.\n+\n+        If 'reverse_dependency' is True, 'source' will be deleted before the\n+        current model, rather than after. (Needed for cascading to parent\n+        models, the one case in which the cascade follows the forwards\n+        direction of an FK rather than the reverse direction.)\n+\n+        If 'keep_parents' is True, data of parent model's will be not deleted.\n+        \"\"\"\n+        if self.can_fast_delete(objs):\n+            self.fast_deletes.append(objs)\n+            return\n+        new_objs = self.add(objs, source, nullable,\n+                            reverse_dependency=reverse_dependency)\n+        if not new_objs:\n+            return\n+\n+        model = new_objs[0].__class__\n+\n+        if not keep_parents:\n+            # Recursively collect concrete model's parent models, but not their\n+            # related objects. These will be found by meta.get_fields()\n+            concrete_model = model._meta.concrete_model\n+            for ptr in concrete_model._meta.parents.values():\n+                if ptr:\n+                    parent_objs = [getattr(obj, ptr.name) for obj in new_objs]\n+                    self.collect(parent_objs, source=model,\n+                                 source_attr=ptr.remote_field.related_name,\n+                                 collect_related=False,\n+                                 reverse_dependency=True)\n+        if collect_related:\n+            parents = model._meta.parents\n+            for related in get_candidate_relations_to_delete(model._meta):\n+                # Preserve parent reverse relationships if keep_parents=True.\n+                if keep_parents and related.model in parents:\n+                    continue\n+                field = related.field\n+                if field.remote_field.on_delete == DO_NOTHING:\n+                    continue\n+                batches = self.get_del_batches(new_objs, field)\n+                for batch in batches:\n+                    sub_objs = self.related_objects(related, batch)\n+                    if self.can_fast_delete(sub_objs, from_field=field):\n+                        self.fast_deletes.append(sub_objs)\n+                    elif sub_objs:\n+                        field.remote_field.on_delete(self, field, sub_objs, self.using)\n+            for field in model._meta.private_fields:\n+                if hasattr(field, 'bulk_related_objects'):\n+                    # It's something like generic foreign key.\n+                    sub_objs = field.bulk_related_objects(new_objs, self.using)\n+                    self.collect(sub_objs, source=model, nullable=True)\n+\n+    def related_objects(self, related, objs):\n+        \"\"\"\n+        Get a QuerySet of objects related to `objs` via the relation `related`.\n+        \"\"\"\n+        return related.related_model._base_manager.using(self.using).filter(\n+            **{\"%s__in\" % related.field.name: objs}\n+        )\n+\n+    def instances_with_model(self):\n+        for model, instances in self.data.items():\n+            for obj in instances:\n+                yield model, obj\n+\n+    def sort(self):\n+        sorted_models = []\n+        concrete_models = set()\n+        models = list(self.data)\n+        while len(sorted_models) < len(models):\n+            found = False\n+            for model in models:\n+                if model in sorted_models:\n+                    continue\n+                dependencies = self.dependencies.get(model._meta.concrete_model)\n+                if not (dependencies and dependencies.difference(concrete_models)):\n+                    sorted_models.append(model)\n+                    concrete_models.add(model._meta.concrete_model)\n+                    found = True\n+            if not found:\n+                return\n+        self.data = {model: self.data[model] for model in sorted_models}\n+\n+    def delete(self):\n+        # sort instance collections\n+        for model, instances in self.data.items():\n+            self.data[model] = sorted(instances, key=attrgetter(\"pk\"))\n+\n+        # if possible, bring the models in an order suitable for databases that\n+        # don't support transactions or cannot defer constraint checks until the\n+        # end of a transaction.\n+        self.sort()\n+        # number of objects deleted for each model label\n+        deleted_counter = Counter()\n+\n+        with transaction.atomic(using=self.using, savepoint=False):\n+            # send pre_delete signals\n+            for model, obj in self.instances_with_model():\n+                if not model._meta.auto_created:\n+                    signals.pre_delete.send(\n+                        sender=model, instance=obj, using=self.using\n+                    )\n+\n+            # fast deletes\n+            for qs in self.fast_deletes:\n+                count = qs._raw_delete(using=self.using)\n+                deleted_counter[qs.model._meta.label] += count\n+\n+            # update fields\n+            for model, instances_for_fieldvalues in self.field_updates.items():\n+                for (field, value), instances in instances_for_fieldvalues.items():\n+                    query = sql.UpdateQuery(model)\n+                    query.update_batch([obj.pk for obj in instances],\n+                                       {field.name: value}, self.using)\n+\n+            # reverse instance collections\n+            for instances in self.data.values():\n+                instances.reverse()\n+\n+            # delete instances\n+            for model, instances in self.data.items():\n+                query = sql.DeleteQuery(model)\n+                pk_list = [obj.pk for obj in instances]\n+                count = query.delete_batch(pk_list, self.using)\n+                deleted_counter[model._meta.label] += count\n+\n+                if not model._meta.auto_created:\n+                    for obj in instances:\n+                        signals.post_delete.send(\n+                            sender=model, instance=obj, using=self.using\n+                        )\n+\n+        # update collected instances\n+        for instances_for_fieldvalues in self.field_updates.values():\n+            for (field, value), instances in instances_for_fieldvalues.items():\n+                for obj in instances:\n+                    setattr(obj, field.attname, value)\n+        for model, instances in self.data.items():\n+            for instance in instances:\n+                setattr(instance, model._meta.pk.attname, None)\n+        return sum(deleted_counter.values()), dict(deleted_counter)\n"
  },
  "django__django-13810": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13810",
    "model_patch": "Fix MiddlewareNotUsed side effects by using a temporary adapted_handler\n\nDon't overwrite `handler` before successfully instantiating the middleware. If the middleware factory raises MiddlewareNotUsed, we must not change the handler (this was causing the middleware chain to become \"poisoned\" with an adapted handler and led to incorrect async/sync handling). Adapt the handler into a temporary variable `adapted_handler` and pass that to the middleware factory; only if the factory returns an instance do we proceed to use it.\n\nModified file:\n- django/core/handlers/base.py: use adapted_handler in the try/except around middleware instantiation.\n\nNo changes detected to generate a patch."
  },
  "django__django-14053": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14053",
    "model_patch": "diff --git a/django/contrib/staticfiles/storage.py b/django/contrib/staticfiles/storage.py\nindex 494890cc23..2e063bcbe0 100644\n--- a/django/contrib/staticfiles/storage.py\n+++ b/django/contrib/staticfiles/storage.py\n@@ -248,79 +248,6 @@ class HashedFilesMixin:\n         # Store the processed paths\n         self.hashed_files.update(hashed_files)\n \n-    def _post_process(self, paths, adjustable_paths, hashed_files):\n-        # Sort the files by directory level\n-        def path_level(name):\n-            return len(name.split(os.sep))\n-\n-        for name in sorted(paths, key=path_level, reverse=True):\n-            substitutions = True\n-            # use the original, local file, not the copied-but-unprocessed\n-            # file, which might be somewhere far away, like S3\n-            storage, path = paths[name]\n-            with storage.open(path) as original_file:\n-                cleaned_name = self.clean_name(name)\n-                hash_key = self.hash_key(cleaned_name)\n-\n-                # generate the hash with the original content, even for\n-                # adjustable files.\n-                if hash_key not in hashed_files:\n-                    hashed_name = self.hashed_name(name, original_file)\n-                else:\n-                    hashed_name = hashed_files[hash_key]\n-\n-                # then get the original's file content..\n-                if hasattr(original_file, 'seek'):\n-                    original_file.seek(0)\n-\n-                hashed_file_exists = self.exists(hashed_name)\n-                processed = False\n-\n-                # ..to apply each replacement pattern to the content\n-                if name in adjustable_paths:\n-                    old_hashed_name = hashed_name\n-                    content = original_file.read().decode('utf-8')\n-                    for extension, patterns in self._patterns.items():\n-                        if matches_patterns(path, (extension,)):\n-                            for pattern, template in patterns:\n-                                converter = self.url_converter(name, hashed_files, template)\n-                                try:\n-                                    content = pattern.sub(converter, content)\n-                                except ValueError as exc:\n-                                    yield name, None, exc, False\n-                    if hashed_file_exists:\n-                        self.delete(hashed_name)\n-                    # then save the processed result\n-                    content_file = ContentFile(content.encode())\n-                    if self.keep_intermediate_files:\n-                        # Save intermediate file for reference\n-                        self._save(hashed_name, content_file)\n-                    hashed_name = self.hashed_name(name, content_file)\n-\n-                    if self.exists(hashed_name):\n-                        self.delete(hashed_name)\n-\n-                    saved_name = self._save(hashed_name, content_file)\n-                    hashed_name = self.clean_name(saved_name)\n-                    # If the file hash stayed the same, this file didn't change\n-                    if old_hashed_name == hashed_name:\n-                        substitutions = False\n-                    processed = True\n-\n-                if not processed:\n-                    # or handle the case in which neither processing nor\n-                    # a change to the original file happened\n-                    if not hashed_file_exists:\n-                        processed = True\n-                        saved_name = self._save(hashed_name, original_file)\n-                        hashed_name = self.clean_name(saved_name)\n-\n-                # and then set the cache accordingly\n-                hashed_files[hash_key] = hashed_name\n-\n-                yield name, hashed_name, processed, substitutions\n-\n-    def clean_name(self, name):\n         return name.replace('\\\\', '/')\n \n     def hash_key(self, name):\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..2e063bcbe0\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,368 @@\n+import hashlib\n+import json\n+import os\n+import posixpath\n+import re\n+from urllib.parse import unquote, urldefrag, urlsplit, urlunsplit\n+\n+from django.conf import settings\n+from django.contrib.staticfiles.utils import check_settings, matches_patterns\n+from django.core.exceptions import ImproperlyConfigured\n+from django.core.files.base import ContentFile\n+from django.core.files.storage import FileSystemStorage, get_storage_class\n+from django.utils.functional import LazyObject\n+\n+\n+class StaticFilesStorage(FileSystemStorage):\n+    \"\"\"\n+    Standard file system storage for static files.\n+\n+    The defaults for ``location`` and ``base_url`` are\n+    ``STATIC_ROOT`` and ``STATIC_URL``.\n+    \"\"\"\n+    def __init__(self, location=None, base_url=None, *args, **kwargs):\n+        if location is None:\n+            location = settings.STATIC_ROOT\n+        if base_url is None:\n+            base_url = settings.STATIC_URL\n+        check_settings(base_url)\n+        super().__init__(location, base_url, *args, **kwargs)\n+        # FileSystemStorage fallbacks to MEDIA_ROOT when location\n+        # is empty, so we restore the empty value.\n+        if not location:\n+            self.base_location = None\n+            self.location = None\n+\n+    def path(self, name):\n+        if not self.location:\n+            raise ImproperlyConfigured(\"You're using the staticfiles app \"\n+                                       \"without having set the STATIC_ROOT \"\n+                                       \"setting to a filesystem path.\")\n+        return super().path(name)\n+\n+\n+class HashedFilesMixin:\n+    default_template = \"\"\"url(\"%s\")\"\"\"\n+    max_post_process_passes = 5\n+    patterns = (\n+        (\"*.css\", (\n+            r\"\"\"(url\\(['\"]{0,1}\\s*(.*?)[\"']{0,1}\\))\"\"\",\n+            (r\"\"\"(@import\\s*[\"']\\s*(.*?)[\"'])\"\"\", \"\"\"@import url(\"%s\")\"\"\"),\n+        )),\n+    )\n+    keep_intermediate_files = True\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self._patterns = {}\n+        self.hashed_files = {}\n+        for extension, patterns in self.patterns:\n+            for pattern in patterns:\n+                if isinstance(pattern, (tuple, list)):\n+                    pattern, template = pattern\n+                else:\n+                    template = self.default_template\n+                compiled = re.compile(pattern, re.IGNORECASE)\n+                self._patterns.setdefault(extension, []).append((compiled, template))\n+\n+    def file_hash(self, name, content=None):\n+        \"\"\"\n+        Return a hash of the file with the given name and optional content.\n+        \"\"\"\n+        if content is None:\n+            return None\n+        md5 = hashlib.md5()\n+        for chunk in content.chunks():\n+            md5.update(chunk)\n+        return md5.hexdigest()[:12]\n+\n+    def hashed_name(self, name, content=None, filename=None):\n+        # `filename` is the name of file to hash if `content` isn't given.\n+        # `name` is the base name to construct the new hashed filename from.\n+        parsed_name = urlsplit(unquote(name))\n+        clean_name = parsed_name.path.strip()\n+        filename = (filename and urlsplit(unquote(filename)).path.strip()) or clean_name\n+        opened = content is None\n+        if opened:\n+            if not self.exists(filename):\n+                raise ValueError(\"The file '%s' could not be found with %r.\" % (filename, self))\n+            try:\n+                content = self.open(filename)\n+            except OSError:\n+                # Handle directory paths and fragments\n+                return name\n+        try:\n+            file_hash = self.file_hash(clean_name, content)\n+        finally:\n+            if opened:\n+                content.close()\n+        path, filename = os.path.split(clean_name)\n+        root, ext = os.path.splitext(filename)\n+        file_hash = ('.%s' % file_hash) if file_hash else ''\n+        hashed_name = os.path.join(path, \"%s%s%s\" %\n+                                   (root, file_hash, ext))\n+        unparsed_name = list(parsed_name)\n+        unparsed_name[2] = hashed_name\n+        # Special casing for a @font-face hack, like url(myfont.eot?#iefix\")\n+        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax\n+        if '?#' in name and not unparsed_name[3]:\n+            unparsed_name[2] += '?'\n+        return urlunsplit(unparsed_name)\n+\n+    def _url(self, hashed_name_func, name, force=False, hashed_files=None):\n+        \"\"\"\n+        Return the non-hashed URL in DEBUG mode.\n+        \"\"\"\n+        if settings.DEBUG and not force:\n+            hashed_name, fragment = name, ''\n+        else:\n+            clean_name, fragment = urldefrag(name)\n+            if urlsplit(clean_name).path.endswith('/'):  # don't hash paths\n+                hashed_name = name\n+            else:\n+                args = (clean_name,)\n+                if hashed_files is not None:\n+                    args += (hashed_files,)\n+                hashed_name = hashed_name_func(*args)\n+\n+        final_url = super().url(hashed_name)\n+\n+        # Special casing for a @font-face hack, like url(myfont.eot?#iefix\")\n+        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax\n+        query_fragment = '?#' in name  # [sic!]\n+        if fragment or query_fragment:\n+            urlparts = list(urlsplit(final_url))\n+            if fragment and not urlparts[4]:\n+                urlparts[4] = fragment\n+            if query_fragment and not urlparts[3]:\n+                urlparts[2] += '?'\n+            final_url = urlunsplit(urlparts)\n+\n+        return unquote(final_url)\n+\n+    def url(self, name, force=False):\n+        \"\"\"\n+        Return the non-hashed URL in DEBUG mode.\n+        \"\"\"\n+        return self._url(self.stored_name, name, force)\n+\n+    def url_converter(self, name, hashed_files, template=None):\n+        \"\"\"\n+        Return the custom URL converter for the given file name.\n+        \"\"\"\n+        if template is None:\n+            template = self.default_template\n+\n+        def converter(matchobj):\n+            \"\"\"\n+            Convert the matched URL to a normalized and hashed URL.\n+\n+            This requires figuring out which files the matched URL resolves\n+            to and calling the url() method of the storage.\n+            \"\"\"\n+            matched, url = matchobj.groups()\n+\n+            # Ignore absolute/protocol-relative and data-uri URLs.\n+            if re.match(r'^[a-z]+:', url):\n+                return matched\n+\n+            # Ignore absolute URLs that don't point to a static file (dynamic\n+            # CSS / JS?). Note that STATIC_URL cannot be empty.\n+            if url.startswith('/') and not url.startswith(settings.STATIC_URL):\n+                return matched\n+\n+            # Strip off the fragment so a path-like fragment won't interfere.\n+            url_path, fragment = urldefrag(url)\n+\n+            if url_path.startswith('/'):\n+                # Otherwise the condition above would have returned prematurely.\n+                assert url_path.startswith(settings.STATIC_URL)\n+                target_name = url_path[len(settings.STATIC_URL):]\n+            else:\n+                # We're using the posixpath module to mix paths and URLs conveniently.\n+                source_name = name if os.sep == '/' else name.replace(os.sep, '/')\n+                target_name = posixpath.join(posixpath.dirname(source_name), url_path)\n+\n+            # Determine the hashed name of the target file with the storage backend.\n+            hashed_url = self._url(\n+                self._stored_name, unquote(target_name),\n+                force=True, hashed_files=hashed_files,\n+            )\n+\n+            transformed_url = '/'.join(url_path.split('/')[:-1] + hashed_url.split('/')[-1:])\n+\n+            # Restore the fragment that was stripped off earlier.\n+            if fragment:\n+                transformed_url += ('?#' if '?#' in url else '#') + fragment\n+\n+            # Return the hashed version to the file\n+            return template % unquote(transformed_url)\n+\n+        return converter\n+\n+    def post_process(self, paths, dry_run=False, **options):\n+        \"\"\"\n+        Post process the given dictionary of files (called from collectstatic).\n+\n+        Processing is actually two separate operations:\n+\n+        1. renaming files to include a hash of their content for cache-busting,\n+           and copying those files to the target storage.\n+        2. adjusting files which contain references to other files so they\n+           refer to the cache-busting filenames.\n+\n+        If either of these are performed on a file, then that file is considered\n+        post-processed.\n+        \"\"\"\n+        # don't even dare to process the files if we're in dry run mode\n+        if dry_run:\n+            return\n+\n+        # where to store the new paths\n+        hashed_files = {}\n+\n+        # build a list of adjustable files\n+        adjustable_paths = [\n+            path for path in paths\n+            if matches_patterns(path, self._patterns)\n+        ]\n+        # Do a single pass first. Post-process all files once, then repeat for\n+        # adjustable files.\n+        for name, hashed_name, processed, _ in self._post_process(paths, adjustable_paths, hashed_files):\n+            yield name, hashed_name, processed\n+\n+        paths = {path: paths[path] for path in adjustable_paths}\n+\n+        for i in range(self.max_post_process_passes):\n+            substitutions = False\n+            for name, hashed_name, processed, subst in self._post_process(paths, adjustable_paths, hashed_files):\n+                yield name, hashed_name, processed\n+                substitutions = substitutions or subst\n+\n+            if not substitutions:\n+                break\n+\n+        if substitutions:\n+            yield 'All', None, RuntimeError('Max post-process passes exceeded.')\n+\n+        # Store the processed paths\n+        self.hashed_files.update(hashed_files)\n+\n+        return name.replace('\\\\', '/')\n+\n+    def hash_key(self, name):\n+        return name\n+\n+    def _stored_name(self, name, hashed_files):\n+        # Normalize the path to avoid multiple names for the same file like\n+        # ../foo/bar.css and ../foo/../foo/bar.css which normalize to the same\n+        # path.\n+        name = posixpath.normpath(name)\n+        cleaned_name = self.clean_name(name)\n+        hash_key = self.hash_key(cleaned_name)\n+        cache_name = hashed_files.get(hash_key)\n+        if cache_name is None:\n+            cache_name = self.clean_name(self.hashed_name(name))\n+        return cache_name\n+\n+    def stored_name(self, name):\n+        cleaned_name = self.clean_name(name)\n+        hash_key = self.hash_key(cleaned_name)\n+        cache_name = self.hashed_files.get(hash_key)\n+        if cache_name:\n+            return cache_name\n+        # No cached name found, recalculate it from the files.\n+        intermediate_name = name\n+        for i in range(self.max_post_process_passes + 1):\n+            cache_name = self.clean_name(\n+                self.hashed_name(name, content=None, filename=intermediate_name)\n+            )\n+            if intermediate_name == cache_name:\n+                # Store the hashed name if there was a miss.\n+                self.hashed_files[hash_key] = cache_name\n+                return cache_name\n+            else:\n+                # Move on to the next intermediate file.\n+                intermediate_name = cache_name\n+        # If the cache name can't be determined after the max number of passes,\n+        # the intermediate files on disk may be corrupt; avoid an infinite loop.\n+        raise ValueError(\"The name '%s' could not be hashed with %r.\" % (name, self))\n+\n+\n+class ManifestFilesMixin(HashedFilesMixin):\n+    manifest_version = '1.0'  # the manifest format standard\n+    manifest_name = 'staticfiles.json'\n+    manifest_strict = True\n+    keep_intermediate_files = False\n+\n+    def __init__(self, *args, **kwargs):\n+        super().__init__(*args, **kwargs)\n+        self.hashed_files = self.load_manifest()\n+\n+    def read_manifest(self):\n+        try:\n+            with self.open(self.manifest_name) as manifest:\n+                return manifest.read().decode()\n+        except FileNotFoundError:\n+            return None\n+\n+    def load_manifest(self):\n+        content = self.read_manifest()\n+        if content is None:\n+            return {}\n+        try:\n+            stored = json.loads(content)\n+        except json.JSONDecodeError:\n+            pass\n+        else:\n+            version = stored.get('version')\n+            if version == '1.0':\n+                return stored.get('paths', {})\n+        raise ValueError(\"Couldn't load manifest '%s' (version %s)\" %\n+                         (self.manifest_name, self.manifest_version))\n+\n+    def post_process(self, *args, **kwargs):\n+        self.hashed_files = {}\n+        yield from super().post_process(*args, **kwargs)\n+        if not kwargs.get('dry_run'):\n+            self.save_manifest()\n+\n+    def save_manifest(self):\n+        payload = {'paths': self.hashed_files, 'version': self.manifest_version}\n+        if self.exists(self.manifest_name):\n+            self.delete(self.manifest_name)\n+        contents = json.dumps(payload).encode()\n+        self._save(self.manifest_name, ContentFile(contents))\n+\n+    def stored_name(self, name):\n+        parsed_name = urlsplit(unquote(name))\n+        clean_name = parsed_name.path.strip()\n+        hash_key = self.hash_key(clean_name)\n+        cache_name = self.hashed_files.get(hash_key)\n+        if cache_name is None:\n+            if self.manifest_strict:\n+                raise ValueError(\"Missing staticfiles manifest entry for '%s'\" % clean_name)\n+            cache_name = self.clean_name(self.hashed_name(name))\n+        unparsed_name = list(parsed_name)\n+        unparsed_name[2] = cache_name\n+        # Special casing for a @font-face hack, like url(myfont.eot?#iefix\")\n+        # http://www.fontspring.com/blog/the-new-bulletproof-font-face-syntax\n+        if '?#' in name and not unparsed_name[3]:\n+            unparsed_name[2] += '?'\n+        return urlunsplit(unparsed_name)\n+\n+\n+class ManifestStaticFilesStorage(ManifestFilesMixin, StaticFilesStorage):\n+    \"\"\"\n+    A static file system storage backend which also saves\n+    hashed copies of the files it saves.\n+    \"\"\"\n+    pass\n+\n+\n+class ConfiguredStorage(LazyObject):\n+    def _setup(self):\n+        self._wrapped = get_storage_class(settings.STATICFILES_STORAGE)()\n+\n+\n+staticfiles_storage = ConfiguredStorage()\n"
  },
  "django__django-13297": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-13297",
    "model_patch": "diff --git a/django/views/generic/base.py b/django/views/generic/base.py\nindex ea5baca08d..f14f730bc1 100644\n--- a/django/views/generic/base.py\n+++ b/django/views/generic/base.py\n@@ -153,17 +153,6 @@ class TemplateResponseMixin:\n         else:\n             return [self.template_name]\n \n-\n-class TemplateView(TemplateResponseMixin, ContextMixin, View):\n-    \"\"\"Render a template.\"\"\"\n-    def get(self, request, *args, **kwargs):\n-        # RemovedInDjango40Warning: when the deprecation ends, replace with:\n-        #   context = self.get_context_data()\n-        context_kwargs = _wrap_url_kwargs_with_deprecation_warning(kwargs)\n-        context = self.get_context_data(**context_kwargs)\n-        return self.render_to_response(context)\n-\n-\n # RemovedInDjango40Warning\n def _wrap_url_kwargs_with_deprecation_warning(url_kwargs):\n     context_kwargs = {}\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..f14f730bc1\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,229 @@\n+import logging\n+import warnings\n+from functools import update_wrapper\n+\n+from django.core.exceptions import ImproperlyConfigured\n+from django.http import (\n+    HttpResponse, HttpResponseGone, HttpResponseNotAllowed,\n+    HttpResponsePermanentRedirect, HttpResponseRedirect,\n+)\n+from django.template.response import TemplateResponse\n+from django.urls import reverse\n+from django.utils.decorators import classonlymethod\n+from django.utils.deprecation import RemovedInDjango40Warning\n+from django.utils.functional import SimpleLazyObject\n+\n+logger = logging.getLogger('django.request')\n+\n+\n+class ContextMixin:\n+    \"\"\"\n+    A default context mixin that passes the keyword arguments received by\n+    get_context_data() as the template context.\n+    \"\"\"\n+    extra_context = None\n+\n+    def get_context_data(self, **kwargs):\n+        kwargs.setdefault('view', self)\n+        if self.extra_context is not None:\n+            kwargs.update(self.extra_context)\n+        return kwargs\n+\n+\n+class View:\n+    \"\"\"\n+    Intentionally simple parent class for all views. Only implements\n+    dispatch-by-method and simple sanity checking.\n+    \"\"\"\n+\n+    http_method_names = ['get', 'post', 'put', 'patch', 'delete', 'head', 'options', 'trace']\n+\n+    def __init__(self, **kwargs):\n+        \"\"\"\n+        Constructor. Called in the URLconf; can contain helpful extra\n+        keyword arguments, and other things.\n+        \"\"\"\n+        # Go through keyword arguments, and either save their values to our\n+        # instance, or raise an error.\n+        for key, value in kwargs.items():\n+            setattr(self, key, value)\n+\n+    @classonlymethod\n+    def as_view(cls, **initkwargs):\n+        \"\"\"Main entry point for a request-response process.\"\"\"\n+        for key in initkwargs:\n+            if key in cls.http_method_names:\n+                raise TypeError(\n+                    'The method name %s is not accepted as a keyword argument '\n+                    'to %s().' % (key, cls.__name__)\n+                )\n+            if not hasattr(cls, key):\n+                raise TypeError(\"%s() received an invalid keyword %r. as_view \"\n+                                \"only accepts arguments that are already \"\n+                                \"attributes of the class.\" % (cls.__name__, key))\n+\n+        def view(request, *args, **kwargs):\n+            self = cls(**initkwargs)\n+            self.setup(request, *args, **kwargs)\n+            if not hasattr(self, 'request'):\n+                raise AttributeError(\n+                    \"%s instance has no 'request' attribute. Did you override \"\n+                    \"setup() and forget to call super()?\" % cls.__name__\n+                )\n+            return self.dispatch(request, *args, **kwargs)\n+        view.view_class = cls\n+        view.view_initkwargs = initkwargs\n+\n+        # take name and docstring from class\n+        update_wrapper(view, cls, updated=())\n+\n+        # and possible attributes set by decorators\n+        # like csrf_exempt from dispatch\n+        update_wrapper(view, cls.dispatch, assigned=())\n+        return view\n+\n+    def setup(self, request, *args, **kwargs):\n+        \"\"\"Initialize attributes shared by all view methods.\"\"\"\n+        if hasattr(self, 'get') and not hasattr(self, 'head'):\n+            self.head = self.get\n+        self.request = request\n+        self.args = args\n+        self.kwargs = kwargs\n+\n+    def dispatch(self, request, *args, **kwargs):\n+        # Try to dispatch to the right method; if a method doesn't exist,\n+        # defer to the error handler. Also defer to the error handler if the\n+        # request method isn't on the approved list.\n+        if request.method.lower() in self.http_method_names:\n+            handler = getattr(self, request.method.lower(), self.http_method_not_allowed)\n+        else:\n+            handler = self.http_method_not_allowed\n+        return handler(request, *args, **kwargs)\n+\n+    def http_method_not_allowed(self, request, *args, **kwargs):\n+        logger.warning(\n+            'Method Not Allowed (%s): %s', request.method, request.path,\n+            extra={'status_code': 405, 'request': request}\n+        )\n+        return HttpResponseNotAllowed(self._allowed_methods())\n+\n+    def options(self, request, *args, **kwargs):\n+        \"\"\"Handle responding to requests for the OPTIONS HTTP verb.\"\"\"\n+        response = HttpResponse()\n+        response['Allow'] = ', '.join(self._allowed_methods())\n+        response['Content-Length'] = '0'\n+        return response\n+\n+    def _allowed_methods(self):\n+        return [m.upper() for m in self.http_method_names if hasattr(self, m)]\n+\n+\n+class TemplateResponseMixin:\n+    \"\"\"A mixin that can be used to render a template.\"\"\"\n+    template_name = None\n+    template_engine = None\n+    response_class = TemplateResponse\n+    content_type = None\n+\n+    def render_to_response(self, context, **response_kwargs):\n+        \"\"\"\n+        Return a response, using the `response_class` for this view, with a\n+        template rendered with the given context.\n+\n+        Pass response_kwargs to the constructor of the response class.\n+        \"\"\"\n+        response_kwargs.setdefault('content_type', self.content_type)\n+        return self.response_class(\n+            request=self.request,\n+            template=self.get_template_names(),\n+            context=context,\n+            using=self.template_engine,\n+            **response_kwargs\n+        )\n+\n+    def get_template_names(self):\n+        \"\"\"\n+        Return a list of template names to be used for the request. Must return\n+        a list. May not be called if render_to_response() is overridden.\n+        \"\"\"\n+        if self.template_name is None:\n+            raise ImproperlyConfigured(\n+                \"TemplateResponseMixin requires either a definition of \"\n+                \"'template_name' or an implementation of 'get_template_names()'\")\n+        else:\n+            return [self.template_name]\n+\n+# RemovedInDjango40Warning\n+def _wrap_url_kwargs_with_deprecation_warning(url_kwargs):\n+    context_kwargs = {}\n+    for key, value in url_kwargs.items():\n+        # Bind into function closure.\n+        @SimpleLazyObject\n+        def access_value(key=key, value=value):\n+            warnings.warn(\n+                'TemplateView passing URL kwargs to the context is '\n+                'deprecated. Reference %s in your template through '\n+                'view.kwargs instead.' % key,\n+                RemovedInDjango40Warning, stacklevel=2,\n+            )\n+            return value\n+        context_kwargs[key] = access_value\n+    return context_kwargs\n+\n+\n+class RedirectView(View):\n+    \"\"\"Provide a redirect on any GET request.\"\"\"\n+    permanent = False\n+    url = None\n+    pattern_name = None\n+    query_string = False\n+\n+    def get_redirect_url(self, *args, **kwargs):\n+        \"\"\"\n+        Return the URL redirect to. Keyword arguments from the URL pattern\n+        match generating the redirect request are provided as kwargs to this\n+        method.\n+        \"\"\"\n+        if self.url:\n+            url = self.url % kwargs\n+        elif self.pattern_name:\n+            url = reverse(self.pattern_name, args=args, kwargs=kwargs)\n+        else:\n+            return None\n+\n+        args = self.request.META.get('QUERY_STRING', '')\n+        if args and self.query_string:\n+            url = \"%s?%s\" % (url, args)\n+        return url\n+\n+    def get(self, request, *args, **kwargs):\n+        url = self.get_redirect_url(*args, **kwargs)\n+        if url:\n+            if self.permanent:\n+                return HttpResponsePermanentRedirect(url)\n+            else:\n+                return HttpResponseRedirect(url)\n+        else:\n+            logger.warning(\n+                'Gone: %s', request.path,\n+                extra={'status_code': 410, 'request': request}\n+            )\n+            return HttpResponseGone()\n+\n+    def head(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n+\n+    def post(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n+\n+    def options(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n+\n+    def delete(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n+\n+    def put(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n+\n+    def patch(self, request, *args, **kwargs):\n+        return self.get(request, *args, **kwargs)\n"
  },
  "sympy__sympy-24213": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sympy__sympy-24213",
    "model_patch": "diff --git a/sympy/physics/units/unitsystem.py b/sympy/physics/units/unitsystem.py\nindex a436880c4b..934d5acc23 100644\n--- a/sympy/physics/units/unitsystem.py\n+++ b/sympy/physics/units/unitsystem.py\n@@ -170,18 +170,6 @@ def _collect_factor_and_dimension(self, expr):\n             if self.get_dimension_system().is_dimensionless(exp_dim):\n                 exp_dim = 1\n             return factor ** exp_factor, dim ** (exp_factor * exp_dim)\n-        elif isinstance(expr, Add):\n-            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n-            for addend in expr.args[1:]:\n-                addend_factor, addend_dim = \\\n-                    self._collect_factor_and_dimension(addend)\n-                if dim != addend_dim:\n-                    raise ValueError(\n-                        'Dimension of \"{}\" is {}, '\n-                        'but it should be {}'.format(\n-                            addend, addend_dim, dim))\n-                factor += addend_factor\n-            return factor, dim\n         elif isinstance(expr, Derivative):\n             factor, dim = self._collect_factor_and_dimension(expr.args[0])\n             for independent, count in expr.variable_count:\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..934d5acc23\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,193 @@\n+\"\"\"\n+Unit system for physical quantities; include definition of constants.\n+\"\"\"\n+\n+from typing import Dict as tDict, Set as tSet\n+\n+from sympy.core.add import Add\n+from sympy.core.function import (Derivative, Function)\n+from sympy.core.mul import Mul\n+from sympy.core.power import Pow\n+from sympy.core.singleton import S\n+from sympy.physics.units.dimensions import _QuantityMapper\n+from sympy.physics.units.quantities import Quantity\n+\n+from .dimensions import Dimension\n+\n+\n+class UnitSystem(_QuantityMapper):\n+    \"\"\"\n+    UnitSystem represents a coherent set of units.\n+\n+    A unit system is basically a dimension system with notions of scales. Many\n+    of the methods are defined in the same way.\n+\n+    It is much better if all base units have a symbol.\n+    \"\"\"\n+\n+    _unit_systems = {}  # type: tDict[str, UnitSystem]\n+\n+    def __init__(self, base_units, units=(), name=\"\", descr=\"\", dimension_system=None, derived_units: tDict[Dimension, Quantity]={}):\n+\n+        UnitSystem._unit_systems[name] = self\n+\n+        self.name = name\n+        self.descr = descr\n+\n+        self._base_units = base_units\n+        self._dimension_system = dimension_system\n+        self._units = tuple(set(base_units) | set(units))\n+        self._base_units = tuple(base_units)\n+        self._derived_units = derived_units\n+\n+        super().__init__()\n+\n+    def __str__(self):\n+        \"\"\"\n+        Return the name of the system.\n+\n+        If it does not exist, then it makes a list of symbols (or names) of\n+        the base dimensions.\n+        \"\"\"\n+\n+        if self.name != \"\":\n+            return self.name\n+        else:\n+            return \"UnitSystem((%s))\" % \", \".join(\n+                str(d) for d in self._base_units)\n+\n+    def __repr__(self):\n+        return '<UnitSystem: %s>' % repr(self._base_units)\n+\n+    def extend(self, base, units=(), name=\"\", description=\"\", dimension_system=None, derived_units: tDict[Dimension, Quantity]={}):\n+        \"\"\"Extend the current system into a new one.\n+\n+        Take the base and normal units of the current system to merge\n+        them to the base and normal units given in argument.\n+        If not provided, name and description are overridden by empty strings.\n+        \"\"\"\n+\n+        base = self._base_units + tuple(base)\n+        units = self._units + tuple(units)\n+\n+        return UnitSystem(base, units, name, description, dimension_system, {**self._derived_units, **derived_units})\n+\n+    def get_dimension_system(self):\n+        return self._dimension_system\n+\n+    def get_quantity_dimension(self, unit):\n+        qdm = self.get_dimension_system()._quantity_dimension_map\n+        if unit in qdm:\n+            return qdm[unit]\n+        return super().get_quantity_dimension(unit)\n+\n+    def get_quantity_scale_factor(self, unit):\n+        qsfm = self.get_dimension_system()._quantity_scale_factors\n+        if unit in qsfm:\n+            return qsfm[unit]\n+        return super().get_quantity_scale_factor(unit)\n+\n+    @staticmethod\n+    def get_unit_system(unit_system):\n+        if isinstance(unit_system, UnitSystem):\n+            return unit_system\n+\n+        if unit_system not in UnitSystem._unit_systems:\n+            raise ValueError(\n+                \"Unit system is not supported. Currently\"\n+                \"supported unit systems are {}\".format(\n+                    \", \".join(sorted(UnitSystem._unit_systems))\n+                )\n+            )\n+\n+        return UnitSystem._unit_systems[unit_system]\n+\n+    @staticmethod\n+    def get_default_unit_system():\n+        return UnitSystem._unit_systems[\"SI\"]\n+\n+    @property\n+    def dim(self):\n+        \"\"\"\n+        Give the dimension of the system.\n+\n+        That is return the number of units forming the basis.\n+        \"\"\"\n+        return len(self._base_units)\n+\n+    @property\n+    def is_consistent(self):\n+        \"\"\"\n+        Check if the underlying dimension system is consistent.\n+        \"\"\"\n+        # test is performed in DimensionSystem\n+        return self.get_dimension_system().is_consistent\n+\n+    @property\n+    def derived_units(self) -> tDict[Dimension, Quantity]:\n+        return self._derived_units\n+\n+    def get_dimensional_expr(self, expr):\n+        from sympy.physics.units import Quantity\n+        if isinstance(expr, Mul):\n+            return Mul(*[self.get_dimensional_expr(i) for i in expr.args])\n+        elif isinstance(expr, Pow):\n+            return self.get_dimensional_expr(expr.base) ** expr.exp\n+        elif isinstance(expr, Add):\n+            return self.get_dimensional_expr(expr.args[0])\n+        elif isinstance(expr, Derivative):\n+            dim = self.get_dimensional_expr(expr.expr)\n+            for independent, count in expr.variable_count:\n+                dim /= self.get_dimensional_expr(independent)**count\n+            return dim\n+        elif isinstance(expr, Function):\n+            args = [self.get_dimensional_expr(arg) for arg in expr.args]\n+            if all(i == 1 for i in args):\n+                return S.One\n+            return expr.func(*args)\n+        elif isinstance(expr, Quantity):\n+            return self.get_quantity_dimension(expr).name\n+        return S.One\n+\n+    def _collect_factor_and_dimension(self, expr):\n+        \"\"\"\n+        Return tuple with scale factor expression and dimension expression.\n+        \"\"\"\n+        from sympy.physics.units import Quantity\n+        if isinstance(expr, Quantity):\n+            return expr.scale_factor, expr.dimension\n+        elif isinstance(expr, Mul):\n+            factor = 1\n+            dimension = Dimension(1)\n+            for arg in expr.args:\n+                arg_factor, arg_dim = self._collect_factor_and_dimension(arg)\n+                factor *= arg_factor\n+                dimension *= arg_dim\n+            return factor, dimension\n+        elif isinstance(expr, Pow):\n+            factor, dim = self._collect_factor_and_dimension(expr.base)\n+            exp_factor, exp_dim = self._collect_factor_and_dimension(expr.exp)\n+            if self.get_dimension_system().is_dimensionless(exp_dim):\n+                exp_dim = 1\n+            return factor ** exp_factor, dim ** (exp_factor * exp_dim)\n+        elif isinstance(expr, Derivative):\n+            factor, dim = self._collect_factor_and_dimension(expr.args[0])\n+            for independent, count in expr.variable_count:\n+                ifactor, idim = self._collect_factor_and_dimension(independent)\n+                factor /= ifactor**count\n+                dim /= idim**count\n+            return factor, dim\n+        elif isinstance(expr, Function):\n+            fds = [self._collect_factor_and_dimension(arg) for arg in expr.args]\n+            dims = [Dimension(1) if self.get_dimension_system().is_dimensionless(d[1]) else d[1] for d in fds]\n+            return (expr.func(*(f[0] for f in fds)), *dims)\n+        elif isinstance(expr, Dimension):\n+            return S.One, expr\n+        else:\n+            return expr, Dimension(1)\n+\n+    def get_units_non_prefixed(self) -> tSet[Quantity]:\n+        \"\"\"\n+        Return the units of the system that do not have a prefix.\n+        \"\"\"\n+        return set(filter(lambda u: not u.is_prefixed and not u.is_physical_constant, self._units))\n"
  },
  "astropy__astropy-7166": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "astropy__astropy-7166",
    "model_patch": "diff --git a/astropy/utils/misc.py b/astropy/utils/misc.py\nindex de3fb01d1..1bdbbb81f 100644\n--- a/astropy/utils/misc.py\n+++ b/astropy/utils/misc.py\n@@ -520,25 +520,6 @@ class InheritDocstrings(type):\n         u'Wiggle the thingamajig'\n     \"\"\"\n \n-    def __init__(cls, name, bases, dct):\n-        def is_public_member(key):\n-            return (\n-                (key.startswith('__') and key.endswith('__')\n-                 and len(key) > 4) or\n-                not key.startswith('_'))\n-\n-        for key, val in dct.items():\n-            if (inspect.isfunction(val) and\n-                is_public_member(key) and\n-                val.__doc__ is None):\n-                for base in cls.__mro__[1:]:\n-                    super_method = getattr(base, key, None)\n-                    if super_method is not None:\n-                        val.__doc__ = super_method.__doc__\n-                        break\n-\n-        super().__init__(name, bases, dct)\n-\n \n class OrderedDescriptor(metaclass=abc.ABCMeta):\n     \"\"\"\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 000000000..1bdbbb81f\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,1110 @@\n+# -*- coding: utf-8 -*-\n+# Licensed under a 3-clause BSD style license - see LICENSE.rst\n+\"\"\"\n+A \"grab bag\" of relatively small general-purpose utilities that don't have\n+a clear module/package to live in.\n+\"\"\"\n+\n+\n+\n+import abc\n+import contextlib\n+import difflib\n+import inspect\n+import json\n+import os\n+import signal\n+import sys\n+import traceback\n+import unicodedata\n+import locale\n+import threading\n+import re\n+import urllib.request\n+\n+from itertools import zip_longest\n+from contextlib import contextmanager\n+from collections import defaultdict, OrderedDict\n+\n+\n+\n+__all__ = ['isiterable', 'silence', 'format_exception', 'NumpyRNGContext',\n+           'find_api_page', 'is_path_hidden', 'walk_skip_hidden',\n+           'JsonCustomEncoder', 'indent', 'InheritDocstrings',\n+           'OrderedDescriptor', 'OrderedDescriptorContainer', 'set_locale',\n+           'ShapedLikeNDArray', 'check_broadcast', 'IncompatibleShapeError',\n+           'dtype_bytes_or_chars']\n+\n+\n+def isiterable(obj):\n+    \"\"\"Returns `True` if the given object is iterable.\"\"\"\n+\n+    try:\n+        iter(obj)\n+        return True\n+    except TypeError:\n+        return False\n+\n+\n+def indent(s, shift=1, width=4):\n+    \"\"\"Indent a block of text.  The indentation is applied to each line.\"\"\"\n+\n+    indented = '\\n'.join(' ' * (width * shift) + l if l else ''\n+                         for l in s.splitlines())\n+    if s[-1] == '\\n':\n+        indented += '\\n'\n+\n+    return indented\n+\n+\n+class _DummyFile:\n+    \"\"\"A noop writeable object.\"\"\"\n+\n+    def write(self, s):\n+        pass\n+\n+\n+@contextlib.contextmanager\n+def silence():\n+    \"\"\"A context manager that silences sys.stdout and sys.stderr.\"\"\"\n+\n+    old_stdout = sys.stdout\n+    old_stderr = sys.stderr\n+    sys.stdout = _DummyFile()\n+    sys.stderr = _DummyFile()\n+    yield\n+    sys.stdout = old_stdout\n+    sys.stderr = old_stderr\n+\n+\n+def format_exception(msg, *args, **kwargs):\n+    \"\"\"\n+    Given an exception message string, uses new-style formatting arguments\n+    ``{filename}``, ``{lineno}``, ``{func}`` and/or ``{text}`` to fill in\n+    information about the exception that occurred.  For example:\n+\n+        try:\n+            1/0\n+        except:\n+            raise ZeroDivisionError(\n+                format_except('A divide by zero occurred in {filename} at '\n+                              'line {lineno} of function {func}.'))\n+\n+    Any additional positional or keyword arguments passed to this function are\n+    also used to format the message.\n+\n+    .. note::\n+        This uses `sys.exc_info` to gather up the information needed to fill\n+        in the formatting arguments. Since `sys.exc_info` is not carried\n+        outside a handled exception, it's not wise to use this\n+        outside of an ``except`` clause - if it is, this will substitute\n+        '<unkown>' for the 4 formatting arguments.\n+    \"\"\"\n+\n+    tb = traceback.extract_tb(sys.exc_info()[2], limit=1)\n+    if len(tb) > 0:\n+        filename, lineno, func, text = tb[0]\n+    else:\n+        filename = lineno = func = text = '<unknown>'\n+\n+    return msg.format(*args, filename=filename, lineno=lineno, func=func,\n+                      text=text, **kwargs)\n+\n+\n+class NumpyRNGContext:\n+    \"\"\"\n+    A context manager (for use with the ``with`` statement) that will seed the\n+    numpy random number generator (RNG) to a specific value, and then restore\n+    the RNG state back to whatever it was before.\n+\n+    This is primarily intended for use in the astropy testing suit, but it\n+    may be useful in ensuring reproducibility of Monte Carlo simulations in a\n+    science context.\n+\n+    Parameters\n+    ----------\n+    seed : int\n+        The value to use to seed the numpy RNG\n+\n+    Examples\n+    --------\n+    A typical use case might be::\n+\n+        with NumpyRNGContext(<some seed value you pick>):\n+            from numpy import random\n+\n+            randarr = random.randn(100)\n+            ... run your test using `randarr` ...\n+\n+        #Any code using numpy.random at this indent level will act just as it\n+        #would have if it had been before the with statement - e.g. whatever\n+        #the default seed is.\n+\n+\n+    \"\"\"\n+\n+    def __init__(self, seed):\n+        self.seed = seed\n+\n+    def __enter__(self):\n+        from numpy import random\n+\n+        self.startstate = random.get_state()\n+        random.seed(self.seed)\n+\n+    def __exit__(self, exc_type, exc_value, traceback):\n+        from numpy import random\n+\n+        random.set_state(self.startstate)\n+\n+\n+def find_api_page(obj, version=None, openinbrowser=True, timeout=None):\n+    \"\"\"\n+    Determines the URL of the API page for the specified object, and\n+    optionally open that page in a web browser.\n+\n+    .. note::\n+        You must be connected to the internet for this to function even if\n+        ``openinbrowser`` is `False`, unless you provide a local version of\n+        the documentation to ``version`` (e.g., ``file:///path/to/docs``).\n+\n+    Parameters\n+    ----------\n+    obj\n+        The object to open the docs for or its fully-qualified name\n+        (as a str).\n+    version : str\n+        The doc version - either a version number like '0.1', 'dev' for\n+        the development/latest docs, or a URL to point to a specific\n+        location that should be the *base* of the documentation. Defaults to\n+        latest if you are on aren't on a release, otherwise, the version you\n+        are on.\n+    openinbrowser : bool\n+        If `True`, the `webbrowser` package will be used to open the doc\n+        page in a new web browser window.\n+    timeout : number, optional\n+        The number of seconds to wait before timing-out the query to\n+        the astropy documentation.  If not given, the default python\n+        stdlib timeout will be used.\n+\n+    Returns\n+    -------\n+    url : str\n+        The loaded URL\n+\n+    Raises\n+    ------\n+    ValueError\n+        If the documentation can't be found\n+\n+    \"\"\"\n+    import webbrowser\n+\n+    from zlib import decompress\n+\n+    if (not isinstance(obj, str) and\n+            hasattr(obj, '__module__') and\n+            hasattr(obj, '__name__')):\n+        obj = obj.__module__ + '.' + obj.__name__\n+    elif inspect.ismodule(obj):\n+        obj = obj.__name__\n+\n+    if version is None:\n+        from .. import version\n+\n+        if version.release:\n+            version = 'v' + version.version\n+        else:\n+            version = 'dev'\n+\n+    if '://' in version:\n+        if version.endswith('index.html'):\n+            baseurl = version[:-10]\n+        elif version.endswith('/'):\n+            baseurl = version\n+        else:\n+            baseurl = version + '/'\n+    elif version == 'dev' or version == 'latest':\n+        baseurl = 'http://devdocs.astropy.org/'\n+    else:\n+        baseurl = 'http://docs.astropy.org/en/{vers}/'.format(vers=version)\n+\n+    if timeout is None:\n+        uf = urllib.request.urlopen(baseurl + 'objects.inv')\n+    else:\n+        uf = urllib.request.urlopen(baseurl + 'objects.inv', timeout=timeout)\n+\n+    try:\n+        oiread = uf.read()\n+\n+        # need to first read/remove the first four lines, which have info before\n+        # the compressed section with the actual object inventory\n+        idx = -1\n+        headerlines = []\n+        for _ in range(4):\n+            oldidx = idx\n+            idx = oiread.index(b'\\n', oldidx + 1)\n+            headerlines.append(oiread[(oldidx+1):idx].decode('utf-8'))\n+\n+        # intersphinx version line, project name, and project version\n+        ivers, proj, vers, compr = headerlines\n+        if 'The remainder of this file is compressed using zlib' not in compr:\n+            raise ValueError('The file downloaded from {0} does not seem to be'\n+                             'the usual Sphinx objects.inv format.  Maybe it '\n+                             'has changed?'.format(baseurl + 'objects.inv'))\n+\n+        compressed = oiread[(idx+1):]\n+    finally:\n+        uf.close()\n+\n+    decompressed = decompress(compressed).decode('utf-8')\n+\n+    resurl = None\n+\n+    for l in decompressed.strip().splitlines():\n+        ls = l.split()\n+        name = ls[0]\n+        loc = ls[3]\n+        if loc.endswith('$'):\n+            loc = loc[:-1] + name\n+\n+        if name == obj:\n+            resurl = baseurl + loc\n+            break\n+\n+    if resurl is None:\n+        raise ValueError('Could not find the docs for the object {obj}'.format(obj=obj))\n+    elif openinbrowser:\n+        webbrowser.open(resurl)\n+\n+    return resurl\n+\n+\n+def signal_number_to_name(signum):\n+    \"\"\"\n+    Given an OS signal number, returns a signal name.  If the signal\n+    number is unknown, returns ``'UNKNOWN'``.\n+    \"\"\"\n+    # Since these numbers and names are platform specific, we use the\n+    # builtin signal module and build a reverse mapping.\n+\n+    signal_to_name_map = dict((k, v) for v, k in signal.__dict__.items()\n+                              if v.startswith('SIG'))\n+\n+    return signal_to_name_map.get(signum, 'UNKNOWN')\n+\n+\n+if sys.platform == 'win32':\n+    import ctypes\n+\n+    def _has_hidden_attribute(filepath):\n+        \"\"\"\n+        Returns True if the given filepath has the hidden attribute on\n+        MS-Windows.  Based on a post here:\n+        http://stackoverflow.com/questions/284115/cross-platform-hidden-file-detection\n+        \"\"\"\n+        if isinstance(filepath, bytes):\n+            filepath = filepath.decode(sys.getfilesystemencoding())\n+        try:\n+            attrs = ctypes.windll.kernel32.GetFileAttributesW(filepath)\n+            result = bool(attrs & 2) and attrs != -1\n+        except AttributeError:\n+            result = False\n+        return result\n+else:\n+    def _has_hidden_attribute(filepath):\n+        return False\n+\n+\n+def is_path_hidden(filepath):\n+    \"\"\"\n+    Determines if a given file or directory is hidden.\n+\n+    Parameters\n+    ----------\n+    filepath : str\n+        The path to a file or directory\n+\n+    Returns\n+    -------\n+    hidden : bool\n+        Returns `True` if the file is hidden\n+    \"\"\"\n+    name = os.path.basename(os.path.abspath(filepath))\n+    if isinstance(name, bytes):\n+        is_dotted = name.startswith(b'.')\n+    else:\n+        is_dotted = name.startswith('.')\n+    return is_dotted or _has_hidden_attribute(filepath)\n+\n+\n+def walk_skip_hidden(top, onerror=None, followlinks=False):\n+    \"\"\"\n+    A wrapper for `os.walk` that skips hidden files and directories.\n+\n+    This function does not have the parameter ``topdown`` from\n+    `os.walk`: the directories must always be recursed top-down when\n+    using this function.\n+\n+    See also\n+    --------\n+    os.walk : For a description of the parameters\n+    \"\"\"\n+    for root, dirs, files in os.walk(\n+            top, topdown=True, onerror=onerror,\n+            followlinks=followlinks):\n+        # These lists must be updated in-place so os.walk will skip\n+        # hidden directories\n+        dirs[:] = [d for d in dirs if not is_path_hidden(d)]\n+        files[:] = [f for f in files if not is_path_hidden(f)]\n+        yield root, dirs, files\n+\n+\n+class JsonCustomEncoder(json.JSONEncoder):\n+    \"\"\"Support for data types that JSON default encoder\n+    does not do.\n+\n+    This includes:\n+\n+        * Numpy array or number\n+        * Complex number\n+        * Set\n+        * Bytes\n+        * astropy.UnitBase\n+        * astropy.Quantity\n+\n+    Examples\n+    --------\n+    >>> import json\n+    >>> import numpy as np\n+    >>> from astropy.utils.misc import JsonCustomEncoder\n+    >>> json.dumps(np.arange(3), cls=JsonCustomEncoder)\n+    '[0, 1, 2]'\n+\n+    \"\"\"\n+\n+    def default(self, obj):\n+        from .. import units as u\n+        import numpy as np\n+        if isinstance(obj, u.Quantity):\n+            return dict(value=obj.value, unit=obj.unit.to_string())\n+        if isinstance(obj, (np.number, np.ndarray)):\n+            return obj.tolist()\n+        elif isinstance(obj, complex):\n+            return [obj.real, obj.imag]\n+        elif isinstance(obj, set):\n+            return list(obj)\n+        elif isinstance(obj, bytes):  # pragma: py3\n+            return obj.decode()\n+        elif isinstance(obj, (u.UnitBase, u.FunctionUnitBase)):\n+            if obj == u.dimensionless_unscaled:\n+                obj = 'dimensionless_unit'\n+            else:\n+                return obj.to_string()\n+\n+        return json.JSONEncoder.default(self, obj)\n+\n+\n+def strip_accents(s):\n+    \"\"\"\n+    Remove accents from a Unicode string.\n+\n+    This helps with matching \"\u00e5ngstr\u00f6m\" to \"angstrom\", for example.\n+    \"\"\"\n+    return ''.join(\n+        c for c in unicodedata.normalize('NFD', s)\n+        if unicodedata.category(c) != 'Mn')\n+\n+\n+def did_you_mean(s, candidates, n=3, cutoff=0.8, fix=None):\n+    \"\"\"\n+    When a string isn't found in a set of candidates, we can be nice\n+    to provide a list of alternatives in the exception.  This\n+    convenience function helps to format that part of the exception.\n+\n+    Parameters\n+    ----------\n+    s : str\n+\n+    candidates : sequence of str or dict of str keys\n+\n+    n : int\n+        The maximum number of results to include.  See\n+        `difflib.get_close_matches`.\n+\n+    cutoff : float\n+        In the range [0, 1]. Possibilities that don't score at least\n+        that similar to word are ignored.  See\n+        `difflib.get_close_matches`.\n+\n+    fix : callable\n+        A callable to modify the results after matching.  It should\n+        take a single string and return a sequence of strings\n+        containing the fixed matches.\n+\n+    Returns\n+    -------\n+    message : str\n+        Returns the string \"Did you mean X, Y, or Z?\", or the empty\n+        string if no alternatives were found.\n+    \"\"\"\n+    if isinstance(s, str):\n+        s = strip_accents(s)\n+    s_lower = s.lower()\n+\n+    # Create a mapping from the lower case name to all capitalization\n+    # variants of that name.\n+    candidates_lower = {}\n+    for candidate in candidates:\n+        candidate_lower = candidate.lower()\n+        candidates_lower.setdefault(candidate_lower, [])\n+        candidates_lower[candidate_lower].append(candidate)\n+\n+    # The heuristic here is to first try \"singularizing\" the word.  If\n+    # that doesn't match anything use difflib to find close matches in\n+    # original, lower and upper case.\n+    if s_lower.endswith('s') and s_lower[:-1] in candidates_lower:\n+        matches = [s_lower[:-1]]\n+    else:\n+        matches = difflib.get_close_matches(\n+            s_lower, candidates_lower, n=n, cutoff=cutoff)\n+\n+    if len(matches):\n+        capitalized_matches = set()\n+        for match in matches:\n+            capitalized_matches.update(candidates_lower[match])\n+        matches = capitalized_matches\n+\n+        if fix is not None:\n+            mapped_matches = []\n+            for match in matches:\n+                mapped_matches.extend(fix(match))\n+            matches = mapped_matches\n+\n+        matches = list(set(matches))\n+        matches = sorted(matches)\n+\n+        if len(matches) == 1:\n+            matches = matches[0]\n+        else:\n+            matches = (', '.join(matches[:-1]) + ' or ' +\n+                       matches[-1])\n+        return 'Did you mean {0}?'.format(matches)\n+\n+    return ''\n+\n+\n+class InheritDocstrings(type):\n+    \"\"\"\n+    This metaclass makes methods of a class automatically have their\n+    docstrings filled in from the methods they override in the base\n+    class.\n+\n+    If the class uses multiple inheritance, the docstring will be\n+    chosen from the first class in the bases list, in the same way as\n+    methods are normally resolved in Python.  If this results in\n+    selecting the wrong docstring, the docstring will need to be\n+    explicitly included on the method.\n+\n+    For example::\n+\n+        >>> from astropy.utils.misc import InheritDocstrings\n+        >>> class A(metaclass=InheritDocstrings):\n+        ...     def wiggle(self):\n+        ...         \"Wiggle the thingamajig\"\n+        ...         pass\n+        >>> class B(A):\n+        ...     def wiggle(self):\n+        ...         pass\n+        >>> B.wiggle.__doc__\n+        u'Wiggle the thingamajig'\n+    \"\"\"\n+\n+\n+class OrderedDescriptor(metaclass=abc.ABCMeta):\n+    \"\"\"\n+    Base class for descriptors whose order in the class body should be\n+    preserved.  Intended for use in concert with the\n+    `OrderedDescriptorContainer` metaclass.\n+\n+    Subclasses of `OrderedDescriptor` must define a value for a class attribute\n+    called ``_class_attribute_``.  This is the name of a class attribute on the\n+    *container* class for these descriptors, which will be set to an\n+    `~collections.OrderedDict` at class creation time.  This\n+    `~collections.OrderedDict` will contain a mapping of all class attributes\n+    that were assigned instances of the `OrderedDescriptor` subclass, to the\n+    instances themselves.  See the documentation for\n+    `OrderedDescriptorContainer` for a concrete example.\n+\n+    Optionally, subclasses of `OrderedDescriptor` may define a value for a\n+    class attribute called ``_name_attribute_``.  This should be the name of\n+    an attribute on instances of the subclass.  When specified, during\n+    creation of a class containing these descriptors, the name attribute on\n+    each instance will be set to the name of the class attribute it was\n+    assigned to on the class.\n+\n+    .. note::\n+\n+        Although this class is intended for use with *descriptors* (i.e.\n+        classes that define any of the ``__get__``, ``__set__``, or\n+        ``__delete__`` magic methods), this base class is not itself a\n+        descriptor, and technically this could be used for classes that are\n+        not descriptors too.  However, use with descriptors is the original\n+        intended purpose.\n+    \"\"\"\n+\n+    # This id increments for each OrderedDescriptor instance created, so they\n+    # are always ordered in the order they were created.  Class bodies are\n+    # guaranteed to be executed from top to bottom.  Not sure if this is\n+    # thread-safe though.\n+    _nextid = 1\n+\n+    @property\n+    @abc.abstractmethod\n+    def _class_attribute_(self):\n+        \"\"\"\n+        Subclasses should define this attribute to the name of an attribute on\n+        classes containing this subclass.  That attribute will contain the mapping\n+        of all instances of that `OrderedDescriptor` subclass defined in the class\n+        body.  If the same descriptor needs to be used with different classes,\n+        each with different names of this attribute, multiple subclasses will be\n+        needed.\n+        \"\"\"\n+\n+    _name_attribute_ = None\n+    \"\"\"\n+    Subclasses may optionally define this attribute to specify the name of an\n+    attribute on instances of the class that should be filled with the\n+    instance's attribute name at class creation time.\n+    \"\"\"\n+\n+    def __init__(self, *args, **kwargs):\n+        # The _nextid attribute is shared across all subclasses so that\n+        # different subclasses of OrderedDescriptors can be sorted correctly\n+        # between themselves\n+        self.__order = OrderedDescriptor._nextid\n+        OrderedDescriptor._nextid += 1\n+        super().__init__()\n+\n+    def __lt__(self, other):\n+        \"\"\"\n+        Defined for convenient sorting of `OrderedDescriptor` instances, which\n+        are defined to sort in their creation order.\n+        \"\"\"\n+\n+        if (isinstance(self, OrderedDescriptor) and\n+                isinstance(other, OrderedDescriptor)):\n+            try:\n+                return self.__order < other.__order\n+            except AttributeError:\n+                raise RuntimeError(\n+                    'Could not determine ordering for {0} and {1}; at least '\n+                    'one of them is not calling super().__init__ in its '\n+                    '__init__.'.format(self, other))\n+        else:\n+            return NotImplemented\n+\n+\n+class OrderedDescriptorContainer(type):\n+    \"\"\"\n+    Classes should use this metaclass if they wish to use `OrderedDescriptor`\n+    attributes, which are class attributes that \"remember\" the order in which\n+    they were defined in the class body.\n+\n+    Every subclass of `OrderedDescriptor` has an attribute called\n+    ``_class_attribute_``.  For example, if we have\n+\n+    .. code:: python\n+\n+        class ExampleDecorator(OrderedDescriptor):\n+            _class_attribute_ = '_examples_'\n+\n+    Then when a class with the `OrderedDescriptorContainer` metaclass is\n+    created, it will automatically be assigned a class attribute ``_examples_``\n+    referencing an `~collections.OrderedDict` containing all instances of\n+    ``ExampleDecorator`` defined in the class body, mapped to by the names of\n+    the attributes they were assigned to.\n+\n+    When subclassing a class with this metaclass, the descriptor dict (i.e.\n+    ``_examples_`` in the above example) will *not* contain descriptors\n+    inherited from the base class.  That is, this only works by default with\n+    decorators explicitly defined in the class body.  However, the subclass\n+    *may* define an attribute ``_inherit_decorators_`` which lists\n+    `OrderedDescriptor` classes that *should* be added from base classes.\n+    See the examples section below for an example of this.\n+\n+    Examples\n+    --------\n+\n+    >>> from astropy.utils import OrderedDescriptor, OrderedDescriptorContainer\n+    >>> class TypedAttribute(OrderedDescriptor):\n+    ...     \\\"\\\"\\\"\n+    ...     Attributes that may only be assigned objects of a specific type,\n+    ...     or subclasses thereof.  For some reason we care about their order.\n+    ...     \\\"\\\"\\\"\n+    ...\n+    ...     _class_attribute_ = 'typed_attributes'\n+    ...     _name_attribute_ = 'name'\n+    ...     # A default name so that instances not attached to a class can\n+    ...     # still be repr'd; useful for debugging\n+    ...     name = '<unbound>'\n+    ...\n+    ...     def __init__(self, type):\n+    ...         # Make sure not to forget to call the super __init__\n+    ...         super().__init__()\n+    ...         self.type = type\n+    ...\n+    ...     def __get__(self, obj, objtype=None):\n+    ...         if obj is None:\n+    ...             return self\n+    ...         if self.name in obj.__dict__:\n+    ...             return obj.__dict__[self.name]\n+    ...         else:\n+    ...             raise AttributeError(self.name)\n+    ...\n+    ...     def __set__(self, obj, value):\n+    ...         if not isinstance(value, self.type):\n+    ...             raise ValueError('{0}.{1} must be of type {2!r}'.format(\n+    ...                 obj.__class__.__name__, self.name, self.type))\n+    ...         obj.__dict__[self.name] = value\n+    ...\n+    ...     def __delete__(self, obj):\n+    ...         if self.name in obj.__dict__:\n+    ...             del obj.__dict__[self.name]\n+    ...         else:\n+    ...             raise AttributeError(self.name)\n+    ...\n+    ...     def __repr__(self):\n+    ...         if isinstance(self.type, tuple) and len(self.type) > 1:\n+    ...             typestr = '({0})'.format(\n+    ...                 ', '.join(t.__name__ for t in self.type))\n+    ...         else:\n+    ...             typestr = self.type.__name__\n+    ...         return '<{0}(name={1}, type={2})>'.format(\n+    ...                 self.__class__.__name__, self.name, typestr)\n+    ...\n+\n+    Now let's create an example class that uses this ``TypedAttribute``::\n+\n+        >>> class Point2D(metaclass=OrderedDescriptorContainer):\n+        ...     x = TypedAttribute((float, int))\n+        ...     y = TypedAttribute((float, int))\n+        ...\n+        ...     def __init__(self, x, y):\n+        ...         self.x, self.y = x, y\n+        ...\n+        >>> p1 = Point2D(1.0, 2.0)\n+        >>> p1.x\n+        1.0\n+        >>> p1.y\n+        2.0\n+        >>> p2 = Point2D('a', 'b')  # doctest: +IGNORE_EXCEPTION_DETAIL\n+        Traceback (most recent call last):\n+            ...\n+        ValueError: Point2D.x must be of type (float, int>)\n+\n+    We see that ``TypedAttribute`` works more or less as advertised, but\n+    there's nothing special about that.  Let's see what\n+    `OrderedDescriptorContainer` did for us::\n+\n+        >>> Point2D.typed_attributes\n+        OrderedDict([('x', <TypedAttribute(name=x, type=(float, int))>),\n+        ('y', <TypedAttribute(name=y, type=(float, int))>)])\n+\n+    If we create a subclass, it does *not* by default add inherited descriptors\n+    to ``typed_attributes``::\n+\n+        >>> class Point3D(Point2D):\n+        ...     z = TypedAttribute((float, int))\n+        ...\n+        >>> Point3D.typed_attributes\n+        OrderedDict([('z', <TypedAttribute(name=z, type=(float, int))>)])\n+\n+    However, if we specify ``_inherit_descriptors_`` from ``Point2D`` then\n+    it will do so::\n+\n+        >>> class Point3D(Point2D):\n+        ...     _inherit_descriptors_ = (TypedAttribute,)\n+        ...     z = TypedAttribute((float, int))\n+        ...\n+        >>> Point3D.typed_attributes\n+        OrderedDict([('x', <TypedAttribute(name=x, type=(float, int))>),\n+        ('y', <TypedAttribute(name=y, type=(float, int))>),\n+        ('z', <TypedAttribute(name=z, type=(float, int))>)])\n+\n+    .. note::\n+\n+        Hopefully it is clear from these examples that this construction\n+        also allows a class of type `OrderedDescriptorContainer` to use\n+        multiple different `OrderedDescriptor` classes simultaneously.\n+    \"\"\"\n+\n+    _inherit_descriptors_ = ()\n+\n+    def __init__(cls, cls_name, bases, members):\n+        descriptors = defaultdict(list)\n+        seen = set()\n+        inherit_descriptors = ()\n+        descr_bases = {}\n+\n+        for mro_cls in cls.__mro__:\n+            for name, obj in mro_cls.__dict__.items():\n+                if name in seen:\n+                    # Checks if we've already seen an attribute of the given\n+                    # name (if so it will override anything of the same name in\n+                    # any base class)\n+                    continue\n+\n+                seen.add(name)\n+\n+                if (not isinstance(obj, OrderedDescriptor) or\n+                        (inherit_descriptors and\n+                            not isinstance(obj, inherit_descriptors))):\n+                    # The second condition applies when checking any\n+                    # subclasses, to see if we can inherit any descriptors of\n+                    # the given type from subclasses (by default inheritance is\n+                    # disabled unless the class has _inherit_descriptors_\n+                    # defined)\n+                    continue\n+\n+                if obj._name_attribute_ is not None:\n+                    setattr(obj, obj._name_attribute_, name)\n+\n+                # Don't just use the descriptor's class directly; instead go\n+                # through its MRO and find the class on which _class_attribute_\n+                # is defined directly.  This way subclasses of some\n+                # OrderedDescriptor *may* override _class_attribute_ and have\n+                # its own _class_attribute_, but by default all subclasses of\n+                # some OrderedDescriptor are still grouped together\n+                # TODO: It might be worth clarifying this in the docs\n+                if obj.__class__ not in descr_bases:\n+                    for obj_cls_base in obj.__class__.__mro__:\n+                        if '_class_attribute_' in obj_cls_base.__dict__:\n+                            descr_bases[obj.__class__] = obj_cls_base\n+                            descriptors[obj_cls_base].append((obj, name))\n+                            break\n+                else:\n+                    # Make sure to put obj first for sorting purposes\n+                    obj_cls_base = descr_bases[obj.__class__]\n+                    descriptors[obj_cls_base].append((obj, name))\n+\n+            if not getattr(mro_cls, '_inherit_descriptors_', False):\n+                # If _inherit_descriptors_ is undefined then we don't inherit\n+                # any OrderedDescriptors from any of the base classes, and\n+                # there's no reason to continue through the MRO\n+                break\n+            else:\n+                inherit_descriptors = mro_cls._inherit_descriptors_\n+\n+        for descriptor_cls, instances in descriptors.items():\n+            instances.sort()\n+            instances = OrderedDict((key, value) for value, key in instances)\n+            setattr(cls, descriptor_cls._class_attribute_, instances)\n+\n+        super().__init__(cls_name, bases, members)\n+\n+\n+LOCALE_LOCK = threading.Lock()\n+\n+\n+@contextmanager\n+def set_locale(name):\n+    \"\"\"\n+    Context manager to temporarily set the locale to ``name``.\n+\n+    An example is setting locale to \"C\" so that the C strtod()\n+    function will use \".\" as the decimal point to enable consistent\n+    numerical string parsing.\n+\n+    Note that one cannot nest multiple set_locale() context manager\n+    statements as this causes a threading lock.\n+\n+    This code taken from https://stackoverflow.com/questions/18593661/how-do-i-strftime-a-date-object-in-a-different-locale.\n+\n+    Parameters\n+    ==========\n+    name : str\n+        Locale name, e.g. \"C\" or \"fr_FR\".\n+    \"\"\"\n+    name = str(name)\n+\n+    with LOCALE_LOCK:\n+        saved = locale.setlocale(locale.LC_ALL)\n+        if saved == name:\n+            # Don't do anything if locale is already the requested locale\n+            yield\n+        else:\n+            try:\n+                locale.setlocale(locale.LC_ALL, name)\n+                yield\n+            finally:\n+                locale.setlocale(locale.LC_ALL, saved)\n+\n+\n+class ShapedLikeNDArray(metaclass=abc.ABCMeta):\n+    \"\"\"Mixin class to provide shape-changing methods.\n+\n+    The class proper is assumed to have some underlying data, which are arrays\n+    or array-like structures. It must define a ``shape`` property, which gives\n+    the shape of those data, as well as an ``_apply`` method that creates a new\n+    instance in which a `~numpy.ndarray` method has been applied to those.\n+\n+    Furthermore, for consistency with `~numpy.ndarray`, it is recommended to\n+    define a setter for the ``shape`` property, which, like the\n+    `~numpy.ndarray.shape` property allows in-place reshaping the internal data\n+    (and, unlike the ``reshape`` method raises an exception if this is not\n+    possible).\n+\n+    This class also defines default implementations for ``ndim`` and ``size``\n+    properties, calculating those from the ``shape``.  These can be overridden\n+    by subclasses if there are faster ways to obtain those numbers.\n+\n+    \"\"\"\n+\n+    # Note to developers: if new methods are added here, be sure to check that\n+    # they work properly with the classes that use this, such as Time and\n+    # BaseRepresentation, i.e., look at their ``_apply`` methods and add\n+    # relevant tests.  This is particularly important for methods that imply\n+    # copies rather than views of data (see the special-case treatment of\n+    # 'flatten' in Time).\n+\n+    @property\n+    @abc.abstractmethod\n+    def shape(self):\n+        \"\"\"The shape of the instance and underlying arrays.\"\"\"\n+\n+    @abc.abstractmethod\n+    def _apply(method, *args, **kwargs):\n+        \"\"\"Create a new instance, with ``method`` applied to underlying data.\n+\n+        The method is any of the shape-changing methods for `~numpy.ndarray`\n+        (``reshape``, ``swapaxes``, etc.), as well as those picking particular\n+        elements (``__getitem__``, ``take``, etc.). It will be applied to the\n+        underlying arrays (e.g., ``jd1`` and ``jd2`` in `~astropy.time.Time`),\n+        with the results used to create a new instance.\n+\n+        Parameters\n+        ----------\n+        method : str\n+            Method to be applied to the instance's internal data arrays.\n+        args : tuple\n+            Any positional arguments for ``method``.\n+        kwargs : dict\n+            Any keyword arguments for ``method``.\n+\n+        \"\"\"\n+\n+    @property\n+    def ndim(self):\n+        \"\"\"The number of dimensions of the instance and underlying arrays.\"\"\"\n+        return len(self.shape)\n+\n+    @property\n+    def size(self):\n+        \"\"\"The size of the object, as calculated from its shape.\"\"\"\n+        size = 1\n+        for sh in self.shape:\n+            size *= sh\n+        return size\n+\n+    @property\n+    def isscalar(self):\n+        return self.shape == ()\n+\n+    def __len__(self):\n+        if self.isscalar:\n+            raise TypeError(\"Scalar {0!r} object has no len()\"\n+                            .format(self.__class__.__name__))\n+        return self.shape[0]\n+\n+    def __bool__(self):\n+        \"\"\"Any instance should evaluate to True, except when it is empty.\"\"\"\n+        return self.size > 0\n+\n+    def __getitem__(self, item):\n+        try:\n+            return self._apply('__getitem__', item)\n+        except IndexError:\n+            if self.isscalar:\n+                raise TypeError('scalar {0!r} object is not subscriptable.'\n+                                .format(self.__class__.__name__))\n+            else:\n+                raise\n+\n+    def __iter__(self):\n+        if self.isscalar:\n+            raise TypeError('scalar {0!r} object is not iterable.'\n+                            .format(self.__class__.__name__))\n+\n+        # We cannot just write a generator here, since then the above error\n+        # would only be raised once we try to use the iterator, rather than\n+        # upon its definition using iter(self).\n+        def self_iter():\n+            for idx in range(len(self)):\n+                yield self[idx]\n+\n+        return self_iter()\n+\n+    def copy(self, *args, **kwargs):\n+        \"\"\"Return an instance containing copies of the internal data.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.copy`.\n+        \"\"\"\n+        return self._apply('copy', *args, **kwargs)\n+\n+    def reshape(self, *args, **kwargs):\n+        \"\"\"Returns an instance containing the same data with a new shape.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.reshape`.  Note that it is\n+        not always possible to change the shape of an array without copying the\n+        data (see :func:`~numpy.reshape` documentation). If you want an error\n+        to be raise if the data is copied, you should assign the new shape to\n+        the shape attribute (note: this may not be implemented for all classes\n+        using ``ShapedLikeNDArray``).\n+        \"\"\"\n+        return self._apply('reshape', *args, **kwargs)\n+\n+    def ravel(self, *args, **kwargs):\n+        \"\"\"Return an instance with the array collapsed into one dimension.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.ravel`. Note that it is\n+        not always possible to unravel an array without copying the data.\n+        If you want an error to be raise if the data is copied, you should\n+        should assign shape ``(-1,)`` to the shape attribute.\n+        \"\"\"\n+        return self._apply('ravel', *args, **kwargs)\n+\n+    def flatten(self, *args, **kwargs):\n+        \"\"\"Return a copy with the array collapsed into one dimension.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.flatten`.\n+        \"\"\"\n+        return self._apply('flatten', *args, **kwargs)\n+\n+    def transpose(self, *args, **kwargs):\n+        \"\"\"Return an instance with the data transposed.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.transpose`.  All internal\n+        data are views of the data of the original.\n+        \"\"\"\n+        return self._apply('transpose', *args, **kwargs)\n+\n+    @property\n+    def T(self):\n+        \"\"\"Return an instance with the data transposed.\n+\n+        Parameters are as for :attr:`~numpy.ndarray.T`.  All internal\n+        data are views of the data of the original.\n+        \"\"\"\n+        if self.ndim < 2:\n+            return self\n+        else:\n+            return self.transpose()\n+\n+    def swapaxes(self, *args, **kwargs):\n+        \"\"\"Return an instance with the given axes interchanged.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.swapaxes`:\n+        ``axis1, axis2``.  All internal data are views of the data of the\n+        original.\n+        \"\"\"\n+        return self._apply('swapaxes', *args, **kwargs)\n+\n+    def diagonal(self, *args, **kwargs):\n+        \"\"\"Return an instance with the specified diagonals.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.diagonal`.  All internal\n+        data are views of the data of the original.\n+        \"\"\"\n+        return self._apply('diagonal', *args, **kwargs)\n+\n+    def squeeze(self, *args, **kwargs):\n+        \"\"\"Return an instance with single-dimensional shape entries removed\n+\n+        Parameters are as for :meth:`~numpy.ndarray.squeeze`.  All internal\n+        data are views of the data of the original.\n+        \"\"\"\n+        return self._apply('squeeze', *args, **kwargs)\n+\n+    def take(self, indices, axis=None, mode='raise'):\n+        \"\"\"Return a new instance formed from the elements at the given indices.\n+\n+        Parameters are as for :meth:`~numpy.ndarray.take`, except that,\n+        obviously, no output array can be given.\n+        \"\"\"\n+        return self._apply('take', indices, axis=axis, mode=mode)\n+\n+\n+class IncompatibleShapeError(ValueError):\n+    def __init__(self, shape_a, shape_a_idx, shape_b, shape_b_idx):\n+        super().__init__(shape_a, shape_a_idx, shape_b, shape_b_idx)\n+\n+\n+def check_broadcast(*shapes):\n+    \"\"\"\n+    Determines whether two or more Numpy arrays can be broadcast with each\n+    other based on their shape tuple alone.\n+\n+    Parameters\n+    ----------\n+    *shapes : tuple\n+        All shapes to include in the comparison.  If only one shape is given it\n+        is passed through unmodified.  If no shapes are given returns an empty\n+        `tuple`.\n+\n+    Returns\n+    -------\n+    broadcast : `tuple`\n+        If all shapes are mutually broadcastable, returns a tuple of the full\n+        broadcast shape.\n+    \"\"\"\n+\n+    if len(shapes) == 0:\n+        return ()\n+    elif len(shapes) == 1:\n+        return shapes[0]\n+\n+    reversed_shapes = (reversed(shape) for shape in shapes)\n+\n+    full_shape = []\n+\n+    for dims in zip_longest(*reversed_shapes, fillvalue=1):\n+        max_dim = 1\n+        max_dim_idx = None\n+        for idx, dim in enumerate(dims):\n+            if dim == 1:\n+                continue\n+\n+            if max_dim == 1:\n+                # The first dimension of size greater than 1\n+                max_dim = dim\n+                max_dim_idx = idx\n+            elif dim != max_dim:\n+                raise IncompatibleShapeError(\n+                    shapes[max_dim_idx], max_dim_idx, shapes[idx], idx)\n+\n+        full_shape.append(max_dim)\n+\n+    return tuple(full_shape[::-1])\n+\n+\n+def dtype_bytes_or_chars(dtype):\n+    \"\"\"\n+    Parse the number out of a dtype.str value like '<U5' or '<f8'.\n+\n+    See #5819 for discussion on the need for this function for getting\n+    the number of characters corresponding to a string dtype.\n+\n+    Parameters\n+    ----------\n+    dtype : numpy dtype object\n+        Input dtype\n+\n+    Returns\n+    -------\n+    bytes_or_chars : int or None\n+        Bits (for numeric types) or characters (for string types)\n+    \"\"\"\n+    match = re.search(r'(\\d+)$', dtype.str)\n+    out = int(match.group(1)) if match else None\n+    return out\n"
  },
  "django__django-7530": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-7530",
    "model_patch": "Updated makemigrations consistency check to call router.allow_migrate() with models specific to each app.\n\nChange:\n- In django/core/management/commands/makemigrations.py, the iteration over models for the consistency check now uses:\n    for model in apps.get_app_config(app_label).get_models()\n  instead of:\n    for model in apps.get_models(app_label)\n\nThis ensures allow_migrate() is called with valid (app_label, model_name) pairs for each app, avoiding incorrect cross-app model combinations.\n\nNo changes detected to generate a patch."
  },
  "django__django-16662": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16662",
    "model_patch": "diff --git a/django/db/migrations/writer.py b/django/db/migrations/writer.py\nindex a59f0c8dcb..1e12af6cd5 100644\n--- a/django/db/migrations/writer.py\n+++ b/django/db/migrations/writer.py\n@@ -173,10 +173,6 @@ class MigrationWriter:\n         else:\n             imports.add(\"from django.db import migrations\")\n \n-        # Sort imports by the package / module to be imported (the part after\n-        # \"from\" in \"from ... import ...\" or after \"import\" in \"import ...\").\n-        sorted_imports = sorted(imports, key=lambda i: i.split()[1])\n-        items[\"imports\"] = \"\\n\".join(sorted_imports) + \"\\n\" if imports else \"\"\n         if migration_imports:\n             items[\"imports\"] += (\n                 \"\\n\\n# Functions from the following migrations need manual \"\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..1e12af6cd5\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,307 @@\n+import os\n+import re\n+from importlib import import_module\n+\n+from django import get_version\n+from django.apps import apps\n+\n+# SettingsReference imported for backwards compatibility in Django 2.2.\n+from django.conf import SettingsReference  # NOQA\n+from django.db import migrations\n+from django.db.migrations.loader import MigrationLoader\n+from django.db.migrations.serializer import Serializer, serializer_factory\n+from django.utils.inspect import get_func_args\n+from django.utils.module_loading import module_dir\n+from django.utils.timezone import now\n+\n+\n+class OperationWriter:\n+    def __init__(self, operation, indentation=2):\n+        self.operation = operation\n+        self.buff = []\n+        self.indentation = indentation\n+\n+    def serialize(self):\n+        def _write(_arg_name, _arg_value):\n+            if _arg_name in self.operation.serialization_expand_args and isinstance(\n+                _arg_value, (list, tuple, dict)\n+            ):\n+                if isinstance(_arg_value, dict):\n+                    self.feed(\"%s={\" % _arg_name)\n+                    self.indent()\n+                    for key, value in _arg_value.items():\n+                        key_string, key_imports = MigrationWriter.serialize(key)\n+                        arg_string, arg_imports = MigrationWriter.serialize(value)\n+                        args = arg_string.splitlines()\n+                        if len(args) > 1:\n+                            self.feed(\"%s: %s\" % (key_string, args[0]))\n+                            for arg in args[1:-1]:\n+                                self.feed(arg)\n+                            self.feed(\"%s,\" % args[-1])\n+                        else:\n+                            self.feed(\"%s: %s,\" % (key_string, arg_string))\n+                        imports.update(key_imports)\n+                        imports.update(arg_imports)\n+                    self.unindent()\n+                    self.feed(\"},\")\n+                else:\n+                    self.feed(\"%s=[\" % _arg_name)\n+                    self.indent()\n+                    for item in _arg_value:\n+                        arg_string, arg_imports = MigrationWriter.serialize(item)\n+                        args = arg_string.splitlines()\n+                        if len(args) > 1:\n+                            for arg in args[:-1]:\n+                                self.feed(arg)\n+                            self.feed(\"%s,\" % args[-1])\n+                        else:\n+                            self.feed(\"%s,\" % arg_string)\n+                        imports.update(arg_imports)\n+                    self.unindent()\n+                    self.feed(\"],\")\n+            else:\n+                arg_string, arg_imports = MigrationWriter.serialize(_arg_value)\n+                args = arg_string.splitlines()\n+                if len(args) > 1:\n+                    self.feed(\"%s=%s\" % (_arg_name, args[0]))\n+                    for arg in args[1:-1]:\n+                        self.feed(arg)\n+                    self.feed(\"%s,\" % args[-1])\n+                else:\n+                    self.feed(\"%s=%s,\" % (_arg_name, arg_string))\n+                imports.update(arg_imports)\n+\n+        imports = set()\n+        name, args, kwargs = self.operation.deconstruct()\n+        operation_args = get_func_args(self.operation.__init__)\n+\n+        # See if this operation is in django.db.migrations. If it is,\n+        # We can just use the fact we already have that imported,\n+        # otherwise, we need to add an import for the operation class.\n+        if getattr(migrations, name, None) == self.operation.__class__:\n+            self.feed(\"migrations.%s(\" % name)\n+        else:\n+            imports.add(\"import %s\" % (self.operation.__class__.__module__))\n+            self.feed(\"%s.%s(\" % (self.operation.__class__.__module__, name))\n+\n+        self.indent()\n+\n+        for i, arg in enumerate(args):\n+            arg_value = arg\n+            arg_name = operation_args[i]\n+            _write(arg_name, arg_value)\n+\n+        i = len(args)\n+        # Only iterate over remaining arguments\n+        for arg_name in operation_args[i:]:\n+            if arg_name in kwargs:  # Don't sort to maintain signature order\n+                arg_value = kwargs[arg_name]\n+                _write(arg_name, arg_value)\n+\n+        self.unindent()\n+        self.feed(\"),\")\n+        return self.render(), imports\n+\n+    def indent(self):\n+        self.indentation += 1\n+\n+    def unindent(self):\n+        self.indentation -= 1\n+\n+    def feed(self, line):\n+        self.buff.append(\" \" * (self.indentation * 4) + line)\n+\n+    def render(self):\n+        return \"\\n\".join(self.buff)\n+\n+\n+class MigrationWriter:\n+    \"\"\"\n+    Take a Migration instance and is able to produce the contents\n+    of the migration file from it.\n+    \"\"\"\n+\n+    def __init__(self, migration, include_header=True):\n+        self.migration = migration\n+        self.include_header = include_header\n+        self.needs_manual_porting = False\n+\n+    def as_string(self):\n+        \"\"\"Return a string of the file contents.\"\"\"\n+        items = {\n+            \"replaces_str\": \"\",\n+            \"initial_str\": \"\",\n+        }\n+\n+        imports = set()\n+\n+        # Deconstruct operations\n+        operations = []\n+        for operation in self.migration.operations:\n+            operation_string, operation_imports = OperationWriter(operation).serialize()\n+            imports.update(operation_imports)\n+            operations.append(operation_string)\n+        items[\"operations\"] = \"\\n\".join(operations) + \"\\n\" if operations else \"\"\n+\n+        # Format dependencies and write out swappable dependencies right\n+        dependencies = []\n+        for dependency in self.migration.dependencies:\n+            if dependency[0] == \"__setting__\":\n+                dependencies.append(\n+                    \"        migrations.swappable_dependency(settings.%s),\"\n+                    % dependency[1]\n+                )\n+                imports.add(\"from django.conf import settings\")\n+            else:\n+                dependencies.append(\"        %s,\" % self.serialize(dependency)[0])\n+        items[\"dependencies\"] = \"\\n\".join(dependencies) + \"\\n\" if dependencies else \"\"\n+\n+        # Format imports nicely, swapping imports of functions from migration files\n+        # for comments\n+        migration_imports = set()\n+        for line in list(imports):\n+            if re.match(r\"^import (.*)\\.\\d+[^\\s]*$\", line):\n+                migration_imports.add(line.split(\"import\")[1].strip())\n+                imports.remove(line)\n+                self.needs_manual_porting = True\n+\n+        # django.db.migrations is always used, but models import may not be.\n+        # If models import exists, merge it with migrations import.\n+        if \"from django.db import models\" in imports:\n+            imports.discard(\"from django.db import models\")\n+            imports.add(\"from django.db import migrations, models\")\n+        else:\n+            imports.add(\"from django.db import migrations\")\n+\n+        if migration_imports:\n+            items[\"imports\"] += (\n+                \"\\n\\n# Functions from the following migrations need manual \"\n+                \"copying.\\n# Move them and any dependencies into this file, \"\n+                \"then update the\\n# RunPython operations to refer to the local \"\n+                \"versions:\\n# %s\"\n+            ) % \"\\n# \".join(sorted(migration_imports))\n+        # If there's a replaces, make a string for it\n+        if self.migration.replaces:\n+            items[\"replaces_str\"] = (\n+                \"\\n    replaces = %s\\n\" % self.serialize(self.migration.replaces)[0]\n+            )\n+        # Hinting that goes into comment\n+        if self.include_header:\n+            items[\"migration_header\"] = MIGRATION_HEADER_TEMPLATE % {\n+                \"version\": get_version(),\n+                \"timestamp\": now().strftime(\"%Y-%m-%d %H:%M\"),\n+            }\n+        else:\n+            items[\"migration_header\"] = \"\"\n+\n+        if self.migration.initial:\n+            items[\"initial_str\"] = \"\\n    initial = True\\n\"\n+\n+        return MIGRATION_TEMPLATE % items\n+\n+    @property\n+    def basedir(self):\n+        migrations_package_name, _ = MigrationLoader.migrations_module(\n+            self.migration.app_label\n+        )\n+\n+        if migrations_package_name is None:\n+            raise ValueError(\n+                \"Django can't create migrations for app '%s' because \"\n+                \"migrations have been disabled via the MIGRATION_MODULES \"\n+                \"setting.\" % self.migration.app_label\n+            )\n+\n+        # See if we can import the migrations module directly\n+        try:\n+            migrations_module = import_module(migrations_package_name)\n+        except ImportError:\n+            pass\n+        else:\n+            try:\n+                return module_dir(migrations_module)\n+            except ValueError:\n+                pass\n+\n+        # Alright, see if it's a direct submodule of the app\n+        app_config = apps.get_app_config(self.migration.app_label)\n+        (\n+            maybe_app_name,\n+            _,\n+            migrations_package_basename,\n+        ) = migrations_package_name.rpartition(\".\")\n+        if app_config.name == maybe_app_name:\n+            return os.path.join(app_config.path, migrations_package_basename)\n+\n+        # In case of using MIGRATION_MODULES setting and the custom package\n+        # doesn't exist, create one, starting from an existing package\n+        existing_dirs, missing_dirs = migrations_package_name.split(\".\"), []\n+        while existing_dirs:\n+            missing_dirs.insert(0, existing_dirs.pop(-1))\n+            try:\n+                base_module = import_module(\".\".join(existing_dirs))\n+            except (ImportError, ValueError):\n+                continue\n+            else:\n+                try:\n+                    base_dir = module_dir(base_module)\n+                except ValueError:\n+                    continue\n+                else:\n+                    break\n+        else:\n+            raise ValueError(\n+                \"Could not locate an appropriate location to create \"\n+                \"migrations package %s. Make sure the toplevel \"\n+                \"package exists and can be imported.\" % migrations_package_name\n+            )\n+\n+        final_dir = os.path.join(base_dir, *missing_dirs)\n+        os.makedirs(final_dir, exist_ok=True)\n+        for missing_dir in missing_dirs:\n+            base_dir = os.path.join(base_dir, missing_dir)\n+            with open(os.path.join(base_dir, \"__init__.py\"), \"w\"):\n+                pass\n+\n+        return final_dir\n+\n+    @property\n+    def filename(self):\n+        return \"%s.py\" % self.migration.name\n+\n+    @property\n+    def path(self):\n+        return os.path.join(self.basedir, self.filename)\n+\n+    @classmethod\n+    def serialize(cls, value):\n+        return serializer_factory(value).serialize()\n+\n+    @classmethod\n+    def register_serializer(cls, type_, serializer):\n+        Serializer.register(type_, serializer)\n+\n+    @classmethod\n+    def unregister_serializer(cls, type_):\n+        Serializer.unregister(type_)\n+\n+\n+MIGRATION_HEADER_TEMPLATE = \"\"\"\\\n+# Generated by Django %(version)s on %(timestamp)s\n+\n+\"\"\"\n+\n+\n+MIGRATION_TEMPLATE = \"\"\"\\\n+%(migration_header)s%(imports)s\n+\n+class Migration(migrations.Migration):\n+%(replaces_str)s%(initial_str)s\n+    dependencies = [\n+%(dependencies)s\\\n+    ]\n+\n+    operations = [\n+%(operations)s\\\n+    ]\n+\"\"\"\n"
  },
  "django__django-10973": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-10973",
    "model_patch": "diff --git a/django/db/backends/postgresql/client.py b/django/db/backends/postgresql/client.py\nindex 7fca6eff30..e69de29bb2 100644\n--- a/django/db/backends/postgresql/client.py\n+++ b/django/db/backends/postgresql/client.py\n@@ -1,71 +0,0 @@\n-import os\n-import signal\n-import subprocess\n-\n-from django.core.files.temp import NamedTemporaryFile\n-from django.db.backends.base.client import BaseDatabaseClient\n-\n-\n-def _escape_pgpass(txt):\n-    \"\"\"\n-    Escape a fragment of a PostgreSQL .pgpass file.\n-    \"\"\"\n-    return txt.replace('\\\\', '\\\\\\\\').replace(':', '\\\\:')\n-\n-\n-class DatabaseClient(BaseDatabaseClient):\n-    executable_name = 'psql'\n-\n-    @classmethod\n-    def runshell_db(cls, conn_params):\n-        args = [cls.executable_name]\n-\n-        host = conn_params.get('host', '')\n-        port = conn_params.get('port', '')\n-        dbname = conn_params.get('database', '')\n-        user = conn_params.get('user', '')\n-        passwd = conn_params.get('password', '')\n-\n-        if user:\n-            args += ['-U', user]\n-        if host:\n-            args += ['-h', host]\n-        if port:\n-            args += ['-p', str(port)]\n-        args += [dbname]\n-\n-        temp_pgpass = None\n-        sigint_handler = signal.getsignal(signal.SIGINT)\n-        try:\n-            if passwd:\n-                # Create temporary .pgpass file.\n-                temp_pgpass = NamedTemporaryFile(mode='w+')\n-                try:\n-                    print(\n-                        _escape_pgpass(host) or '*',\n-                        str(port) or '*',\n-                        _escape_pgpass(dbname) or '*',\n-                        _escape_pgpass(user) or '*',\n-                        _escape_pgpass(passwd),\n-                        file=temp_pgpass,\n-                        sep=':',\n-                        flush=True,\n-                    )\n-                    os.environ['PGPASSFILE'] = temp_pgpass.name\n-                except UnicodeEncodeError:\n-                    # If the current locale can't encode the data, let the\n-                    # user input the password manually.\n-                    pass\n-            # Allow SIGINT to pass to psql to abort queries.\n-            signal.signal(signal.SIGINT, signal.SIG_IGN)\n-            subprocess.check_call(args)\n-        finally:\n-            # Restore the original SIGINT handler.\n-            signal.signal(signal.SIGINT, sigint_handler)\n-            if temp_pgpass:\n-                temp_pgpass.close()\n-                if 'PGPASSFILE' in os.environ:  # unit tests need cleanup\n-                    del os.environ['PGPASSFILE']\n-\n-    def runshell(self):\n-        DatabaseClient.runshell_db(self.connection.get_connection_params())\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..e69de29bb2\n"
  },
  "django__django-12406": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-12406",
    "model_patch": "diff --git a/django/forms/models.py b/django/forms/models.py\nindex dafc11f995..d791abd26a 100644\n--- a/django/forms/models.py\n+++ b/django/forms/models.py\n@@ -159,6 +159,17 @@ def fields_for_model(model, fields=None, exclude=None, widgets=None,\n         kwargs = {}\n         if widgets and f.name in widgets:\n             kwargs['widget'] = widgets[f.name]\n+            # RadioSelect widgets have an inherent unfilled state. If the\n+            # model field doesn't allow blank values (blank=False), avoid\n+            # adding an empty_label (the \"---------\" choice) which would\n+            # otherwise render as a selected, but misleading, blank option.\n+            try:\n+                from django.forms.widgets import RadioSelect as _RadioSelect\n+            except Exception:\n+                _RadioSelect = None\n+            if _RadioSelect is not None and isinstance(kwargs['widget'], _RadioSelect) and not getattr(f, 'blank', True):\n+                # Ensure ModelChoiceField will not include an empty_label.\n+                kwargs.setdefault('empty_label', None)\n         if localized_fields == ALL_FIELDS or (localized_fields and f.name in localized_fields):\n             kwargs['localize'] = True\n         if labels and f.name in labels:\n"
  },
  "sphinx-doc__sphinx-9230": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9230",
    "model_patch": "diff --git a/sphinx/ext/napoleon/docstring.py b/sphinx/ext/napoleon/docstring.py\nindex d8cb75a5f..208b01e59 100644\n--- a/sphinx/ext/napoleon/docstring.py\n+++ b/sphinx/ext/napoleon/docstring.py\n@@ -256,31 +256,6 @@ class GoogleDocstring:\n             line = self._line_iter.peek()\n         return lines\n \n-    def _consume_field(self, parse_type: bool = True, prefer_type: bool = False\n-                       ) -> Tuple[str, str, List[str]]:\n-        line = next(self._line_iter)\n-\n-        before, colon, after = self._partition_field_on_colon(line)\n-        _name, _type, _desc = before, '', after\n-\n-        if parse_type:\n-            match = _google_typed_arg_regex.match(before)\n-            if match:\n-                _name = match.group(1).strip()\n-                _type = match.group(2)\n-\n-        _name = self._escape_args_and_kwargs(_name)\n-\n-        if prefer_type and not _type:\n-            _type, _name = _name, _type\n-\n-        if _type and self._config.napoleon_preprocess_types:\n-            _type = _convert_type_spec(_type, self._config.napoleon_type_aliases or {})\n-\n-        indent = self._get_indent(line) + 1\n-        _descs = [_desc] + self._dedent(self._consume_indented_block(indent))\n-        _descs = self.__class__(_descs, self._config).lines()\n-        return _name, _type, _descs\n \n     def _consume_fields(self, parse_type: bool = True, prefer_type: bool = False,\n                         multiple: bool = False) -> List[Tuple[str, str, List[str]]]:\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 000000000..208b01e59\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,1313 @@\n+\"\"\"\n+    sphinx.ext.napoleon.docstring\n+    ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+\n+    Classes for docstring parsing and formatting.\n+\n+\n+    :copyright: Copyright 2007-2021 by the Sphinx team, see AUTHORS.\n+    :license: BSD, see LICENSE for details.\n+\"\"\"\n+\n+import collections\n+import inspect\n+import re\n+from functools import partial\n+from typing import Any, Callable, Dict, List, Tuple, Type, Union\n+\n+from sphinx.application import Sphinx\n+from sphinx.config import Config as SphinxConfig\n+from sphinx.ext.napoleon.iterators import modify_iter\n+from sphinx.locale import _, __\n+from sphinx.util import logging\n+from sphinx.util.inspect import stringify_annotation\n+from sphinx.util.typing import get_type_hints\n+\n+logger = logging.getLogger(__name__)\n+\n+_directive_regex = re.compile(r'\\.\\. \\S+::')\n+_google_section_regex = re.compile(r'^(\\s|\\w)+:\\s*$')\n+_google_typed_arg_regex = re.compile(r'(.+?)\\(\\s*(.*[^\\s]+)\\s*\\)')\n+_numpy_section_regex = re.compile(r'^[=\\-`:\\'\"~^_*+#<>]{2,}\\s*$')\n+_single_colon_regex = re.compile(r'(?<!:):(?!:)')\n+_xref_or_code_regex = re.compile(\n+    r'((?::(?:[a-zA-Z0-9]+[\\-_+:.])*[a-zA-Z0-9]+:`.+?`)|'\n+    r'(?:``.+?``))')\n+_xref_regex = re.compile(\n+    r'(?:(?::(?:[a-zA-Z0-9]+[\\-_+:.])*[a-zA-Z0-9]+:)?`.+?`)'\n+)\n+_bullet_list_regex = re.compile(r'^(\\*|\\+|\\-)(\\s+\\S|\\s*$)')\n+_enumerated_list_regex = re.compile(\n+    r'^(?P<paren>\\()?'\n+    r'(\\d+|#|[ivxlcdm]+|[IVXLCDM]+|[a-zA-Z])'\n+    r'(?(paren)\\)|\\.)(\\s+\\S|\\s*$)')\n+_token_regex = re.compile(\n+    r\"(,\\sor\\s|\\sor\\s|\\sof\\s|:\\s|\\sto\\s|,\\sand\\s|\\sand\\s|,\\s\"\n+    r\"|[{]|[}]\"\n+    r'|\"(?:\\\\\"|[^\"])*\"'\n+    r\"|'(?:\\\\'|[^'])*')\"\n+)\n+_default_regex = re.compile(\n+    r\"^default[^_0-9A-Za-z].*$\",\n+)\n+_SINGLETONS = (\"None\", \"True\", \"False\", \"Ellipsis\")\n+\n+\n+def _convert_type_spec(_type: str, translations: Dict[str, str] = {}) -> str:\n+    \"\"\"Convert type specification to reference in reST.\"\"\"\n+    if _type in translations:\n+        return translations[_type]\n+    else:\n+        if _type == 'None':\n+            return ':obj:`None`'\n+        else:\n+            return ':class:`%s`' % _type\n+\n+    return _type\n+\n+\n+class GoogleDocstring:\n+    \"\"\"Convert Google style docstrings to reStructuredText.\n+\n+    Parameters\n+    ----------\n+    docstring : :obj:`str` or :obj:`list` of :obj:`str`\n+        The docstring to parse, given either as a string or split into\n+        individual lines.\n+    config: :obj:`sphinx.ext.napoleon.Config` or :obj:`sphinx.config.Config`\n+        The configuration settings to use. If not given, defaults to the\n+        config object on `app`; or if `app` is not given defaults to the\n+        a new :class:`sphinx.ext.napoleon.Config` object.\n+\n+\n+    Other Parameters\n+    ----------------\n+    app : :class:`sphinx.application.Sphinx`, optional\n+        Application object representing the Sphinx process.\n+    what : :obj:`str`, optional\n+        A string specifying the type of the object to which the docstring\n+        belongs. Valid values: \"module\", \"class\", \"exception\", \"function\",\n+        \"method\", \"attribute\".\n+    name : :obj:`str`, optional\n+        The fully qualified name of the object.\n+    obj : module, class, exception, function, method, or attribute\n+        The object to which the docstring belongs.\n+    options : :class:`sphinx.ext.autodoc.Options`, optional\n+        The options given to the directive: an object with attributes\n+        inherited_members, undoc_members, show_inheritance and noindex that\n+        are True if the flag option of same name was given to the auto\n+        directive.\n+\n+\n+    Example\n+    -------\n+    >>> from sphinx.ext.napoleon import Config\n+    >>> config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n+    >>> docstring = '''One line summary.\n+    ...\n+    ... Extended description.\n+    ...\n+    ... Args:\n+    ...   arg1(int): Description of `arg1`\n+    ...   arg2(str): Description of `arg2`\n+    ... Returns:\n+    ...   str: Description of return value.\n+    ... '''\n+    >>> print(GoogleDocstring(docstring, config))\n+    One line summary.\n+    <BLANKLINE>\n+    Extended description.\n+    <BLANKLINE>\n+    :param arg1: Description of `arg1`\n+    :type arg1: int\n+    :param arg2: Description of `arg2`\n+    :type arg2: str\n+    <BLANKLINE>\n+    :returns: Description of return value.\n+    :rtype: str\n+    <BLANKLINE>\n+\n+    \"\"\"\n+\n+    _name_rgx = re.compile(r\"^\\s*((?::(?P<role>\\S+):)?`(?P<name>~?[a-zA-Z0-9_.-]+)`|\"\n+                           r\" (?P<name2>~?[a-zA-Z0-9_.-]+))\\s*\", re.X)\n+\n+    def __init__(self, docstring: Union[str, List[str]], config: SphinxConfig = None,\n+                 app: Sphinx = None, what: str = '', name: str = '',\n+                 obj: Any = None, options: Any = None) -> None:\n+        self._config = config\n+        self._app = app\n+\n+        if not self._config:\n+            from sphinx.ext.napoleon import Config\n+            self._config = self._app.config if self._app else Config()  # type: ignore\n+\n+        if not what:\n+            if inspect.isclass(obj):\n+                what = 'class'\n+            elif inspect.ismodule(obj):\n+                what = 'module'\n+            elif callable(obj):\n+                what = 'function'\n+            else:\n+                what = 'object'\n+\n+        self._what = what\n+        self._name = name\n+        self._obj = obj\n+        self._opt = options\n+        if isinstance(docstring, str):\n+            lines = docstring.splitlines()\n+        else:\n+            lines = docstring\n+        self._line_iter = modify_iter(lines, modifier=lambda s: s.rstrip())\n+        self._parsed_lines: List[str] = []\n+        self._is_in_section = False\n+        self._section_indent = 0\n+        if not hasattr(self, '_directive_sections'):\n+            self._directive_sections: List[str] = []\n+        if not hasattr(self, '_sections'):\n+            self._sections: Dict[str, Callable] = {\n+                'args': self._parse_parameters_section,\n+                'arguments': self._parse_parameters_section,\n+                'attention': partial(self._parse_admonition, 'attention'),\n+                'attributes': self._parse_attributes_section,\n+                'caution': partial(self._parse_admonition, 'caution'),\n+                'danger': partial(self._parse_admonition, 'danger'),\n+                'error': partial(self._parse_admonition, 'error'),\n+                'example': self._parse_examples_section,\n+                'examples': self._parse_examples_section,\n+                'hint': partial(self._parse_admonition, 'hint'),\n+                'important': partial(self._parse_admonition, 'important'),\n+                'keyword args': self._parse_keyword_arguments_section,\n+                'keyword arguments': self._parse_keyword_arguments_section,\n+                'methods': self._parse_methods_section,\n+                'note': partial(self._parse_admonition, 'note'),\n+                'notes': self._parse_notes_section,\n+                'other parameters': self._parse_other_parameters_section,\n+                'parameters': self._parse_parameters_section,\n+                'receive': self._parse_receives_section,\n+                'receives': self._parse_receives_section,\n+                'return': self._parse_returns_section,\n+                'returns': self._parse_returns_section,\n+                'raise': self._parse_raises_section,\n+                'raises': self._parse_raises_section,\n+                'references': self._parse_references_section,\n+                'see also': self._parse_see_also_section,\n+                'tip': partial(self._parse_admonition, 'tip'),\n+                'todo': partial(self._parse_admonition, 'todo'),\n+                'warning': partial(self._parse_admonition, 'warning'),\n+                'warnings': partial(self._parse_admonition, 'warning'),\n+                'warn': self._parse_warns_section,\n+                'warns': self._parse_warns_section,\n+                'yield': self._parse_yields_section,\n+                'yields': self._parse_yields_section,\n+            }\n+\n+        self._load_custom_sections()\n+\n+        self._parse()\n+\n+    def __str__(self) -> str:\n+        \"\"\"Return the parsed docstring in reStructuredText format.\n+\n+        Returns\n+        -------\n+        unicode\n+            Unicode version of the docstring.\n+\n+        \"\"\"\n+        return '\\n'.join(self.lines())\n+\n+    def lines(self) -> List[str]:\n+        \"\"\"Return the parsed lines of the docstring in reStructuredText format.\n+\n+        Returns\n+        -------\n+        list(str)\n+            The lines of the docstring in a list.\n+\n+        \"\"\"\n+        return self._parsed_lines\n+\n+    def _consume_indented_block(self, indent: int = 1) -> List[str]:\n+        lines = []\n+        line = self._line_iter.peek()\n+        while(not self._is_section_break() and\n+              (not line or self._is_indented(line, indent))):\n+            lines.append(next(self._line_iter))\n+            line = self._line_iter.peek()\n+        return lines\n+\n+    def _consume_contiguous(self) -> List[str]:\n+        lines = []\n+        while (self._line_iter.has_next() and\n+               self._line_iter.peek() and\n+               not self._is_section_header()):\n+            lines.append(next(self._line_iter))\n+        return lines\n+\n+    def _consume_empty(self) -> List[str]:\n+        lines = []\n+        line = self._line_iter.peek()\n+        while self._line_iter.has_next() and not line:\n+            lines.append(next(self._line_iter))\n+            line = self._line_iter.peek()\n+        return lines\n+\n+\n+    def _consume_fields(self, parse_type: bool = True, prefer_type: bool = False,\n+                        multiple: bool = False) -> List[Tuple[str, str, List[str]]]:\n+        self._consume_empty()\n+        fields = []\n+        while not self._is_section_break():\n+            _name, _type, _desc = self._consume_field(parse_type, prefer_type)\n+            if multiple and _name:\n+                for name in _name.split(\",\"):\n+                    fields.append((name.strip(), _type, _desc))\n+            elif _name or _type or _desc:\n+                fields.append((_name, _type, _desc,))\n+        return fields\n+\n+    def _consume_inline_attribute(self) -> Tuple[str, List[str]]:\n+        line = next(self._line_iter)\n+        _type, colon, _desc = self._partition_field_on_colon(line)\n+        if not colon or not _desc:\n+            _type, _desc = _desc, _type\n+            _desc += colon\n+        _descs = [_desc] + self._dedent(self._consume_to_end())\n+        _descs = self.__class__(_descs, self._config).lines()\n+        return _type, _descs\n+\n+    def _consume_returns_section(self, preprocess_types: bool = False\n+                                 ) -> List[Tuple[str, str, List[str]]]:\n+        lines = self._dedent(self._consume_to_next_section())\n+        if lines:\n+            before, colon, after = self._partition_field_on_colon(lines[0])\n+            _name, _type, _desc = '', '', lines\n+\n+            if colon:\n+                if after:\n+                    _desc = [after] + lines[1:]\n+                else:\n+                    _desc = lines[1:]\n+\n+                _type = before\n+\n+            if (_type and preprocess_types and\n+                    self._config.napoleon_preprocess_types):\n+                _type = _convert_type_spec(_type, self._config.napoleon_type_aliases or {})\n+\n+            _desc = self.__class__(_desc, self._config).lines()\n+            return [(_name, _type, _desc,)]\n+        else:\n+            return []\n+\n+    def _consume_usage_section(self) -> List[str]:\n+        lines = self._dedent(self._consume_to_next_section())\n+        return lines\n+\n+    def _consume_section_header(self) -> str:\n+        section = next(self._line_iter)\n+        stripped_section = section.strip(':')\n+        if stripped_section.lower() in self._sections:\n+            section = stripped_section\n+        return section\n+\n+    def _consume_to_end(self) -> List[str]:\n+        lines = []\n+        while self._line_iter.has_next():\n+            lines.append(next(self._line_iter))\n+        return lines\n+\n+    def _consume_to_next_section(self) -> List[str]:\n+        self._consume_empty()\n+        lines = []\n+        while not self._is_section_break():\n+            lines.append(next(self._line_iter))\n+        return lines + self._consume_empty()\n+\n+    def _dedent(self, lines: List[str], full: bool = False) -> List[str]:\n+        if full:\n+            return [line.lstrip() for line in lines]\n+        else:\n+            min_indent = self._get_min_indent(lines)\n+            return [line[min_indent:] for line in lines]\n+\n+    def _escape_args_and_kwargs(self, name: str) -> str:\n+        if name.endswith('_') and getattr(self._config, 'strip_signature_backslash', False):\n+            name = name[:-1] + r'\\_'\n+\n+        if name[:2] == '**':\n+            return r'\\*\\*' + name[2:]\n+        elif name[:1] == '*':\n+            return r'\\*' + name[1:]\n+        else:\n+            return name\n+\n+    def _fix_field_desc(self, desc: List[str]) -> List[str]:\n+        if self._is_list(desc):\n+            desc = [''] + desc\n+        elif desc[0].endswith('::'):\n+            desc_block = desc[1:]\n+            indent = self._get_indent(desc[0])\n+            block_indent = self._get_initial_indent(desc_block)\n+            if block_indent > indent:\n+                desc = [''] + desc\n+            else:\n+                desc = ['', desc[0]] + self._indent(desc_block, 4)\n+        return desc\n+\n+    def _format_admonition(self, admonition: str, lines: List[str]) -> List[str]:\n+        lines = self._strip_empty(lines)\n+        if len(lines) == 1:\n+            return ['.. %s:: %s' % (admonition, lines[0].strip()), '']\n+        elif lines:\n+            lines = self._indent(self._dedent(lines), 3)\n+            return ['.. %s::' % admonition, ''] + lines + ['']\n+        else:\n+            return ['.. %s::' % admonition, '']\n+\n+    def _format_block(self, prefix: str, lines: List[str], padding: str = None) -> List[str]:\n+        if lines:\n+            if padding is None:\n+                padding = ' ' * len(prefix)\n+            result_lines = []\n+            for i, line in enumerate(lines):\n+                if i == 0:\n+                    result_lines.append((prefix + line).rstrip())\n+                elif line:\n+                    result_lines.append(padding + line)\n+                else:\n+                    result_lines.append('')\n+            return result_lines\n+        else:\n+            return [prefix]\n+\n+    def _format_docutils_params(self, fields: List[Tuple[str, str, List[str]]],\n+                                field_role: str = 'param', type_role: str = 'type'\n+                                ) -> List[str]:\n+        lines = []\n+        for _name, _type, _desc in fields:\n+            _desc = self._strip_empty(_desc)\n+            if any(_desc):\n+                _desc = self._fix_field_desc(_desc)\n+                field = ':%s %s: ' % (field_role, _name)\n+                lines.extend(self._format_block(field, _desc))\n+            else:\n+                lines.append(':%s %s:' % (field_role, _name))\n+\n+            if _type:\n+                lines.append(':%s %s: %s' % (type_role, _name, _type))\n+        return lines + ['']\n+\n+    def _format_field(self, _name: str, _type: str, _desc: List[str]) -> List[str]:\n+        _desc = self._strip_empty(_desc)\n+        has_desc = any(_desc)\n+        separator = ' -- ' if has_desc else ''\n+        if _name:\n+            if _type:\n+                if '`' in _type:\n+                    field = '**%s** (%s)%s' % (_name, _type, separator)\n+                else:\n+                    field = '**%s** (*%s*)%s' % (_name, _type, separator)\n+            else:\n+                field = '**%s**%s' % (_name, separator)\n+        elif _type:\n+            if '`' in _type:\n+                field = '%s%s' % (_type, separator)\n+            else:\n+                field = '*%s*%s' % (_type, separator)\n+        else:\n+            field = ''\n+\n+        if has_desc:\n+            _desc = self._fix_field_desc(_desc)\n+            if _desc[0]:\n+                return [field + _desc[0]] + _desc[1:]\n+            else:\n+                return [field] + _desc\n+        else:\n+            return [field]\n+\n+    def _format_fields(self, field_type: str, fields: List[Tuple[str, str, List[str]]]\n+                       ) -> List[str]:\n+        field_type = ':%s:' % field_type.strip()\n+        padding = ' ' * len(field_type)\n+        multi = len(fields) > 1\n+        lines: List[str] = []\n+        for _name, _type, _desc in fields:\n+            field = self._format_field(_name, _type, _desc)\n+            if multi:\n+                if lines:\n+                    lines.extend(self._format_block(padding + ' * ', field))\n+                else:\n+                    lines.extend(self._format_block(field_type + ' * ', field))\n+            else:\n+                lines.extend(self._format_block(field_type + ' ', field))\n+        if lines and lines[-1]:\n+            lines.append('')\n+        return lines\n+\n+    def _get_current_indent(self, peek_ahead: int = 0) -> int:\n+        line = self._line_iter.peek(peek_ahead + 1)[peek_ahead]\n+        while line != self._line_iter.sentinel:\n+            if line:\n+                return self._get_indent(line)\n+            peek_ahead += 1\n+            line = self._line_iter.peek(peek_ahead + 1)[peek_ahead]\n+        return 0\n+\n+    def _get_indent(self, line: str) -> int:\n+        for i, s in enumerate(line):\n+            if not s.isspace():\n+                return i\n+        return len(line)\n+\n+    def _get_initial_indent(self, lines: List[str]) -> int:\n+        for line in lines:\n+            if line:\n+                return self._get_indent(line)\n+        return 0\n+\n+    def _get_min_indent(self, lines: List[str]) -> int:\n+        min_indent = None\n+        for line in lines:\n+            if line:\n+                indent = self._get_indent(line)\n+                if min_indent is None:\n+                    min_indent = indent\n+                elif indent < min_indent:\n+                    min_indent = indent\n+        return min_indent or 0\n+\n+    def _indent(self, lines: List[str], n: int = 4) -> List[str]:\n+        return [(' ' * n) + line for line in lines]\n+\n+    def _is_indented(self, line: str, indent: int = 1) -> bool:\n+        for i, s in enumerate(line):\n+            if i >= indent:\n+                return True\n+            elif not s.isspace():\n+                return False\n+        return False\n+\n+    def _is_list(self, lines: List[str]) -> bool:\n+        if not lines:\n+            return False\n+        if _bullet_list_regex.match(lines[0]):\n+            return True\n+        if _enumerated_list_regex.match(lines[0]):\n+            return True\n+        if len(lines) < 2 or lines[0].endswith('::'):\n+            return False\n+        indent = self._get_indent(lines[0])\n+        next_indent = indent\n+        for line in lines[1:]:\n+            if line:\n+                next_indent = self._get_indent(line)\n+                break\n+        return next_indent > indent\n+\n+    def _is_section_header(self) -> bool:\n+        section = self._line_iter.peek().lower()\n+        match = _google_section_regex.match(section)\n+        if match and section.strip(':') in self._sections:\n+            header_indent = self._get_indent(section)\n+            section_indent = self._get_current_indent(peek_ahead=1)\n+            return section_indent > header_indent\n+        elif self._directive_sections:\n+            if _directive_regex.match(section):\n+                for directive_section in self._directive_sections:\n+                    if section.startswith(directive_section):\n+                        return True\n+        return False\n+\n+    def _is_section_break(self) -> bool:\n+        line = self._line_iter.peek()\n+        return (not self._line_iter.has_next() or\n+                self._is_section_header() or\n+                (self._is_in_section and\n+                    line and\n+                    not self._is_indented(line, self._section_indent)))\n+\n+    def _load_custom_sections(self) -> None:\n+        if self._config.napoleon_custom_sections is not None:\n+            for entry in self._config.napoleon_custom_sections:\n+                if isinstance(entry, str):\n+                    # if entry is just a label, add to sections list,\n+                    # using generic section logic.\n+                    self._sections[entry.lower()] = self._parse_custom_generic_section\n+                else:\n+                    # otherwise, assume entry is container;\n+                    if entry[1] == \"params_style\":\n+                        self._sections[entry[0].lower()] = \\\n+                            self._parse_custom_params_style_section\n+                    elif entry[1] == \"returns_style\":\n+                        self._sections[entry[0].lower()] = \\\n+                            self._parse_custom_returns_style_section\n+                    else:\n+                        # [0] is new section, [1] is the section to alias.\n+                        # in the case of key mismatch, just handle as generic section.\n+                        self._sections[entry[0].lower()] = \\\n+                            self._sections.get(entry[1].lower(),\n+                                               self._parse_custom_generic_section)\n+\n+    def _parse(self) -> None:\n+        self._parsed_lines = self._consume_empty()\n+\n+        if self._name and self._what in ('attribute', 'data', 'property'):\n+            # Implicit stop using StopIteration no longer allowed in\n+            # Python 3.7; see PEP 479\n+            res: List[str] = []\n+            try:\n+                res = self._parse_attribute_docstring()\n+            except StopIteration:\n+                pass\n+            self._parsed_lines.extend(res)\n+            return\n+\n+        while self._line_iter.has_next():\n+            if self._is_section_header():\n+                try:\n+                    section = self._consume_section_header()\n+                    self._is_in_section = True\n+                    self._section_indent = self._get_current_indent()\n+                    if _directive_regex.match(section):\n+                        lines = [section] + self._consume_to_next_section()\n+                    else:\n+                        lines = self._sections[section.lower()](section)\n+                finally:\n+                    self._is_in_section = False\n+                    self._section_indent = 0\n+            else:\n+                if not self._parsed_lines:\n+                    lines = self._consume_contiguous() + self._consume_empty()\n+                else:\n+                    lines = self._consume_to_next_section()\n+            self._parsed_lines.extend(lines)\n+\n+    def _parse_admonition(self, admonition: str, section: str) -> List[str]:\n+        # type (str, str) -> List[str]\n+        lines = self._consume_to_next_section()\n+        return self._format_admonition(admonition, lines)\n+\n+    def _parse_attribute_docstring(self) -> List[str]:\n+        _type, _desc = self._consume_inline_attribute()\n+        lines = self._format_field('', '', _desc)\n+        if _type:\n+            lines.extend(['', ':type: %s' % _type])\n+        return lines\n+\n+    def _parse_attributes_section(self, section: str) -> List[str]:\n+        lines = []\n+        for _name, _type, _desc in self._consume_fields():\n+            if not _type:\n+                _type = self._lookup_annotation(_name)\n+            if self._config.napoleon_use_ivar:\n+                _name = self._qualify_name(_name, self._obj)\n+                field = ':ivar %s: ' % _name\n+                lines.extend(self._format_block(field, _desc))\n+                if _type:\n+                    lines.append(':vartype %s: %s' % (_name, _type))\n+            else:\n+                lines.append('.. attribute:: ' + _name)\n+                if self._opt and 'noindex' in self._opt:\n+                    lines.append('   :noindex:')\n+                lines.append('')\n+\n+                fields = self._format_field('', '', _desc)\n+                lines.extend(self._indent(fields, 3))\n+                if _type:\n+                    lines.append('')\n+                    lines.extend(self._indent([':type: %s' % _type], 3))\n+                lines.append('')\n+        if self._config.napoleon_use_ivar:\n+            lines.append('')\n+        return lines\n+\n+    def _parse_examples_section(self, section: str) -> List[str]:\n+        labels = {\n+            'example': _('Example'),\n+            'examples': _('Examples'),\n+        }\n+        use_admonition = self._config.napoleon_use_admonition_for_examples\n+        label = labels.get(section.lower(), section)\n+        return self._parse_generic_section(label, use_admonition)\n+\n+    def _parse_custom_generic_section(self, section: str) -> List[str]:\n+        # for now, no admonition for simple custom sections\n+        return self._parse_generic_section(section, False)\n+\n+    def _parse_custom_params_style_section(self, section: str) -> List[str]:\n+        return self._format_fields(section, self._consume_fields())\n+\n+    def _parse_custom_returns_style_section(self, section: str) -> List[str]:\n+        fields = self._consume_returns_section(preprocess_types=True)\n+        return self._format_fields(section, fields)\n+\n+    def _parse_usage_section(self, section: str) -> List[str]:\n+        header = ['.. rubric:: Usage:', '']\n+        block = ['.. code-block:: python', '']\n+        lines = self._consume_usage_section()\n+        lines = self._indent(lines, 3)\n+        return header + block + lines + ['']\n+\n+    def _parse_generic_section(self, section: str, use_admonition: bool) -> List[str]:\n+        lines = self._strip_empty(self._consume_to_next_section())\n+        lines = self._dedent(lines)\n+        if use_admonition:\n+            header = '.. admonition:: %s' % section\n+            lines = self._indent(lines, 3)\n+        else:\n+            header = '.. rubric:: %s' % section\n+        if lines:\n+            return [header, ''] + lines + ['']\n+        else:\n+            return [header, '']\n+\n+    def _parse_keyword_arguments_section(self, section: str) -> List[str]:\n+        fields = self._consume_fields()\n+        if self._config.napoleon_use_keyword:\n+            return self._format_docutils_params(\n+                fields,\n+                field_role=\"keyword\",\n+                type_role=\"kwtype\")\n+        else:\n+            return self._format_fields(_('Keyword Arguments'), fields)\n+\n+    def _parse_methods_section(self, section: str) -> List[str]:\n+        lines: List[str] = []\n+        for _name, _type, _desc in self._consume_fields(parse_type=False):\n+            lines.append('.. method:: %s' % _name)\n+            if self._opt and 'noindex' in self._opt:\n+                lines.append('   :noindex:')\n+            if _desc:\n+                lines.extend([''] + self._indent(_desc, 3))\n+            lines.append('')\n+        return lines\n+\n+    def _parse_notes_section(self, section: str) -> List[str]:\n+        use_admonition = self._config.napoleon_use_admonition_for_notes\n+        return self._parse_generic_section(_('Notes'), use_admonition)\n+\n+    def _parse_other_parameters_section(self, section: str) -> List[str]:\n+        if self._config.napoleon_use_param:\n+            # Allow to declare multiple parameters at once (ex: x, y: int)\n+            fields = self._consume_fields(multiple=True)\n+            return self._format_docutils_params(fields)\n+        else:\n+            fields = self._consume_fields()\n+            return self._format_fields(_('Other Parameters'), fields)\n+\n+    def _parse_parameters_section(self, section: str) -> List[str]:\n+        if self._config.napoleon_use_param:\n+            # Allow to declare multiple parameters at once (ex: x, y: int)\n+            fields = self._consume_fields(multiple=True)\n+            return self._format_docutils_params(fields)\n+        else:\n+            fields = self._consume_fields()\n+            return self._format_fields(_('Parameters'), fields)\n+\n+    def _parse_raises_section(self, section: str) -> List[str]:\n+        fields = self._consume_fields(parse_type=False, prefer_type=True)\n+        lines: List[str] = []\n+        for _name, _type, _desc in fields:\n+            m = self._name_rgx.match(_type)\n+            if m and m.group('name'):\n+                _type = m.group('name')\n+            elif _xref_regex.match(_type):\n+                pos = _type.find('`')\n+                _type = _type[pos + 1:-1]\n+            _type = ' ' + _type if _type else ''\n+            _desc = self._strip_empty(_desc)\n+            _descs = ' ' + '\\n    '.join(_desc) if any(_desc) else ''\n+            lines.append(':raises%s:%s' % (_type, _descs))\n+        if lines:\n+            lines.append('')\n+        return lines\n+\n+    def _parse_receives_section(self, section: str) -> List[str]:\n+        if self._config.napoleon_use_param:\n+            # Allow to declare multiple parameters at once (ex: x, y: int)\n+            fields = self._consume_fields(multiple=True)\n+            return self._format_docutils_params(fields)\n+        else:\n+            fields = self._consume_fields()\n+            return self._format_fields(_('Receives'), fields)\n+\n+    def _parse_references_section(self, section: str) -> List[str]:\n+        use_admonition = self._config.napoleon_use_admonition_for_references\n+        return self._parse_generic_section(_('References'), use_admonition)\n+\n+    def _parse_returns_section(self, section: str) -> List[str]:\n+        fields = self._consume_returns_section()\n+        multi = len(fields) > 1\n+        if multi:\n+            use_rtype = False\n+        else:\n+            use_rtype = self._config.napoleon_use_rtype\n+\n+        lines: List[str] = []\n+        for _name, _type, _desc in fields:\n+            if use_rtype:\n+                field = self._format_field(_name, '', _desc)\n+            else:\n+                field = self._format_field(_name, _type, _desc)\n+\n+            if multi:\n+                if lines:\n+                    lines.extend(self._format_block('          * ', field))\n+                else:\n+                    lines.extend(self._format_block(':returns: * ', field))\n+            else:\n+                lines.extend(self._format_block(':returns: ', field))\n+                if _type and use_rtype:\n+                    lines.extend([':rtype: %s' % _type, ''])\n+        if lines and lines[-1]:\n+            lines.append('')\n+        return lines\n+\n+    def _parse_see_also_section(self, section: str) -> List[str]:\n+        return self._parse_admonition('seealso', section)\n+\n+    def _parse_warns_section(self, section: str) -> List[str]:\n+        return self._format_fields(_('Warns'), self._consume_fields())\n+\n+    def _parse_yields_section(self, section: str) -> List[str]:\n+        fields = self._consume_returns_section(preprocess_types=True)\n+        return self._format_fields(_('Yields'), fields)\n+\n+    def _partition_field_on_colon(self, line: str) -> Tuple[str, str, str]:\n+        before_colon = []\n+        after_colon = []\n+        colon = ''\n+        found_colon = False\n+        for i, source in enumerate(_xref_or_code_regex.split(line)):\n+            if found_colon:\n+                after_colon.append(source)\n+            else:\n+                m = _single_colon_regex.search(source)\n+                if (i % 2) == 0 and m:\n+                    found_colon = True\n+                    colon = source[m.start(): m.end()]\n+                    before_colon.append(source[:m.start()])\n+                    after_colon.append(source[m.end():])\n+                else:\n+                    before_colon.append(source)\n+\n+        return (\"\".join(before_colon).strip(),\n+                colon,\n+                \"\".join(after_colon).strip())\n+\n+    def _qualify_name(self, attr_name: str, klass: Type) -> str:\n+        if klass and '.' not in attr_name:\n+            if attr_name.startswith('~'):\n+                attr_name = attr_name[1:]\n+            try:\n+                q = klass.__qualname__\n+            except AttributeError:\n+                q = klass.__name__\n+            return '~%s.%s' % (q, attr_name)\n+        return attr_name\n+\n+    def _strip_empty(self, lines: List[str]) -> List[str]:\n+        if lines:\n+            start = -1\n+            for i, line in enumerate(lines):\n+                if line:\n+                    start = i\n+                    break\n+            if start == -1:\n+                lines = []\n+            end = -1\n+            for i in reversed(range(len(lines))):\n+                line = lines[i]\n+                if line:\n+                    end = i\n+                    break\n+            if start > 0 or end + 1 < len(lines):\n+                lines = lines[start:end + 1]\n+        return lines\n+\n+    def _lookup_annotation(self, _name: str) -> str:\n+        if self._config.napoleon_attr_annotations:\n+            if self._what in (\"module\", \"class\", \"exception\") and self._obj:\n+                # cache the class annotations\n+                if not hasattr(self, \"_annotations\"):\n+                    localns = getattr(self._config, \"autodoc_type_aliases\", {})\n+                    localns.update(getattr(\n+                                   self._config, \"napoleon_type_aliases\", {}\n+                                   ) or {})\n+                    self._annotations = get_type_hints(self._obj, None, localns)\n+                if _name in self._annotations:\n+                    return stringify_annotation(self._annotations[_name])\n+        # No annotation found\n+        return \"\"\n+\n+\n+def _recombine_set_tokens(tokens: List[str]) -> List[str]:\n+    token_queue = collections.deque(tokens)\n+    keywords = (\"optional\", \"default\")\n+\n+    def takewhile_set(tokens):\n+        open_braces = 0\n+        previous_token = None\n+        while True:\n+            try:\n+                token = tokens.popleft()\n+            except IndexError:\n+                break\n+\n+            if token == \", \":\n+                previous_token = token\n+                continue\n+\n+            if not token.strip():\n+                continue\n+\n+            if token in keywords:\n+                tokens.appendleft(token)\n+                if previous_token is not None:\n+                    tokens.appendleft(previous_token)\n+                break\n+\n+            if previous_token is not None:\n+                yield previous_token\n+                previous_token = None\n+\n+            if token == \"{\":\n+                open_braces += 1\n+            elif token == \"}\":\n+                open_braces -= 1\n+\n+            yield token\n+\n+            if open_braces == 0:\n+                break\n+\n+    def combine_set(tokens):\n+        while True:\n+            try:\n+                token = tokens.popleft()\n+            except IndexError:\n+                break\n+\n+            if token == \"{\":\n+                tokens.appendleft(\"{\")\n+                yield \"\".join(takewhile_set(tokens))\n+            else:\n+                yield token\n+\n+    return list(combine_set(token_queue))\n+\n+\n+def _tokenize_type_spec(spec: str) -> List[str]:\n+    def postprocess(item):\n+        if _default_regex.match(item):\n+            default = item[:7]\n+            # can't be separated by anything other than a single space\n+            # for now\n+            other = item[8:]\n+\n+            return [default, \" \", other]\n+        else:\n+            return [item]\n+\n+    tokens = list(\n+        item\n+        for raw_token in _token_regex.split(spec)\n+        for item in postprocess(raw_token)\n+        if item\n+    )\n+    return tokens\n+\n+\n+def _token_type(token: str, location: str = None) -> str:\n+    def is_numeric(token):\n+        try:\n+            # use complex to make sure every numeric value is detected as literal\n+            complex(token)\n+        except ValueError:\n+            return False\n+        else:\n+            return True\n+\n+    if token.startswith(\" \") or token.endswith(\" \"):\n+        type_ = \"delimiter\"\n+    elif (\n+            is_numeric(token) or\n+            (token.startswith(\"{\") and token.endswith(\"}\")) or\n+            (token.startswith('\"') and token.endswith('\"')) or\n+            (token.startswith(\"'\") and token.endswith(\"'\"))\n+    ):\n+        type_ = \"literal\"\n+    elif token.startswith(\"{\"):\n+        logger.warning(\n+            __(\"invalid value set (missing closing brace): %s\"),\n+            token,\n+            location=location,\n+        )\n+        type_ = \"literal\"\n+    elif token.endswith(\"}\"):\n+        logger.warning(\n+            __(\"invalid value set (missing opening brace): %s\"),\n+            token,\n+            location=location,\n+        )\n+        type_ = \"literal\"\n+    elif token.startswith(\"'\") or token.startswith('\"'):\n+        logger.warning(\n+            __(\"malformed string literal (missing closing quote): %s\"),\n+            token,\n+            location=location,\n+        )\n+        type_ = \"literal\"\n+    elif token.endswith(\"'\") or token.endswith('\"'):\n+        logger.warning(\n+            __(\"malformed string literal (missing opening quote): %s\"),\n+            token,\n+            location=location,\n+        )\n+        type_ = \"literal\"\n+    elif token in (\"optional\", \"default\"):\n+        # default is not a official keyword (yet) but supported by the\n+        # reference implementation (numpydoc) and widely used\n+        type_ = \"control\"\n+    elif _xref_regex.match(token):\n+        type_ = \"reference\"\n+    else:\n+        type_ = \"obj\"\n+\n+    return type_\n+\n+\n+def _convert_numpy_type_spec(_type: str, location: str = None, translations: dict = {}) -> str:\n+    def convert_obj(obj, translations, default_translation):\n+        translation = translations.get(obj, obj)\n+\n+        # use :class: (the default) only if obj is not a standard singleton\n+        if translation in _SINGLETONS and default_translation == \":class:`%s`\":\n+            default_translation = \":obj:`%s`\"\n+        elif translation == \"...\" and default_translation == \":class:`%s`\":\n+            # allow referencing the builtin ...\n+            default_translation = \":obj:`%s <Ellipsis>`\"\n+\n+        if _xref_regex.match(translation) is None:\n+            translation = default_translation % translation\n+\n+        return translation\n+\n+    tokens = _tokenize_type_spec(_type)\n+    combined_tokens = _recombine_set_tokens(tokens)\n+    types = [\n+        (token, _token_type(token, location))\n+        for token in combined_tokens\n+    ]\n+\n+    converters = {\n+        \"literal\": lambda x: \"``%s``\" % x,\n+        \"obj\": lambda x: convert_obj(x, translations, \":class:`%s`\"),\n+        \"control\": lambda x: \"*%s*\" % x,\n+        \"delimiter\": lambda x: x,\n+        \"reference\": lambda x: x,\n+    }\n+\n+    converted = \"\".join(converters.get(type_)(token) for token, type_ in types)\n+\n+    return converted\n+\n+\n+class NumpyDocstring(GoogleDocstring):\n+    \"\"\"Convert NumPy style docstrings to reStructuredText.\n+\n+    Parameters\n+    ----------\n+    docstring : :obj:`str` or :obj:`list` of :obj:`str`\n+        The docstring to parse, given either as a string or split into\n+        individual lines.\n+    config: :obj:`sphinx.ext.napoleon.Config` or :obj:`sphinx.config.Config`\n+        The configuration settings to use. If not given, defaults to the\n+        config object on `app`; or if `app` is not given defaults to the\n+        a new :class:`sphinx.ext.napoleon.Config` object.\n+\n+\n+    Other Parameters\n+    ----------------\n+    app : :class:`sphinx.application.Sphinx`, optional\n+        Application object representing the Sphinx process.\n+    what : :obj:`str`, optional\n+        A string specifying the type of the object to which the docstring\n+        belongs. Valid values: \"module\", \"class\", \"exception\", \"function\",\n+        \"method\", \"attribute\".\n+    name : :obj:`str`, optional\n+        The fully qualified name of the object.\n+    obj : module, class, exception, function, method, or attribute\n+        The object to which the docstring belongs.\n+    options : :class:`sphinx.ext.autodoc.Options`, optional\n+        The options given to the directive: an object with attributes\n+        inherited_members, undoc_members, show_inheritance and noindex that\n+        are True if the flag option of same name was given to the auto\n+        directive.\n+\n+\n+    Example\n+    -------\n+    >>> from sphinx.ext.napoleon import Config\n+    >>> config = Config(napoleon_use_param=True, napoleon_use_rtype=True)\n+    >>> docstring = '''One line summary.\n+    ...\n+    ... Extended description.\n+    ...\n+    ... Parameters\n+    ... ----------\n+    ... arg1 : int\n+    ...     Description of `arg1`\n+    ... arg2 : str\n+    ...     Description of `arg2`\n+    ... Returns\n+    ... -------\n+    ... str\n+    ...     Description of return value.\n+    ... '''\n+    >>> print(NumpyDocstring(docstring, config))\n+    One line summary.\n+    <BLANKLINE>\n+    Extended description.\n+    <BLANKLINE>\n+    :param arg1: Description of `arg1`\n+    :type arg1: int\n+    :param arg2: Description of `arg2`\n+    :type arg2: str\n+    <BLANKLINE>\n+    :returns: Description of return value.\n+    :rtype: str\n+    <BLANKLINE>\n+\n+    Methods\n+    -------\n+    __str__()\n+        Return the parsed docstring in reStructuredText format.\n+\n+        Returns\n+        -------\n+        str\n+            UTF-8 encoded version of the docstring.\n+\n+    __unicode__()\n+        Return the parsed docstring in reStructuredText format.\n+\n+        Returns\n+        -------\n+        unicode\n+            Unicode version of the docstring.\n+\n+    lines()\n+        Return the parsed lines of the docstring in reStructuredText format.\n+\n+        Returns\n+        -------\n+        list(str)\n+            The lines of the docstring in a list.\n+\n+    \"\"\"\n+    def __init__(self, docstring: Union[str, List[str]], config: SphinxConfig = None,\n+                 app: Sphinx = None, what: str = '', name: str = '',\n+                 obj: Any = None, options: Any = None) -> None:\n+        self._directive_sections = ['.. index::']\n+        super().__init__(docstring, config, app, what, name, obj, options)\n+\n+    def _get_location(self) -> str:\n+        try:\n+            filepath = inspect.getfile(self._obj) if self._obj is not None else None\n+        except TypeError:\n+            filepath = None\n+        name = self._name\n+\n+        if filepath is None and name is None:\n+            return None\n+        elif filepath is None:\n+            filepath = \"\"\n+\n+        return \":\".join([filepath, \"docstring of %s\" % name])\n+\n+    def _escape_args_and_kwargs(self, name: str) -> str:\n+        func = super()._escape_args_and_kwargs\n+\n+        if \", \" in name:\n+            return \", \".join(func(param) for param in name.split(\", \"))\n+        else:\n+            return func(name)\n+\n+    def _consume_field(self, parse_type: bool = True, prefer_type: bool = False\n+                       ) -> Tuple[str, str, List[str]]:\n+        line = next(self._line_iter)\n+        if parse_type:\n+            _name, _, _type = self._partition_field_on_colon(line)\n+        else:\n+            _name, _type = line, ''\n+        _name, _type = _name.strip(), _type.strip()\n+        _name = self._escape_args_and_kwargs(_name)\n+\n+        if parse_type and not _type:\n+            _type = self._lookup_annotation(_name)\n+\n+        if prefer_type and not _type:\n+            _type, _name = _name, _type\n+\n+        if self._config.napoleon_preprocess_types:\n+            _type = _convert_numpy_type_spec(\n+                _type,\n+                location=self._get_location(),\n+                translations=self._config.napoleon_type_aliases or {},\n+            )\n+\n+        indent = self._get_indent(line) + 1\n+        _desc = self._dedent(self._consume_indented_block(indent))\n+        _desc = self.__class__(_desc, self._config).lines()\n+        return _name, _type, _desc\n+\n+    def _consume_returns_section(self, preprocess_types: bool = False\n+                                 ) -> List[Tuple[str, str, List[str]]]:\n+        return self._consume_fields(prefer_type=True)\n+\n+    def _consume_section_header(self) -> str:\n+        section = next(self._line_iter)\n+        if not _directive_regex.match(section):\n+            # Consume the header underline\n+            next(self._line_iter)\n+        return section\n+\n+    def _is_section_break(self) -> bool:\n+        line1, line2 = self._line_iter.peek(2)\n+        return (not self._line_iter.has_next() or\n+                self._is_section_header() or\n+                ['', ''] == [line1, line2] or\n+                (self._is_in_section and\n+                    line1 and\n+                    not self._is_indented(line1, self._section_indent)))\n+\n+    def _is_section_header(self) -> bool:\n+        section, underline = self._line_iter.peek(2)\n+        section = section.lower()\n+        if section in self._sections and isinstance(underline, str):\n+            return bool(_numpy_section_regex.match(underline))\n+        elif self._directive_sections:\n+            if _directive_regex.match(section):\n+                for directive_section in self._directive_sections:\n+                    if section.startswith(directive_section):\n+                        return True\n+        return False\n+\n+    def _parse_see_also_section(self, section: str) -> List[str]:\n+        lines = self._consume_to_next_section()\n+        try:\n+            return self._parse_numpydoc_see_also_section(lines)\n+        except ValueError:\n+            return self._format_admonition('seealso', lines)\n+\n+    def _parse_numpydoc_see_also_section(self, content: List[str]) -> List[str]:\n+        \"\"\"\n+        Derived from the NumpyDoc implementation of _parse_see_also.\n+\n+        See Also\n+        --------\n+        func_name : Descriptive text\n+            continued text\n+        another_func_name : Descriptive text\n+        func_name1, func_name2, :meth:`func_name`, func_name3\n+\n+        \"\"\"\n+        items = []\n+\n+        def parse_item_name(text: str) -> Tuple[str, str]:\n+            \"\"\"Match ':role:`name`' or 'name'\"\"\"\n+            m = self._name_rgx.match(text)\n+            if m:\n+                g = m.groups()\n+                if g[1] is None:\n+                    return g[3], None\n+                else:\n+                    return g[2], g[1]\n+            raise ValueError(\"%s is not a item name\" % text)\n+\n+        def push_item(name: str, rest: List[str]) -> None:\n+            if not name:\n+                return\n+            name, role = parse_item_name(name)\n+            items.append((name, list(rest), role))\n+            del rest[:]\n+\n+        def translate(func, description, role):\n+            translations = self._config.napoleon_type_aliases\n+            if role is not None or not translations:\n+                return func, description, role\n+\n+            translated = translations.get(func, func)\n+            match = self._name_rgx.match(translated)\n+            if not match:\n+                return translated, description, role\n+\n+            groups = match.groupdict()\n+            role = groups[\"role\"]\n+            new_func = groups[\"name\"] or groups[\"name2\"]\n+\n+            return new_func, description, role\n+\n+        current_func = None\n+        rest: List[str] = []\n+\n+        for line in content:\n+            if not line.strip():\n+                continue\n+\n+            m = self._name_rgx.match(line)\n+            if m and line[m.end():].strip().startswith(':'):\n+                push_item(current_func, rest)\n+                current_func, line = line[:m.end()], line[m.end():]\n+                rest = [line.split(':', 1)[1].strip()]\n+                if not rest[0]:\n+                    rest = []\n+            elif not line.startswith(' '):\n+                push_item(current_func, rest)\n+                current_func = None\n+                if ',' in line:\n+                    for func in line.split(','):\n+                        if func.strip():\n+                            push_item(func, [])\n+                elif line.strip():\n+                    current_func = line\n+            elif current_func is not None:\n+                rest.append(line.strip())\n+        push_item(current_func, rest)\n+\n+        if not items:\n+            return []\n+\n+        # apply type aliases\n+        items = [\n+            translate(func, description, role)\n+            for func, description, role in items\n+        ]\n+\n+        lines: List[str] = []\n+        last_had_desc = True\n+        for name, desc, role in items:\n+            if role:\n+                link = ':%s:`%s`' % (role, name)\n+            else:\n+                link = ':obj:`%s`' % name\n+            if desc or last_had_desc:\n+                lines += ['']\n+                lines += [link]\n+            else:\n+                lines[-1] += \", %s\" % link\n+            if desc:\n+                lines += self._indent([' '.join(desc)])\n+                last_had_desc = True\n+            else:\n+                last_had_desc = False\n+        lines += ['']\n+\n+        return self._format_admonition('seealso', lines)\n"
  },
  "psf__requests-2931": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-2931",
    "model_patch": "diff --git a/requests/models.py b/requests/models.py\nindex 9c624d3..5638572 100644\n--- a/requests/models.py\n+++ b/requests/models.py\n@@ -71,32 +71,6 @@ class RequestEncodingMixin(object):\n \n         return ''.join(url)\n \n-    @staticmethod\n-    def _encode_params(data):\n-        \"\"\"Encode parameters in a piece of data.\n-\n-        Will successfully encode parameters when passed as a dict or a list of\n-        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n-        if parameters are supplied as a dict.\n-        \"\"\"\n-\n-        if isinstance(data, (str, bytes)):\n-            return to_native_string(data)\n-        elif hasattr(data, 'read'):\n-            return data\n-        elif hasattr(data, '__iter__'):\n-            result = []\n-            for k, vs in to_key_val_list(data):\n-                if isinstance(vs, basestring) or not hasattr(vs, '__iter__'):\n-                    vs = [vs]\n-                for v in vs:\n-                    if v is not None:\n-                        result.append(\n-                            (k.encode('utf-8') if isinstance(k, str) else k,\n-                             v.encode('utf-8') if isinstance(v, str) else v))\n-            return urlencode(result, doseq=True)\n-        else:\n-            return data\n \n     @staticmethod\n     def _encode_files(files, data):\ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000..5638572\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,822 @@\n+# -*- coding: utf-8 -*-\n+\n+\"\"\"\n+requests.models\n+~~~~~~~~~~~~~~~\n+\n+This module contains the primary objects that power Requests.\n+\"\"\"\n+\n+import collections\n+import datetime\n+\n+from io import BytesIO, UnsupportedOperation\n+from .hooks import default_hooks\n+from .structures import CaseInsensitiveDict\n+\n+from .auth import HTTPBasicAuth\n+from .cookies import cookiejar_from_dict, get_cookie_header, _copy_cookie_jar\n+from .packages.urllib3.fields import RequestField\n+from .packages.urllib3.filepost import encode_multipart_formdata\n+from .packages.urllib3.util import parse_url\n+from .packages.urllib3.exceptions import (\n+    DecodeError, ReadTimeoutError, ProtocolError, LocationParseError)\n+from .exceptions import (\n+    HTTPError, MissingSchema, InvalidURL, ChunkedEncodingError,\n+    ContentDecodingError, ConnectionError, StreamConsumedError)\n+from .utils import (\n+    guess_filename, get_auth_from_url, requote_uri,\n+    stream_decode_response_unicode, to_key_val_list, parse_header_links,\n+    iter_slices, guess_json_utf, super_len, to_native_string)\n+from .compat import (\n+    cookielib, urlunparse, urlsplit, urlencode, str, bytes, StringIO,\n+    is_py2, chardet, builtin_str, basestring)\n+from .compat import json as complexjson\n+from .status_codes import codes\n+\n+#: The set of HTTP status codes that indicate an automatically\n+#: processable redirect.\n+REDIRECT_STATI = (\n+    codes.moved,              # 301\n+    codes.found,              # 302\n+    codes.other,              # 303\n+    codes.temporary_redirect, # 307\n+    codes.permanent_redirect, # 308\n+)\n+\n+DEFAULT_REDIRECT_LIMIT = 30\n+CONTENT_CHUNK_SIZE = 10 * 1024\n+ITER_CHUNK_SIZE = 512\n+\n+\n+class RequestEncodingMixin(object):\n+    @property\n+    def path_url(self):\n+        \"\"\"Build the path URL to use.\"\"\"\n+\n+        url = []\n+\n+        p = urlsplit(self.url)\n+\n+        path = p.path\n+        if not path:\n+            path = '/'\n+\n+        url.append(path)\n+\n+        query = p.query\n+        if query:\n+            url.append('?')\n+            url.append(query)\n+\n+        return ''.join(url)\n+\n+\n+    @staticmethod\n+    def _encode_files(files, data):\n+        \"\"\"Build the body for a multipart/form-data request.\n+\n+        Will successfully encode files when passed as a dict or a list of\n+        2-tuples. Order is retained if data is a list of 2-tuples but arbitrary\n+        if parameters are supplied as a dict.\n+\n+        \"\"\"\n+        if (not files):\n+            raise ValueError(\"Files must be provided.\")\n+        elif isinstance(data, basestring):\n+            raise ValueError(\"Data must not be a string.\")\n+\n+        new_fields = []\n+        fields = to_key_val_list(data or {})\n+        files = to_key_val_list(files or {})\n+\n+        for field, val in fields:\n+            if isinstance(val, basestring) or not hasattr(val, '__iter__'):\n+                val = [val]\n+            for v in val:\n+                if v is not None:\n+                    # Don't call str() on bytestrings: in Py3 it all goes wrong.\n+                    if not isinstance(v, bytes):\n+                        v = str(v)\n+\n+                    new_fields.append(\n+                        (field.decode('utf-8') if isinstance(field, bytes) else field,\n+                         v.encode('utf-8') if isinstance(v, str) else v))\n+\n+        for (k, v) in files:\n+            # support for explicit filename\n+            ft = None\n+            fh = None\n+            if isinstance(v, (tuple, list)):\n+                if len(v) == 2:\n+                    fn, fp = v\n+                elif len(v) == 3:\n+                    fn, fp, ft = v\n+                else:\n+                    fn, fp, ft, fh = v\n+            else:\n+                fn = guess_filename(v) or k\n+                fp = v\n+\n+            if isinstance(fp, (str, bytes, bytearray)):\n+                fdata = fp\n+            else:\n+                fdata = fp.read()\n+\n+            rf = RequestField(name=k, data=fdata, filename=fn, headers=fh)\n+            rf.make_multipart(content_type=ft)\n+            new_fields.append(rf)\n+\n+        body, content_type = encode_multipart_formdata(new_fields)\n+\n+        return body, content_type\n+\n+\n+class RequestHooksMixin(object):\n+    def register_hook(self, event, hook):\n+        \"\"\"Properly register a hook.\"\"\"\n+\n+        if event not in self.hooks:\n+            raise ValueError('Unsupported event specified, with event name \"%s\"' % (event))\n+\n+        if isinstance(hook, collections.Callable):\n+            self.hooks[event].append(hook)\n+        elif hasattr(hook, '__iter__'):\n+            self.hooks[event].extend(h for h in hook if isinstance(h, collections.Callable))\n+\n+    def deregister_hook(self, event, hook):\n+        \"\"\"Deregister a previously registered hook.\n+        Returns True if the hook existed, False if not.\n+        \"\"\"\n+\n+        try:\n+            self.hooks[event].remove(hook)\n+            return True\n+        except ValueError:\n+            return False\n+\n+\n+class Request(RequestHooksMixin):\n+    \"\"\"A user-created :class:`Request <Request>` object.\n+\n+    Used to prepare a :class:`PreparedRequest <PreparedRequest>`, which is sent to the server.\n+\n+    :param method: HTTP method to use.\n+    :param url: URL to send.\n+    :param headers: dictionary of headers to send.\n+    :param files: dictionary of {filename: fileobject} files to multipart upload.\n+    :param data: the body to attach to the request. If a dictionary is provided, form-encoding will take place.\n+    :param json: json for the body to attach to the request (if files or data is not specified).\n+    :param params: dictionary of URL parameters to append to the URL.\n+    :param auth: Auth handler or (user, pass) tuple.\n+    :param cookies: dictionary or CookieJar of cookies to attach to this request.\n+    :param hooks: dictionary of callback hooks, for internal usage.\n+\n+    Usage::\n+\n+      >>> import requests\n+      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n+      >>> req.prepare()\n+      <PreparedRequest [GET]>\n+\n+    \"\"\"\n+    def __init__(self, method=None, url=None, headers=None, files=None,\n+        data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n+\n+        # Default empty dicts for dict params.\n+        data = [] if data is None else data\n+        files = [] if files is None else files\n+        headers = {} if headers is None else headers\n+        params = {} if params is None else params\n+        hooks = {} if hooks is None else hooks\n+\n+        self.hooks = default_hooks()\n+        for (k, v) in list(hooks.items()):\n+            self.register_hook(event=k, hook=v)\n+\n+        self.method = method\n+        self.url = url\n+        self.headers = headers\n+        self.files = files\n+        self.data = data\n+        self.json = json\n+        self.params = params\n+        self.auth = auth\n+        self.cookies = cookies\n+\n+    def __repr__(self):\n+        return '<Request [%s]>' % (self.method)\n+\n+    def prepare(self):\n+        \"\"\"Constructs a :class:`PreparedRequest <PreparedRequest>` for transmission and returns it.\"\"\"\n+        p = PreparedRequest()\n+        p.prepare(\n+            method=self.method,\n+            url=self.url,\n+            headers=self.headers,\n+            files=self.files,\n+            data=self.data,\n+            json=self.json,\n+            params=self.params,\n+            auth=self.auth,\n+            cookies=self.cookies,\n+            hooks=self.hooks,\n+        )\n+        return p\n+\n+\n+class PreparedRequest(RequestEncodingMixin, RequestHooksMixin):\n+    \"\"\"The fully mutable :class:`PreparedRequest <PreparedRequest>` object,\n+    containing the exact bytes that will be sent to the server.\n+\n+    Generated from either a :class:`Request <Request>` object or manually.\n+\n+    Usage::\n+\n+      >>> import requests\n+      >>> req = requests.Request('GET', 'http://httpbin.org/get')\n+      >>> r = req.prepare()\n+      <PreparedRequest [GET]>\n+\n+      >>> s = requests.Session()\n+      >>> s.send(r)\n+      <Response [200]>\n+\n+    \"\"\"\n+\n+    def __init__(self):\n+        #: HTTP verb to send to the server.\n+        self.method = None\n+        #: HTTP URL to send the request to.\n+        self.url = None\n+        #: dictionary of HTTP headers.\n+        self.headers = None\n+        # The `CookieJar` used to create the Cookie header will be stored here\n+        # after prepare_cookies is called\n+        self._cookies = None\n+        #: request body to send to the server.\n+        self.body = None\n+        #: dictionary of callback hooks, for internal usage.\n+        self.hooks = default_hooks()\n+\n+    def prepare(self, method=None, url=None, headers=None, files=None,\n+        data=None, params=None, auth=None, cookies=None, hooks=None, json=None):\n+        \"\"\"Prepares the entire request with the given parameters.\"\"\"\n+\n+        self.prepare_method(method)\n+        self.prepare_url(url, params)\n+        self.prepare_headers(headers)\n+        self.prepare_cookies(cookies)\n+        self.prepare_body(data, files, json)\n+        self.prepare_auth(auth, url)\n+\n+        # Note that prepare_auth must be last to enable authentication schemes\n+        # such as OAuth to work on a fully prepared request.\n+\n+        # This MUST go after prepare_auth. Authenticators could add a hook\n+        self.prepare_hooks(hooks)\n+\n+    def __repr__(self):\n+        return '<PreparedRequest [%s]>' % (self.method)\n+\n+    def copy(self):\n+        p = PreparedRequest()\n+        p.method = self.method\n+        p.url = self.url\n+        p.headers = self.headers.copy() if self.headers is not None else None\n+        p._cookies = _copy_cookie_jar(self._cookies)\n+        p.body = self.body\n+        p.hooks = self.hooks\n+        return p\n+\n+    def prepare_method(self, method):\n+        \"\"\"Prepares the given HTTP method.\"\"\"\n+        self.method = method\n+        if self.method is not None:\n+            self.method = to_native_string(self.method.upper())\n+\n+    def prepare_url(self, url, params):\n+        \"\"\"Prepares the given HTTP URL.\"\"\"\n+        #: Accept objects that have string representations.\n+        #: We're unable to blindly call unicode/str functions\n+        #: as this will include the bytestring indicator (b'')\n+        #: on python 3.x.\n+        #: https://github.com/kennethreitz/requests/pull/2238\n+        if isinstance(url, bytes):\n+            url = url.decode('utf8')\n+        else:\n+            url = unicode(url) if is_py2 else str(url)\n+\n+        # Don't do any URL preparation for non-HTTP schemes like `mailto`,\n+        # `data` etc to work around exceptions from `url_parse`, which\n+        # handles RFC 3986 only.\n+        if ':' in url and not url.lower().startswith('http'):\n+            self.url = url\n+            return\n+\n+        # Support for unicode domain names and paths.\n+        try:\n+            scheme, auth, host, port, path, query, fragment = parse_url(url)\n+        except LocationParseError as e:\n+            raise InvalidURL(*e.args)\n+\n+        if not scheme:\n+            error = (\"Invalid URL {0!r}: No schema supplied. Perhaps you meant http://{0}?\")\n+            error = error.format(to_native_string(url, 'utf8'))\n+\n+            raise MissingSchema(error)\n+\n+        if not host:\n+            raise InvalidURL(\"Invalid URL %r: No host supplied\" % url)\n+\n+        # Only want to apply IDNA to the hostname\n+        try:\n+            host = host.encode('idna').decode('utf-8')\n+        except UnicodeError:\n+            raise InvalidURL('URL has an invalid label.')\n+\n+        # Carefully reconstruct the network location\n+        netloc = auth or ''\n+        if netloc:\n+            netloc += '@'\n+        netloc += host\n+        if port:\n+            netloc += ':' + str(port)\n+\n+        # Bare domains aren't valid URLs.\n+        if not path:\n+            path = '/'\n+\n+        if is_py2:\n+            if isinstance(scheme, str):\n+                scheme = scheme.encode('utf-8')\n+            if isinstance(netloc, str):\n+                netloc = netloc.encode('utf-8')\n+            if isinstance(path, str):\n+                path = path.encode('utf-8')\n+            if isinstance(query, str):\n+                query = query.encode('utf-8')\n+            if isinstance(fragment, str):\n+                fragment = fragment.encode('utf-8')\n+\n+        enc_params = self._encode_params(params)\n+        if enc_params:\n+            if query:\n+                query = '%s&%s' % (query, enc_params)\n+            else:\n+                query = enc_params\n+\n+        url = requote_uri(urlunparse([scheme, netloc, path, None, query, fragment]))\n+        self.url = url\n+\n+    def prepare_headers(self, headers):\n+        \"\"\"Prepares the given HTTP headers.\"\"\"\n+\n+        if headers:\n+            self.headers = CaseInsensitiveDict((to_native_string(name), value) for name, value in headers.items())\n+        else:\n+            self.headers = CaseInsensitiveDict()\n+\n+    def prepare_body(self, data, files, json=None):\n+        \"\"\"Prepares the given HTTP body data.\"\"\"\n+\n+        # Check if file, fo, generator, iterator.\n+        # If not, run through normal process.\n+\n+        # Nottin' on you.\n+        body = None\n+        content_type = None\n+        length = None\n+\n+        if not data and json is not None:\n+            content_type = 'application/json'\n+            body = complexjson.dumps(json)\n+\n+        is_stream = all([\n+            hasattr(data, '__iter__'),\n+            not isinstance(data, (basestring, list, tuple, dict))\n+        ])\n+\n+        try:\n+            length = super_len(data)\n+        except (TypeError, AttributeError, UnsupportedOperation):\n+            length = None\n+\n+        if is_stream:\n+            body = data\n+\n+            if files:\n+                raise NotImplementedError('Streamed bodies and files are mutually exclusive.')\n+\n+            if length:\n+                self.headers['Content-Length'] = builtin_str(length)\n+            else:\n+                self.headers['Transfer-Encoding'] = 'chunked'\n+        else:\n+            # Multi-part file uploads.\n+            if files:\n+                (body, content_type) = self._encode_files(files, data)\n+            else:\n+                if data:\n+                    body = self._encode_params(data)\n+                    if isinstance(data, basestring) or hasattr(data, 'read'):\n+                        content_type = None\n+                    else:\n+                        content_type = 'application/x-www-form-urlencoded'\n+\n+            self.prepare_content_length(body)\n+\n+            # Add content-type if it wasn't explicitly provided.\n+            if content_type and ('content-type' not in self.headers):\n+                self.headers['Content-Type'] = content_type\n+\n+        self.body = body\n+\n+    def prepare_content_length(self, body):\n+        if hasattr(body, 'seek') and hasattr(body, 'tell'):\n+            body.seek(0, 2)\n+            self.headers['Content-Length'] = builtin_str(body.tell())\n+            body.seek(0, 0)\n+        elif body is not None:\n+            l = super_len(body)\n+            if l:\n+                self.headers['Content-Length'] = builtin_str(l)\n+        elif (self.method not in ('GET', 'HEAD')) and (self.headers.get('Content-Length') is None):\n+            self.headers['Content-Length'] = '0'\n+\n+    def prepare_auth(self, auth, url=''):\n+        \"\"\"Prepares the given HTTP auth data.\"\"\"\n+\n+        # If no Auth is explicitly provided, extract it from the URL first.\n+        if auth is None:\n+            url_auth = get_auth_from_url(self.url)\n+            auth = url_auth if any(url_auth) else None\n+\n+        if auth:\n+            if isinstance(auth, tuple) and len(auth) == 2:\n+                # special-case basic HTTP auth\n+                auth = HTTPBasicAuth(*auth)\n+\n+            # Allow auth to make its changes.\n+            r = auth(self)\n+\n+            # Update self to reflect the auth changes.\n+            self.__dict__.update(r.__dict__)\n+\n+            # Recompute Content-Length\n+            self.prepare_content_length(self.body)\n+\n+    def prepare_cookies(self, cookies):\n+        \"\"\"Prepares the given HTTP cookie data.\n+\n+        This function eventually generates a ``Cookie`` header from the\n+        given cookies using cookielib. Due to cookielib's design, the header\n+        will not be regenerated if it already exists, meaning this function\n+        can only be called once for the life of the\n+        :class:`PreparedRequest <PreparedRequest>` object. Any subsequent calls\n+        to ``prepare_cookies`` will have no actual effect, unless the \"Cookie\"\n+        header is removed beforehand.\"\"\"\n+\n+        if isinstance(cookies, cookielib.CookieJar):\n+            self._cookies = cookies\n+        else:\n+            self._cookies = cookiejar_from_dict(cookies)\n+\n+        cookie_header = get_cookie_header(self._cookies, self)\n+        if cookie_header is not None:\n+            self.headers['Cookie'] = cookie_header\n+\n+    def prepare_hooks(self, hooks):\n+        \"\"\"Prepares the given hooks.\"\"\"\n+        # hooks can be passed as None to the prepare method and to this\n+        # method. To prevent iterating over None, simply use an empty list\n+        # if hooks is False-y\n+        hooks = hooks or []\n+        for event in hooks:\n+            self.register_hook(event, hooks[event])\n+\n+\n+class Response(object):\n+    \"\"\"The :class:`Response <Response>` object, which contains a\n+    server's response to an HTTP request.\n+    \"\"\"\n+\n+    __attrs__ = [\n+        '_content', 'status_code', 'headers', 'url', 'history',\n+        'encoding', 'reason', 'cookies', 'elapsed', 'request'\n+    ]\n+\n+    def __init__(self):\n+        super(Response, self).__init__()\n+\n+        self._content = False\n+        self._content_consumed = False\n+\n+        #: Integer Code of responded HTTP Status, e.g. 404 or 200.\n+        self.status_code = None\n+\n+        #: Case-insensitive Dictionary of Response Headers.\n+        #: For example, ``headers['content-encoding']`` will return the\n+        #: value of a ``'Content-Encoding'`` response header.\n+        self.headers = CaseInsensitiveDict()\n+\n+        #: File-like object representation of response (for advanced usage).\n+        #: Use of ``raw`` requires that ``stream=True`` be set on the request.\n+        # This requirement does not apply for use internally to Requests.\n+        self.raw = None\n+\n+        #: Final URL location of Response.\n+        self.url = None\n+\n+        #: Encoding to decode with when accessing r.text.\n+        self.encoding = None\n+\n+        #: A list of :class:`Response <Response>` objects from\n+        #: the history of the Request. Any redirect responses will end\n+        #: up here. The list is sorted from the oldest to the most recent request.\n+        self.history = []\n+\n+        #: Textual reason of responded HTTP Status, e.g. \"Not Found\" or \"OK\".\n+        self.reason = None\n+\n+        #: A CookieJar of Cookies the server sent back.\n+        self.cookies = cookiejar_from_dict({})\n+\n+        #: The amount of time elapsed between sending the request\n+        #: and the arrival of the response (as a timedelta).\n+        #: This property specifically measures the time taken between sending\n+        #: the first byte of the request and finishing parsing the headers. It\n+        #: is therefore unaffected by consuming the response content or the\n+        #: value of the ``stream`` keyword argument.\n+        self.elapsed = datetime.timedelta(0)\n+\n+        #: The :class:`PreparedRequest <PreparedRequest>` object to which this\n+        #: is a response.\n+        self.request = None\n+\n+    def __getstate__(self):\n+        # Consume everything; accessing the content attribute makes\n+        # sure the content has been fully read.\n+        if not self._content_consumed:\n+            self.content\n+\n+        return dict(\n+            (attr, getattr(self, attr, None))\n+            for attr in self.__attrs__\n+        )\n+\n+    def __setstate__(self, state):\n+        for name, value in state.items():\n+            setattr(self, name, value)\n+\n+        # pickled objects do not have .raw\n+        setattr(self, '_content_consumed', True)\n+        setattr(self, 'raw', None)\n+\n+    def __repr__(self):\n+        return '<Response [%s]>' % (self.status_code)\n+\n+    def __bool__(self):\n+        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n+        return self.ok\n+\n+    def __nonzero__(self):\n+        \"\"\"Returns true if :attr:`status_code` is 'OK'.\"\"\"\n+        return self.ok\n+\n+    def __iter__(self):\n+        \"\"\"Allows you to use a response as an iterator.\"\"\"\n+        return self.iter_content(128)\n+\n+    @property\n+    def ok(self):\n+        try:\n+            self.raise_for_status()\n+        except HTTPError:\n+            return False\n+        return True\n+\n+    @property\n+    def is_redirect(self):\n+        \"\"\"True if this Response is a well-formed HTTP redirect that could have\n+        been processed automatically (by :meth:`Session.resolve_redirects`).\n+        \"\"\"\n+        return ('location' in self.headers and self.status_code in REDIRECT_STATI)\n+\n+    @property\n+    def is_permanent_redirect(self):\n+        \"\"\"True if this Response one of the permanent versions of redirect\"\"\"\n+        return ('location' in self.headers and self.status_code in (codes.moved_permanently, codes.permanent_redirect))\n+\n+    @property\n+    def apparent_encoding(self):\n+        \"\"\"The apparent encoding, provided by the chardet library\"\"\"\n+        return chardet.detect(self.content)['encoding']\n+\n+    def iter_content(self, chunk_size=1, decode_unicode=False):\n+        \"\"\"Iterates over the response data.  When stream=True is set on the\n+        request, this avoids reading the content at once into memory for\n+        large responses.  The chunk size is the number of bytes it should\n+        read into memory.  This is not necessarily the length of each item\n+        returned as decoding can take place.\n+\n+        If decode_unicode is True, content will be decoded using the best\n+        available encoding based on the response.\n+        \"\"\"\n+\n+        def generate():\n+            # Special case for urllib3.\n+            if hasattr(self.raw, 'stream'):\n+                try:\n+                    for chunk in self.raw.stream(chunk_size, decode_content=True):\n+                        yield chunk\n+                except ProtocolError as e:\n+                    raise ChunkedEncodingError(e)\n+                except DecodeError as e:\n+                    raise ContentDecodingError(e)\n+                except ReadTimeoutError as e:\n+                    raise ConnectionError(e)\n+            else:\n+                # Standard file-like object.\n+                while True:\n+                    chunk = self.raw.read(chunk_size)\n+                    if not chunk:\n+                        break\n+                    yield chunk\n+\n+            self._content_consumed = True\n+\n+        if self._content_consumed and isinstance(self._content, bool):\n+            raise StreamConsumedError()\n+        # simulate reading small chunks of the content\n+        reused_chunks = iter_slices(self._content, chunk_size)\n+\n+        stream_chunks = generate()\n+\n+        chunks = reused_chunks if self._content_consumed else stream_chunks\n+\n+        if decode_unicode:\n+            chunks = stream_decode_response_unicode(chunks, self)\n+\n+        return chunks\n+\n+    def iter_lines(self, chunk_size=ITER_CHUNK_SIZE, decode_unicode=None, delimiter=None):\n+        \"\"\"Iterates over the response data, one line at a time.  When\n+        stream=True is set on the request, this avoids reading the\n+        content at once into memory for large responses.\n+\n+        .. note:: This method is not reentrant safe.\n+        \"\"\"\n+\n+        pending = None\n+\n+        for chunk in self.iter_content(chunk_size=chunk_size, decode_unicode=decode_unicode):\n+\n+            if pending is not None:\n+                chunk = pending + chunk\n+\n+            if delimiter:\n+                lines = chunk.split(delimiter)\n+            else:\n+                lines = chunk.splitlines()\n+\n+            if lines and lines[-1] and chunk and lines[-1][-1] == chunk[-1]:\n+                pending = lines.pop()\n+            else:\n+                pending = None\n+\n+            for line in lines:\n+                yield line\n+\n+        if pending is not None:\n+            yield pending\n+\n+    @property\n+    def content(self):\n+        \"\"\"Content of the response, in bytes.\"\"\"\n+\n+        if self._content is False:\n+            # Read the contents.\n+            try:\n+                if self._content_consumed:\n+                    raise RuntimeError(\n+                        'The content for this response was already consumed')\n+\n+                if self.status_code == 0:\n+                    self._content = None\n+                else:\n+                    self._content = bytes().join(self.iter_content(CONTENT_CHUNK_SIZE)) or bytes()\n+\n+            except AttributeError:\n+                self._content = None\n+\n+        self._content_consumed = True\n+        # don't need to release the connection; that's been handled by urllib3\n+        # since we exhausted the data.\n+        return self._content\n+\n+    @property\n+    def text(self):\n+        \"\"\"Content of the response, in unicode.\n+\n+        If Response.encoding is None, encoding will be guessed using\n+        ``chardet``.\n+\n+        The encoding of the response content is determined based solely on HTTP\n+        headers, following RFC 2616 to the letter. If you can take advantage of\n+        non-HTTP knowledge to make a better guess at the encoding, you should\n+        set ``r.encoding`` appropriately before accessing this property.\n+        \"\"\"\n+\n+        # Try charset from content-type\n+        content = None\n+        encoding = self.encoding\n+\n+        if not self.content:\n+            return str('')\n+\n+        # Fallback to auto-detected encoding.\n+        if self.encoding is None:\n+            encoding = self.apparent_encoding\n+\n+        # Decode unicode from given encoding.\n+        try:\n+            content = str(self.content, encoding, errors='replace')\n+        except (LookupError, TypeError):\n+            # A LookupError is raised if the encoding was not found which could\n+            # indicate a misspelling or similar mistake.\n+            #\n+            # A TypeError can be raised if encoding is None\n+            #\n+            # So we try blindly encoding.\n+            content = str(self.content, errors='replace')\n+\n+        return content\n+\n+    def json(self, **kwargs):\n+        \"\"\"Returns the json-encoded content of a response, if any.\n+\n+        :param \\*\\*kwargs: Optional arguments that ``json.loads`` takes.\n+        \"\"\"\n+\n+        if not self.encoding and len(self.content) > 3:\n+            # No encoding set. JSON RFC 4627 section 3 states we should expect\n+            # UTF-8, -16 or -32. Detect which one to use; If the detection or\n+            # decoding fails, fall back to `self.text` (using chardet to make\n+            # a best guess).\n+            encoding = guess_json_utf(self.content)\n+            if encoding is not None:\n+                try:\n+                    return complexjson.loads(\n+                        self.content.decode(encoding), **kwargs\n+                    )\n+                except UnicodeDecodeError:\n+                    # Wrong UTF codec detected; usually because it's not UTF-8\n+                    # but some other 8-bit codec.  This is an RFC violation,\n+                    # and the server didn't bother to tell us what codec *was*\n+                    # used.\n+                    pass\n+        return complexjson.loads(self.text, **kwargs)\n+\n+    @property\n+    def links(self):\n+        \"\"\"Returns the parsed header links of the response, if any.\"\"\"\n+\n+        header = self.headers.get('link')\n+\n+        # l = MultiDict()\n+        l = {}\n+\n+        if header:\n+            links = parse_header_links(header)\n+\n+            for link in links:\n+                key = link.get('rel') or link.get('url')\n+                l[key] = link\n+\n+        return l\n+\n+    def raise_for_status(self):\n+        \"\"\"Raises stored :class:`HTTPError`, if one occurred.\"\"\"\n+\n+        http_error_msg = ''\n+\n+        if 400 <= self.status_code < 500:\n+            http_error_msg = '%s Client Error: %s for url: %s' % (self.status_code, self.reason, self.url)\n+\n+        elif 500 <= self.status_code < 600:\n+            http_error_msg = '%s Server Error: %s for url: %s' % (self.status_code, self.reason, self.url)\n+\n+        if http_error_msg:\n+            raise HTTPError(http_error_msg, response=self)\n+\n+    def close(self):\n+        \"\"\"Releases the connection back to the pool. Once this method has been\n+        called the underlying ``raw`` object must not be accessed again.\n+\n+        *Note: Should not normally need to be called explicitly.*\n+        \"\"\"\n+        if not self._content_consumed:\n+            return self.raw.close()\n+\n+        return self.raw.release_conn()\n"
  },
  "django__django-14011": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-14011",
    "model_patch": "diff --git a/django/core/servers/basehttp.py b/django/core/servers/basehttp.py\nindex 14dab517c9..383c0db230 100644\n--- a/django/core/servers/basehttp.py\n+++ b/django/core/servers/basehttp.py\n@@ -11,12 +11,6 @@ import logging\n import socket\n import socketserver\n import sys\n-from wsgiref import simple_server\n-\n-from django.core.exceptions import ImproperlyConfigured\n-from django.core.handlers.wsgi import LimitedStream\n-from django.core.wsgi import get_wsgi_application\n-from django.utils.module_loading import import_string\n \n __all__ = ('WSGIServer', 'WSGIRequestHandler')\n \ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..383c0db230\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,209 @@\n+\"\"\"\n+HTTP server that implements the Python WSGI protocol (PEP 333, rev 1.21).\n+\n+Based on wsgiref.simple_server which is part of the standard library since 2.5.\n+\n+This is a simple server for use in testing or debugging Django apps. It hasn't\n+been reviewed for security issues. DON'T USE IT FOR PRODUCTION USE!\n+\"\"\"\n+\n+import logging\n+import socket\n+import socketserver\n+import sys\n+\n+__all__ = ('WSGIServer', 'WSGIRequestHandler')\n+\n+logger = logging.getLogger('django.server')\n+\n+\n+def get_internal_wsgi_application():\n+    \"\"\"\n+    Load and return the WSGI application as configured by the user in\n+    ``settings.WSGI_APPLICATION``. With the default ``startproject`` layout,\n+    this will be the ``application`` object in ``projectname/wsgi.py``.\n+\n+    This function, and the ``WSGI_APPLICATION`` setting itself, are only useful\n+    for Django's internal server (runserver); external WSGI servers should just\n+    be configured to point to the correct application object directly.\n+\n+    If settings.WSGI_APPLICATION is not set (is ``None``), return\n+    whatever ``django.core.wsgi.get_wsgi_application`` returns.\n+    \"\"\"\n+    from django.conf import settings\n+    app_path = getattr(settings, 'WSGI_APPLICATION')\n+    if app_path is None:\n+        return get_wsgi_application()\n+\n+    try:\n+        return import_string(app_path)\n+    except ImportError as err:\n+        raise ImproperlyConfigured(\n+            \"WSGI application '%s' could not be loaded; \"\n+            \"Error importing module.\" % app_path\n+        ) from err\n+\n+\n+def is_broken_pipe_error():\n+    exc_type, _, _ = sys.exc_info()\n+    return issubclass(exc_type, (\n+        BrokenPipeError,\n+        ConnectionAbortedError,\n+        ConnectionResetError,\n+    ))\n+\n+\n+class WSGIServer(simple_server.WSGIServer):\n+    \"\"\"BaseHTTPServer that implements the Python WSGI protocol\"\"\"\n+\n+    request_queue_size = 10\n+\n+    def __init__(self, *args, ipv6=False, allow_reuse_address=True, **kwargs):\n+        if ipv6:\n+            self.address_family = socket.AF_INET6\n+        self.allow_reuse_address = allow_reuse_address\n+        super().__init__(*args, **kwargs)\n+\n+    def handle_error(self, request, client_address):\n+        if is_broken_pipe_error():\n+            logger.info(\"- Broken pipe from %s\\n\", client_address)\n+        else:\n+            super().handle_error(request, client_address)\n+\n+\n+class ThreadedWSGIServer(socketserver.ThreadingMixIn, WSGIServer):\n+    \"\"\"A threaded version of the WSGIServer\"\"\"\n+    daemon_threads = True\n+\n+\n+class ServerHandler(simple_server.ServerHandler):\n+    http_version = '1.1'\n+\n+    def __init__(self, stdin, stdout, stderr, environ, **kwargs):\n+        \"\"\"\n+        Use a LimitedStream so that unread request data will be ignored at\n+        the end of the request. WSGIRequest uses a LimitedStream but it\n+        shouldn't discard the data since the upstream servers usually do this.\n+        This fix applies only for testserver/runserver.\n+        \"\"\"\n+        try:\n+            content_length = int(environ.get('CONTENT_LENGTH'))\n+        except (ValueError, TypeError):\n+            content_length = 0\n+        super().__init__(LimitedStream(stdin, content_length), stdout, stderr, environ, **kwargs)\n+\n+    def cleanup_headers(self):\n+        super().cleanup_headers()\n+        # HTTP/1.1 requires support for persistent connections. Send 'close' if\n+        # the content length is unknown to prevent clients from reusing the\n+        # connection.\n+        if 'Content-Length' not in self.headers:\n+            self.headers['Connection'] = 'close'\n+        # Persistent connections require threading server.\n+        elif not isinstance(self.request_handler.server, socketserver.ThreadingMixIn):\n+            self.headers['Connection'] = 'close'\n+        # Mark the connection for closing if it's set as such above or if the\n+        # application sent the header.\n+        if self.headers.get('Connection') == 'close':\n+            self.request_handler.close_connection = True\n+\n+    def close(self):\n+        self.get_stdin()._read_limited()\n+        super().close()\n+\n+\n+class WSGIRequestHandler(simple_server.WSGIRequestHandler):\n+    protocol_version = 'HTTP/1.1'\n+\n+    def address_string(self):\n+        # Short-circuit parent method to not call socket.getfqdn\n+        return self.client_address[0]\n+\n+    def log_message(self, format, *args):\n+        extra = {\n+            'request': self.request,\n+            'server_time': self.log_date_time_string(),\n+        }\n+        if args[1][0] == '4':\n+            # 0x16 = Handshake, 0x03 = SSL 3.0 or TLS 1.x\n+            if args[0].startswith('\\x16\\x03'):\n+                extra['status_code'] = 500\n+                logger.error(\n+                    \"You're accessing the development server over HTTPS, but \"\n+                    \"it only supports HTTP.\\n\", extra=extra,\n+                )\n+                return\n+\n+        if args[1].isdigit() and len(args[1]) == 3:\n+            status_code = int(args[1])\n+            extra['status_code'] = status_code\n+\n+            if status_code >= 500:\n+                level = logger.error\n+            elif status_code >= 400:\n+                level = logger.warning\n+            else:\n+                level = logger.info\n+        else:\n+            level = logger.info\n+\n+        level(format, *args, extra=extra)\n+\n+    def get_environ(self):\n+        # Strip all headers with underscores in the name before constructing\n+        # the WSGI environ. This prevents header-spoofing based on ambiguity\n+        # between underscores and dashes both normalized to underscores in WSGI\n+        # env vars. Nginx and Apache 2.4+ both do this as well.\n+        for k in self.headers:\n+            if '_' in k:\n+                del self.headers[k]\n+\n+        return super().get_environ()\n+\n+    def handle(self):\n+        self.close_connection = True\n+        self.handle_one_request()\n+        while not self.close_connection:\n+            self.handle_one_request()\n+        try:\n+            self.connection.shutdown(socket.SHUT_WR)\n+        except (AttributeError, OSError):\n+            pass\n+\n+    def handle_one_request(self):\n+        \"\"\"Copy of WSGIRequestHandler.handle() but with different ServerHandler\"\"\"\n+        self.raw_requestline = self.rfile.readline(65537)\n+        if len(self.raw_requestline) > 65536:\n+            self.requestline = ''\n+            self.request_version = ''\n+            self.command = ''\n+            self.send_error(414)\n+            return\n+\n+        if not self.parse_request():  # An error code has been sent, just exit\n+            return\n+\n+        handler = ServerHandler(\n+            self.rfile, self.wfile, self.get_stderr(), self.get_environ()\n+        )\n+        handler.request_handler = self      # backpointer for logging & connection closing\n+        handler.run(self.server.get_app())\n+\n+\n+def run(addr, port, wsgi_handler, ipv6=False, threading=False, server_cls=WSGIServer):\n+    server_address = (addr, port)\n+    if threading:\n+        httpd_cls = type('WSGIServer', (socketserver.ThreadingMixIn, server_cls), {})\n+    else:\n+        httpd_cls = server_cls\n+    httpd = httpd_cls(server_address, WSGIRequestHandler, ipv6=ipv6)\n+    if threading:\n+        # ThreadingMixIn.daemon_threads indicates how threads will behave on an\n+        # abrupt shutdown; like quitting the server by the user or restarting\n+        # by the auto-reloader. True means the server will not wait for thread\n+        # termination before it quits. This will make auto-reloader faster\n+        # and will prevent the need to kill the server manually if a thread\n+        # isn't terminating correctly.\n+        httpd.daemon_threads = True\n+    httpd.set_app(wsgi_handler)\n+    httpd.serve_forever()\n"
  },
  "sphinx-doc__sphinx-9658": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-9658",
    "model_patch": "diff --git a/sphinx/util/typing.py b/sphinx/util/typing.py\nindex 87707d48f..93a3715e6 100644\n--- a/sphinx/util/typing.py\n+++ b/sphinx/util/typing.py\n@@ -115,6 +115,12 @@ def restify(cls: Optional[Type]) -> str:\n             return '...'\n         elif isinstance(cls, str):\n             return cls\n+        elif hasattr(cls, '__display_name__'):\n+            # mocked objects created by autodoc_mock_imports carry a\n+            # __display_name__ attribute (module + '.' + name). Use that\n+            # to avoid empty qualname for mocks (see issue with mocked\n+            # base classes showing 'torch.nn.' instead of 'torch.nn.Module').\n+            return ':py:class:`%s`' % getattr(cls, '__display_name__')\n         elif cls in INVALID_BUILTIN_CLASSES:\n             return ':py:class:`%s`' % INVALID_BUILTIN_CLASSES[cls]\n         elif inspect.isNewType(cls):\n@@ -192,6 +198,10 @@ def _restify_py37(cls: Optional[Type]) -> str:\n     elif isinstance(cls, typing._SpecialForm):\n         return ':py:obj:`~%s.%s`' % (cls.__module__, cls._name)\n     elif hasattr(cls, '__qualname__'):\n+        # If the class is a mocked object, it may provide a __display_name__\n+        # which contains the intended module + name. Prefer that if present.\n+        if hasattr(cls, '__display_name__'):\n+            return ':py:class:`%s`' % getattr(cls, '__display_name__')\n         if cls.__module__ == 'typing':\n             return ':py:class:`~%s.%s`' % (cls.__module__, cls.__qualname__)\n         else:\n@@ -220,7 +230,11 @@ def _restify_py36(cls: Optional[Type]) -> str:\n         else:\n             qualname = repr(cls).replace('typing.', '')\n     elif hasattr(cls, '__qualname__'):\n-        qualname = '%s.%s' % (module, cls.__qualname__)\n+        # Prefer display name for mocked classes which may have empty __qualname__.\n+        if hasattr(cls, '__display_name__'):\n+            qualname = getattr(cls, '__display_name__')\n+        else:\n+            qualname = '%s.%s' % (module, cls.__qualname__)\n     else:\n         qualname = repr(cls)\n \n@@ -274,6 +288,10 @@ def _restify_py36(cls: Optional[Type]) -> str:\n         else:\n             return ':py:obj:`Union`'\n     elif hasattr(cls, '__qualname__'):\n+        # If the class is a mocked object, it may provide a __display_name__\n+        # which contains the intended module + name. Prefer that if present.\n+        if hasattr(cls, '__display_name__'):\n+            return ':py:class:`%s`' % getattr(cls, '__display_name__')\n         if cls.__module__ == 'typing':\n             return ':py:class:`~%s.%s`' % (cls.__module__, cls.__qualname__)\n         else:\n"
  },
  "django__django-16631": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "django__django-16631",
    "model_patch": "diff --git a/docs/ref/settings.txt b/docs/ref/settings.txt\nindex 41387886c0..586dea4f6c 100644\n--- a/docs/ref/settings.txt\n+++ b/docs/ref/settings.txt\n@@ -2301,23 +2301,6 @@ Default: ``[]``\n A list of fallback secret keys for a particular Django installation. These are\n used to allow rotation of the ``SECRET_KEY``.\n \n-In order to rotate your secret keys, set a new ``SECRET_KEY`` and move the\n-previous value to the beginning of ``SECRET_KEY_FALLBACKS``. Then remove the\n-old values from the end of the ``SECRET_KEY_FALLBACKS`` when you are ready to\n-expire the sessions, password reset tokens, and so on, that make use of them.\n-\n-.. note::\n-\n-    Signing operations are computationally expensive. Having multiple old key\n-    values in ``SECRET_KEY_FALLBACKS`` adds additional overhead to all checks\n-    that don't match an earlier key.\n-\n-    As such, fallback values should be removed after an appropriate period,\n-    allowing for key rotation.\n-\n-Uses of the secret key values shouldn't assume that they are text or bytes.\n-Every use should go through :func:`~django.utils.encoding.force_str` or\n-:func:`~django.utils.encoding.force_bytes` to convert it to the desired type.\n \n .. setting:: SECURE_CONTENT_TYPE_NOSNIFF\n \ndiff --git a/temp_file.txt b/temp_file.txt\nnew file mode 100644\nindex 0000000000..586dea4f6c\n--- /dev/null\n+++ b/temp_file.txt\n@@ -0,0 +1,3766 @@\n+========\n+Settings\n+========\n+\n+.. contents::\n+    :local:\n+    :depth: 1\n+\n+.. warning::\n+\n+    Be careful when you override settings, especially when the default value\n+    is a non-empty list or dictionary, such as :setting:`STATICFILES_FINDERS`.\n+    Make sure you keep the components required by the features of Django you\n+    wish to use.\n+\n+Core Settings\n+=============\n+\n+Here's a list of settings available in Django core and their default values.\n+Settings provided by contrib apps are listed below, followed by a topical index\n+of the core settings. For introductory material, see the :doc:`settings topic\n+guide </topics/settings>`.\n+\n+.. setting:: ABSOLUTE_URL_OVERRIDES\n+\n+``ABSOLUTE_URL_OVERRIDES``\n+--------------------------\n+\n+Default: ``{}`` (Empty dictionary)\n+\n+A dictionary mapping ``\"app_label.model_name\"`` strings to functions that take\n+a model object and return its URL. This is a way of inserting or overriding\n+``get_absolute_url()`` methods on a per-installation basis. Example::\n+\n+    ABSOLUTE_URL_OVERRIDES = {\n+        \"blogs.blog\": lambda o: \"/blogs/%s/\" % o.slug,\n+        \"news.story\": lambda o: \"/stories/%s/%s/\" % (o.pub_year, o.slug),\n+    }\n+\n+The model name used in this setting should be all lowercase, regardless of the\n+case of the actual model class name.\n+\n+.. setting:: ADMINS\n+\n+``ADMINS``\n+----------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of all the people who get code error notifications. When\n+:setting:`DEBUG=False <DEBUG>` and :class:`~django.utils.log.AdminEmailHandler`\n+is configured in :setting:`LOGGING` (done by default), Django emails these\n+people the details of exceptions raised in the request/response cycle.\n+\n+Each item in the list should be a tuple of (Full name, email address). Example::\n+\n+    [(\"John\", \"john@example.com\"), (\"Mary\", \"mary@example.com\")]\n+\n+.. setting:: ALLOWED_HOSTS\n+\n+``ALLOWED_HOSTS``\n+-----------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of strings representing the host/domain names that this Django site can\n+serve. This is a security measure to prevent :ref:`HTTP Host header attacks\n+<host-headers-virtual-hosting>`, which are possible even under many\n+seemingly-safe web server configurations.\n+\n+Values in this list can be fully qualified names (e.g. ``'www.example.com'``),\n+in which case they will be matched against the request's ``Host`` header\n+exactly (case-insensitive, not including port). A value beginning with a period\n+can be used as a subdomain wildcard: ``'.example.com'`` will match\n+``example.com``, ``www.example.com``, and any other subdomain of\n+``example.com``. A value of ``'*'`` will match anything; in this case you are\n+responsible to provide your own validation of the ``Host`` header (perhaps in a\n+middleware; if so this middleware must be listed first in\n+:setting:`MIDDLEWARE`).\n+\n+Django also allows the `fully qualified domain name (FQDN)`_ of any entries.\n+Some browsers include a trailing dot in the ``Host`` header which Django\n+strips when performing host validation.\n+\n+.. _`fully qualified domain name (FQDN)`: https://en.wikipedia.org/wiki/Fully_qualified_domain_name\n+\n+If the ``Host`` header (or ``X-Forwarded-Host`` if\n+:setting:`USE_X_FORWARDED_HOST` is enabled) does not match any value in this\n+list, the :meth:`django.http.HttpRequest.get_host()` method will raise\n+:exc:`~django.core.exceptions.SuspiciousOperation`.\n+\n+When :setting:`DEBUG` is ``True`` and ``ALLOWED_HOSTS`` is empty, the host\n+is validated against ``['.localhost', '127.0.0.1', '[::1]']``.\n+\n+``ALLOWED_HOSTS`` is also :ref:`checked when running tests\n+<topics-testing-advanced-multiple-hosts>`.\n+\n+This validation only applies via :meth:`~django.http.HttpRequest.get_host()`;\n+if your code accesses the ``Host`` header directly from ``request.META`` you\n+are bypassing this security protection.\n+\n+.. setting:: APPEND_SLASH\n+\n+``APPEND_SLASH``\n+----------------\n+\n+Default: ``True``\n+\n+When set to ``True``, if the request URL does not match any of the patterns\n+in the URLconf and it doesn't end in a slash, an HTTP redirect is issued to the\n+same URL with a slash appended. Note that the redirect may cause any data\n+submitted in a POST request to be lost.\n+\n+The :setting:`APPEND_SLASH` setting is only used if\n+:class:`~django.middleware.common.CommonMiddleware` is installed\n+(see :doc:`/topics/http/middleware`). See also :setting:`PREPEND_WWW`.\n+\n+.. setting:: CACHES\n+\n+``CACHES``\n+----------\n+\n+Default::\n+\n+    {\n+        \"default\": {\n+            \"BACKEND\": \"django.core.cache.backends.locmem.LocMemCache\",\n+        }\n+    }\n+\n+A dictionary containing the settings for all caches to be used with\n+Django. It is a nested dictionary whose contents maps cache aliases\n+to a dictionary containing the options for an individual cache.\n+\n+The :setting:`CACHES` setting must configure a ``default`` cache;\n+any number of additional caches may also be specified. If you\n+are using a cache backend other than the local memory cache, or\n+you need to define multiple caches, other options will be required.\n+The following cache options are available.\n+\n+.. setting:: CACHES-BACKEND\n+\n+``BACKEND``\n+~~~~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The cache backend to use. The built-in cache backends are:\n+\n+* ``'django.core.cache.backends.db.DatabaseCache'``\n+* ``'django.core.cache.backends.dummy.DummyCache'``\n+* ``'django.core.cache.backends.filebased.FileBasedCache'``\n+* ``'django.core.cache.backends.locmem.LocMemCache'``\n+* ``'django.core.cache.backends.memcached.PyMemcacheCache'``\n+* ``'django.core.cache.backends.memcached.PyLibMCCache'``\n+* ``'django.core.cache.backends.redis.RedisCache'``\n+\n+You can use a cache backend that doesn't ship with Django by setting\n+:setting:`BACKEND <CACHES-BACKEND>` to a fully-qualified path of a cache\n+backend class (i.e. ``mypackage.backends.whatever.WhateverCache``).\n+\n+.. setting:: CACHES-KEY_FUNCTION\n+\n+``KEY_FUNCTION``\n+~~~~~~~~~~~~~~~~\n+\n+A string containing a dotted path to a function (or any callable) that defines how to\n+compose a prefix, version and key into a final cache key. The default\n+implementation is equivalent to the function::\n+\n+    def make_key(key, key_prefix, version):\n+        return \":\".join([key_prefix, str(version), key])\n+\n+You may use any key function you want, as long as it has the same\n+argument signature.\n+\n+See the :ref:`cache documentation <cache_key_transformation>` for more\n+information.\n+\n+.. setting:: CACHES-KEY_PREFIX\n+\n+``KEY_PREFIX``\n+~~~~~~~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+A string that will be automatically included (prepended by default) to\n+all cache keys used by the Django server.\n+\n+See the :ref:`cache documentation <cache_key_prefixing>` for more information.\n+\n+.. setting:: CACHES-LOCATION\n+\n+``LOCATION``\n+~~~~~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The location of the cache to use. This might be the directory for a\n+file system cache, a host and port for a memcache server, or an identifying\n+name for a local memory cache. e.g.::\n+\n+    CACHES = {\n+        \"default\": {\n+            \"BACKEND\": \"django.core.cache.backends.filebased.FileBasedCache\",\n+            \"LOCATION\": \"/var/tmp/django_cache\",\n+        }\n+    }\n+\n+.. setting:: CACHES-OPTIONS\n+\n+``OPTIONS``\n+~~~~~~~~~~~\n+\n+Default: ``None``\n+\n+Extra parameters to pass to the cache backend. Available parameters\n+vary depending on your cache backend.\n+\n+Some information on available parameters can be found in the\n+:ref:`cache arguments <cache_arguments>` documentation. For more information,\n+consult your backend module's own documentation.\n+\n+.. setting:: CACHES-TIMEOUT\n+\n+``TIMEOUT``\n+~~~~~~~~~~~\n+\n+Default: ``300``\n+\n+The number of seconds before a cache entry is considered stale. If the value of\n+this setting is ``None``, cache entries will not expire. A value of ``0``\n+causes keys to immediately expire (effectively \"don't cache\").\n+\n+.. setting:: CACHES-VERSION\n+\n+``VERSION``\n+~~~~~~~~~~~\n+\n+Default: ``1``\n+\n+The default version number for cache keys generated by the Django server.\n+\n+See the :ref:`cache documentation <cache_versioning>` for more information.\n+\n+.. setting:: CACHE_MIDDLEWARE_ALIAS\n+\n+``CACHE_MIDDLEWARE_ALIAS``\n+--------------------------\n+\n+Default: ``'default'``\n+\n+The cache connection to use for the :ref:`cache middleware\n+<the-per-site-cache>`.\n+\n+.. setting:: CACHE_MIDDLEWARE_KEY_PREFIX\n+\n+``CACHE_MIDDLEWARE_KEY_PREFIX``\n+-------------------------------\n+\n+Default: ``''`` (Empty string)\n+\n+A string which will be prefixed to the cache keys generated by the :ref:`cache\n+middleware <the-per-site-cache>`. This prefix is combined with the\n+:setting:`KEY_PREFIX <CACHES-KEY_PREFIX>` setting; it does not replace it.\n+\n+See :doc:`/topics/cache`.\n+\n+.. setting:: CACHE_MIDDLEWARE_SECONDS\n+\n+``CACHE_MIDDLEWARE_SECONDS``\n+----------------------------\n+\n+Default: ``600``\n+\n+The default number of seconds to cache a page for the :ref:`cache middleware\n+<the-per-site-cache>`.\n+\n+See :doc:`/topics/cache`.\n+\n+.. _settings-csrf:\n+\n+.. setting:: CSRF_COOKIE_AGE\n+\n+``CSRF_COOKIE_AGE``\n+-------------------\n+\n+Default: ``31449600`` (approximately 1 year, in seconds)\n+\n+The age of CSRF cookies, in seconds.\n+\n+The reason for setting a long-lived expiration time is to avoid problems in\n+the case of a user closing a browser or bookmarking a page and then loading\n+that page from a browser cache. Without persistent cookies, the form submission\n+would fail in this case.\n+\n+Some browsers (specifically Internet Explorer) can disallow the use of\n+persistent cookies or can have the indexes to the cookie jar corrupted on disk,\n+thereby causing CSRF protection checks to (sometimes intermittently) fail.\n+Change this setting to ``None`` to use session-based CSRF cookies, which\n+keep the cookies in-memory instead of on persistent storage.\n+\n+.. setting:: CSRF_COOKIE_DOMAIN\n+\n+``CSRF_COOKIE_DOMAIN``\n+----------------------\n+\n+Default: ``None``\n+\n+The domain to be used when setting the CSRF cookie.  This can be useful for\n+easily allowing cross-subdomain requests to be excluded from the normal cross\n+site request forgery protection.  It should be set to a string such as\n+``\".example.com\"`` to allow a POST request from a form on one subdomain to be\n+accepted by a view served from another subdomain.\n+\n+Please note that the presence of this setting does not imply that Django's CSRF\n+protection is safe from cross-subdomain attacks by default - please see the\n+:ref:`CSRF limitations <csrf-limitations>` section.\n+\n+.. setting:: CSRF_COOKIE_HTTPONLY\n+\n+``CSRF_COOKIE_HTTPONLY``\n+------------------------\n+\n+Default: ``False``\n+\n+Whether to use ``HttpOnly`` flag on the CSRF cookie. If this is set to\n+``True``, client-side JavaScript will not be able to access the CSRF cookie.\n+\n+Designating the CSRF cookie as ``HttpOnly`` doesn't offer any practical\n+protection because CSRF is only to protect against cross-domain attacks. If an\n+attacker can read the cookie via JavaScript, they're already on the same domain\n+as far as the browser knows, so they can do anything they like anyway. (XSS is\n+a much bigger hole than CSRF.)\n+\n+Although the setting offers little practical benefit, it's sometimes required\n+by security auditors.\n+\n+If you enable this and need to send the value of the CSRF token with an AJAX\n+request, your JavaScript must pull the value :ref:`from a hidden CSRF token\n+form input <acquiring-csrf-token-from-html>` instead of :ref:`from the cookie\n+<acquiring-csrf-token-from-cookie>`.\n+\n+See :setting:`SESSION_COOKIE_HTTPONLY` for details on ``HttpOnly``.\n+\n+.. setting:: CSRF_COOKIE_NAME\n+\n+``CSRF_COOKIE_NAME``\n+--------------------\n+\n+Default: ``'csrftoken'``\n+\n+The name of the cookie to use for the CSRF authentication token. This can be\n+whatever you want (as long as it's different from the other cookie names in\n+your application). See :doc:`/ref/csrf`.\n+\n+.. setting:: CSRF_COOKIE_PATH\n+\n+``CSRF_COOKIE_PATH``\n+--------------------\n+\n+Default: ``'/'``\n+\n+The path set on the CSRF cookie. This should either match the URL path of your\n+Django installation or be a parent of that path.\n+\n+This is useful if you have multiple Django instances running under the same\n+hostname. They can use different cookie paths, and each instance will only see\n+its own CSRF cookie.\n+\n+.. setting:: CSRF_COOKIE_SAMESITE\n+\n+``CSRF_COOKIE_SAMESITE``\n+------------------------\n+\n+Default: ``'Lax'``\n+\n+The value of the `SameSite`_ flag on the CSRF cookie. This flag prevents the\n+cookie from being sent in cross-site requests.\n+\n+See :setting:`SESSION_COOKIE_SAMESITE` for details about ``SameSite``.\n+\n+.. setting:: CSRF_COOKIE_SECURE\n+\n+``CSRF_COOKIE_SECURE``\n+----------------------\n+\n+Default: ``False``\n+\n+Whether to use a secure cookie for the CSRF cookie. If this is set to ``True``,\n+the cookie will be marked as \"secure\", which means browsers may ensure that the\n+cookie is only sent with an HTTPS connection.\n+\n+.. setting:: CSRF_USE_SESSIONS\n+\n+``CSRF_USE_SESSIONS``\n+---------------------\n+\n+Default: ``False``\n+\n+Whether to store the CSRF token in the user's session instead of in a cookie.\n+It requires the use of :mod:`django.contrib.sessions`.\n+\n+Storing the CSRF token in a cookie (Django's default) is safe, but storing it\n+in the session is common practice in other web frameworks and therefore\n+sometimes demanded by security auditors.\n+\n+Since the :ref:`default error views <error-views>` require the CSRF token,\n+:class:`~django.contrib.sessions.middleware.SessionMiddleware` must appear in\n+:setting:`MIDDLEWARE` before any middleware that may raise an exception to\n+trigger an error view (such as :exc:`~django.core.exceptions.PermissionDenied`)\n+if you're using ``CSRF_USE_SESSIONS``. See :ref:`middleware-ordering`.\n+\n+.. setting:: CSRF_FAILURE_VIEW\n+\n+``CSRF_FAILURE_VIEW``\n+---------------------\n+\n+Default: ``'django.views.csrf.csrf_failure'``\n+\n+A dotted path to the view function to be used when an incoming request is\n+rejected by the :doc:`CSRF protection </ref/csrf>`. The function should have\n+this signature::\n+\n+    def csrf_failure(request, reason=\"\"):\n+        ...\n+\n+where ``reason`` is a short message (intended for developers or logging, not\n+for end users) indicating the reason the request was rejected. It should return\n+an :class:`~django.http.HttpResponseForbidden`.\n+\n+``django.views.csrf.csrf_failure()`` accepts an additional ``template_name``\n+parameter that defaults to ``'403_csrf.html'``. If a template with that name\n+exists, it will be used to render the page.\n+\n+.. setting:: CSRF_HEADER_NAME\n+\n+``CSRF_HEADER_NAME``\n+--------------------\n+\n+Default: ``'HTTP_X_CSRFTOKEN'``\n+\n+The name of the request header used for CSRF authentication.\n+\n+As with other HTTP headers in ``request.META``, the header name received from\n+the server is normalized by converting all characters to uppercase, replacing\n+any hyphens with underscores, and adding an ``'HTTP_'`` prefix to the name.\n+For example, if your client sends a ``'X-XSRF-TOKEN'`` header, the setting\n+should be ``'HTTP_X_XSRF_TOKEN'``.\n+\n+.. setting:: CSRF_TRUSTED_ORIGINS\n+\n+``CSRF_TRUSTED_ORIGINS``\n+------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of trusted origins for unsafe requests (e.g. ``POST``).\n+\n+For requests that include the ``Origin`` header, Django's CSRF protection\n+requires that header match the origin present in the ``Host`` header.\n+\n+For a :meth:`secure <django.http.HttpRequest.is_secure>` unsafe\n+request that doesn't include the ``Origin`` header, the request must have a\n+``Referer`` header that matches the origin present in the ``Host`` header.\n+\n+These checks prevent, for example, a ``POST`` request from\n+``subdomain.example.com`` from succeeding against ``api.example.com``. If you\n+need cross-origin unsafe requests, continuing the example, add\n+``'https://subdomain.example.com'`` to this list (and/or ``http://...`` if\n+requests originate from an insecure page).\n+\n+The setting also supports subdomains, so you could add\n+``'https://*.example.com'``, for example, to allow access from all subdomains\n+of ``example.com``.\n+\n+.. setting:: DATABASES\n+\n+``DATABASES``\n+-------------\n+\n+Default: ``{}`` (Empty dictionary)\n+\n+A dictionary containing the settings for all databases to be used with\n+Django. It is a nested dictionary whose contents map a database alias\n+to a dictionary containing the options for an individual database.\n+\n+The :setting:`DATABASES` setting must configure a ``default`` database;\n+any number of additional databases may also be specified.\n+\n+The simplest possible settings file is for a single-database setup using\n+SQLite. This can be configured using the following::\n+\n+    DATABASES = {\n+        \"default\": {\n+            \"ENGINE\": \"django.db.backends.sqlite3\",\n+            \"NAME\": \"mydatabase\",\n+        }\n+    }\n+\n+When connecting to other database backends, such as MariaDB, MySQL, Oracle, or\n+PostgreSQL, additional connection parameters will be required. See\n+the :setting:`ENGINE <DATABASE-ENGINE>` setting below on how to specify\n+other database types. This example is for PostgreSQL::\n+\n+    DATABASES = {\n+        \"default\": {\n+            \"ENGINE\": \"django.db.backends.postgresql\",\n+            \"NAME\": \"mydatabase\",\n+            \"USER\": \"mydatabaseuser\",\n+            \"PASSWORD\": \"mypassword\",\n+            \"HOST\": \"127.0.0.1\",\n+            \"PORT\": \"5432\",\n+        }\n+    }\n+\n+The following inner options that may be required for more complex\n+configurations are available:\n+\n+.. setting:: DATABASE-ATOMIC_REQUESTS\n+\n+``ATOMIC_REQUESTS``\n+~~~~~~~~~~~~~~~~~~~\n+\n+Default: ``False``\n+\n+Set this to ``True`` to wrap each view in a transaction on this database. See\n+:ref:`tying-transactions-to-http-requests`.\n+\n+.. setting:: DATABASE-AUTOCOMMIT\n+\n+``AUTOCOMMIT``\n+~~~~~~~~~~~~~~\n+\n+Default: ``True``\n+\n+Set this to ``False`` if you want to :ref:`disable Django's transaction\n+management <deactivate-transaction-management>` and implement your own.\n+\n+.. setting:: DATABASE-ENGINE\n+\n+``ENGINE``\n+~~~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The database backend to use. The built-in database backends are:\n+\n+* ``'django.db.backends.postgresql'``\n+* ``'django.db.backends.mysql'``\n+* ``'django.db.backends.sqlite3'``\n+* ``'django.db.backends.oracle'``\n+\n+You can use a database backend that doesn't ship with Django by setting\n+``ENGINE`` to a fully-qualified path (i.e. ``mypackage.backends.whatever``).\n+\n+.. setting:: HOST\n+\n+``HOST``\n+~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+Which host to use when connecting to the database. An empty string means\n+localhost. Not used with SQLite.\n+\n+If this value starts with a forward slash (``'/'``) and you're using MySQL,\n+MySQL will connect via a Unix socket to the specified socket. For example::\n+\n+    \"HOST\": \"/var/run/mysql\"\n+\n+If you're using MySQL and this value *doesn't* start with a forward slash, then\n+this value is assumed to be the host.\n+\n+If you're using PostgreSQL, by default (empty :setting:`HOST`), the connection\n+to the database is done through UNIX domain sockets ('local' lines in\n+``pg_hba.conf``). If your UNIX domain socket is not in the standard location,\n+use the same value of ``unix_socket_directory`` from ``postgresql.conf``.\n+If you want to connect through TCP sockets, set :setting:`HOST` to 'localhost'\n+or '127.0.0.1' ('host' lines in ``pg_hba.conf``).\n+On Windows, you should always define :setting:`HOST`, as UNIX domain sockets\n+are not available.\n+\n+.. setting:: NAME\n+\n+``NAME``\n+~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The name of the database to use. For SQLite, it's the full path to the database\n+file. When specifying the path, always use forward slashes, even on Windows\n+(e.g. ``C:/homes/user/mysite/sqlite3.db``).\n+\n+.. setting:: CONN_MAX_AGE\n+\n+``CONN_MAX_AGE``\n+~~~~~~~~~~~~~~~~\n+\n+Default: ``0``\n+\n+The lifetime of a database connection, as an integer of seconds. Use ``0`` to\n+close database connections at the end of each request \u2014 Django's historical\n+behavior \u2014 and ``None`` for unlimited :ref:`persistent database connections\n+<persistent-database-connections>`.\n+\n+.. setting:: CONN_HEALTH_CHECKS\n+\n+``CONN_HEALTH_CHECKS``\n+~~~~~~~~~~~~~~~~~~~~~~\n+\n+Default: ``False``\n+\n+If set to ``True``, existing :ref:`persistent database connections\n+<persistent-database-connections>` will be health checked before they are\n+reused in each request performing database access. If the health check fails,\n+the connection will be reestablished without failing the request when the\n+connection is no longer usable but the database server is ready to accept and\n+serve new connections (e.g. after database server restart closing existing\n+connections).\n+\n+.. setting:: OPTIONS\n+\n+``OPTIONS``\n+~~~~~~~~~~~\n+\n+Default: ``{}`` (Empty dictionary)\n+\n+Extra parameters to use when connecting to the database. Available parameters\n+vary depending on your database backend.\n+\n+Some information on available parameters can be found in the\n+:doc:`Database Backends </ref/databases>` documentation. For more information,\n+consult your backend module's own documentation.\n+\n+.. setting:: PASSWORD\n+\n+``PASSWORD``\n+~~~~~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The password to use when connecting to the database. Not used with SQLite.\n+\n+.. setting:: PORT\n+\n+``PORT``\n+~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The port to use when connecting to the database. An empty string means the\n+default port. Not used with SQLite.\n+\n+.. setting:: DATABASE-TIME_ZONE\n+\n+``TIME_ZONE``\n+~~~~~~~~~~~~~\n+\n+Default: ``None``\n+\n+A string representing the time zone for this database connection or ``None``.\n+This inner option of the :setting:`DATABASES` setting accepts the same values\n+as the general :setting:`TIME_ZONE` setting.\n+\n+When :setting:`USE_TZ` is ``True`` and this option is set, reading datetimes\n+from the database returns aware datetimes in this time zone instead of UTC.\n+When :setting:`USE_TZ` is ``False``, it is an error to set this option.\n+\n+* If the database backend doesn't support time zones (e.g. SQLite, MySQL,\n+  Oracle), Django reads and writes datetimes in local time according to this\n+  option if it is set and in UTC if it isn't.\n+\n+  Changing the connection time zone changes how datetimes are read from and\n+  written to the database.\n+\n+  * If Django manages the database and you don't have a strong reason to do\n+    otherwise, you should leave this option unset. It's best to store datetimes\n+    in UTC because it avoids ambiguous or nonexistent datetimes during daylight\n+    saving time changes. Also, receiving datetimes in UTC keeps datetime\n+    arithmetic simple \u2014 there's no need to consider potential offset changes\n+    over a DST transition.\n+\n+  * If you're connecting to a third-party database that stores datetimes in a\n+    local time rather than UTC, then you must set this option to the\n+    appropriate time zone. Likewise, if Django manages the database but\n+    third-party systems connect to the same database and expect to find\n+    datetimes in local time, then you must set this option.\n+\n+* If the database backend supports time zones (e.g. PostgreSQL), the\n+  ``TIME_ZONE`` option is very rarely needed. It can be changed at any time;\n+  the database takes care of converting datetimes to the desired time zone.\n+\n+  Setting the time zone of the database connection may be useful for running\n+  raw SQL queries involving date/time functions provided by the database, such\n+  as ``date_trunc``, because their results depend on the time zone.\n+\n+  However, this has a downside: receiving all datetimes in local time makes\n+  datetime arithmetic more tricky \u2014 you must account for possible offset\n+  changes over DST transitions.\n+\n+  Consider converting to local time explicitly with ``AT TIME ZONE`` in raw SQL\n+  queries instead of setting the ``TIME_ZONE`` option.\n+\n+.. setting:: DATABASE-DISABLE_SERVER_SIDE_CURSORS\n+\n+``DISABLE_SERVER_SIDE_CURSORS``\n+~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n+\n+Default: ``False``\n+\n+Set this to ``True`` if you want to disable the use of server-side cursors with\n+:meth:`.QuerySet.iterator`. :ref:`transaction-pooling-server-side-cursors`\n+describes the use case.\n+\n+This is a PostgreSQL-specific setting.\n+\n+.. setting:: USER\n+\n+``USER``\n+~~~~~~~~\n+\n+Default: ``''`` (Empty string)\n+\n+The username to use when connecting to the database. Not used with SQLite.\n+\n+.. setting:: DATABASE-TEST\n+\n+``TEST``\n+~~~~~~~~\n+\n+Default: ``{}`` (Empty dictionary)\n+\n+A dictionary of settings for test databases; for more details about the\n+creation and use of test databases, see :ref:`the-test-database`.\n+\n+Here's an example with a test database configuration::\n+\n+    DATABASES = {\n+        \"default\": {\n+            \"ENGINE\": \"django.db.backends.postgresql\",\n+            \"USER\": \"mydatabaseuser\",\n+            \"NAME\": \"mydatabase\",\n+            \"TEST\": {\n+                \"NAME\": \"mytestdatabase\",\n+            },\n+        },\n+    }\n+\n+The following keys in the ``TEST`` dictionary are available:\n+\n+.. setting:: TEST_CHARSET\n+\n+``CHARSET``\n+^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+The character set encoding used to create the test database. The value of this\n+string is passed directly through to the database, so its format is\n+backend-specific.\n+\n+Supported by the PostgreSQL_ (``postgresql``) and MySQL_ (``mysql``) backends.\n+\n+.. _PostgreSQL: https://www.postgresql.org/docs/current/multibyte.html\n+.. _MySQL: https://dev.mysql.com/doc/refman/en/charset-charsets.html\n+\n+.. setting:: TEST_COLLATION\n+\n+``COLLATION``\n+^^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+The collation order to use when creating the test database. This value is\n+passed directly to the backend, so its format is backend-specific.\n+\n+Only supported for the ``mysql`` backend (see the `MySQL manual`_ for details).\n+\n+.. _MySQL manual: MySQL_\n+\n+.. setting:: TEST_DEPENDENCIES\n+\n+``DEPENDENCIES``\n+^^^^^^^^^^^^^^^^\n+\n+Default: ``['default']``, for all databases other than ``default``,\n+which has no dependencies.\n+\n+The creation-order dependencies of the database. See the documentation\n+on :ref:`controlling the creation order of test databases\n+<topics-testing-creation-dependencies>` for details.\n+\n+.. setting:: TEST_MIGRATE\n+\n+``MIGRATE``\n+^^^^^^^^^^^\n+\n+Default: ``True``\n+\n+When set to ``False``, migrations won't run when creating the test database.\n+This is similar to setting ``None`` as a value in :setting:`MIGRATION_MODULES`,\n+but for all apps.\n+\n+.. setting:: TEST_MIRROR\n+\n+``MIRROR``\n+^^^^^^^^^^\n+\n+Default: ``None``\n+\n+The alias of the database that this database should mirror during\n+testing. It depends on transactions and therefore must be used within\n+:class:`~django.test.TransactionTestCase` instead of\n+:class:`~django.test.TestCase`.\n+\n+This setting exists to allow for testing of primary/replica\n+(referred to as master/slave by some databases)\n+configurations of multiple databases. See the documentation on\n+:ref:`testing primary/replica configurations\n+<topics-testing-primaryreplica>` for details.\n+\n+.. setting:: TEST_NAME\n+\n+``NAME``\n+^^^^^^^^\n+\n+Default: ``None``\n+\n+The name of database to use when running the test suite.\n+\n+If the default value (``None``) is used with the SQLite database engine, the\n+tests will use a memory resident database. For all other database engines the\n+test database will use the name ``'test_' + DATABASE_NAME``.\n+\n+See :ref:`the-test-database`.\n+\n+.. setting:: TEST_TEMPLATE\n+\n+``TEMPLATE``\n+^^^^^^^^^^^^\n+\n+This is a PostgreSQL-specific setting.\n+\n+The name of a `template`_ (e.g. ``'template0'``) from which to create the test\n+database.\n+\n+.. _template: https://www.postgresql.org/docs/current/sql-createdatabase.html\n+\n+.. setting:: TEST_CREATE\n+\n+``CREATE_DB``\n+^^^^^^^^^^^^^\n+\n+Default: ``True``\n+\n+This is an Oracle-specific setting.\n+\n+If it is set to ``False``, the test tablespaces won't be automatically created\n+at the beginning of the tests or dropped at the end.\n+\n+.. setting:: TEST_USER_CREATE\n+\n+``CREATE_USER``\n+^^^^^^^^^^^^^^^\n+\n+Default: ``True``\n+\n+This is an Oracle-specific setting.\n+\n+If it is set to ``False``, the test user won't be automatically created at the\n+beginning of the tests and dropped at the end.\n+\n+.. setting:: TEST_USER\n+\n+``USER``\n+^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The username to use when connecting to the Oracle database that will be used\n+when running tests. If not provided, Django will use ``'test_' + USER``.\n+\n+.. setting:: TEST_PASSWD\n+\n+``PASSWORD``\n+^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The password to use when connecting to the Oracle database that will be used\n+when running tests. If not provided, Django will generate a random password.\n+\n+.. setting:: TEST_ORACLE_MANAGED_FILES\n+\n+``ORACLE_MANAGED_FILES``\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``False``\n+\n+This is an Oracle-specific setting.\n+\n+If set to ``True``, Oracle Managed Files (OMF) tablespaces will be used.\n+:setting:`DATAFILE` and :setting:`DATAFILE_TMP` will be ignored.\n+\n+.. setting:: TEST_TBLSPACE\n+\n+``TBLSPACE``\n+^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The name of the tablespace that will be used when running tests. If not\n+provided, Django will use ``'test_' + USER``.\n+\n+.. setting:: TEST_TBLSPACE_TMP\n+\n+``TBLSPACE_TMP``\n+^^^^^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The name of the temporary tablespace that will be used when running tests. If\n+not provided, Django will use ``'test_' + USER + '_temp'``.\n+\n+.. setting:: DATAFILE\n+\n+``DATAFILE``\n+^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The name of the datafile to use for the TBLSPACE. If not provided, Django will\n+use ``TBLSPACE + '.dbf'``.\n+\n+.. setting:: DATAFILE_TMP\n+\n+``DATAFILE_TMP``\n+^^^^^^^^^^^^^^^^\n+\n+Default: ``None``\n+\n+This is an Oracle-specific setting.\n+\n+The name of the datafile to use for the TBLSPACE_TMP. If not provided, Django\n+will use ``TBLSPACE_TMP + '.dbf'``.\n+\n+.. setting:: DATAFILE_MAXSIZE\n+\n+``DATAFILE_MAXSIZE``\n+^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``'500M'``\n+\n+This is an Oracle-specific setting.\n+\n+The maximum size that the DATAFILE is allowed to grow to.\n+\n+.. setting:: DATAFILE_TMP_MAXSIZE\n+\n+``DATAFILE_TMP_MAXSIZE``\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``'500M'``\n+\n+This is an Oracle-specific setting.\n+\n+The maximum size that the DATAFILE_TMP is allowed to grow to.\n+\n+.. setting:: DATAFILE_SIZE\n+\n+``DATAFILE_SIZE``\n+^^^^^^^^^^^^^^^^^\n+\n+Default: ``'50M'``\n+\n+This is an Oracle-specific setting.\n+\n+The initial size of the DATAFILE.\n+\n+.. setting:: DATAFILE_TMP_SIZE\n+\n+``DATAFILE_TMP_SIZE``\n+^^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``'50M'``\n+\n+This is an Oracle-specific setting.\n+\n+The initial size of the DATAFILE_TMP.\n+\n+.. setting:: DATAFILE_EXTSIZE\n+\n+``DATAFILE_EXTSIZE``\n+^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``'25M'``\n+\n+This is an Oracle-specific setting.\n+\n+The amount by which the DATAFILE is extended when more space is required.\n+\n+.. setting:: DATAFILE_TMP_EXTSIZE\n+\n+``DATAFILE_TMP_EXTSIZE``\n+^^^^^^^^^^^^^^^^^^^^^^^^\n+\n+Default: ``'25M'``\n+\n+This is an Oracle-specific setting.\n+\n+The amount by which the DATAFILE_TMP is extended when more space is required.\n+\n+.. setting:: DATA_UPLOAD_MAX_MEMORY_SIZE\n+\n+``DATA_UPLOAD_MAX_MEMORY_SIZE``\n+-------------------------------\n+\n+Default: ``2621440`` (i.e. 2.5 MB).\n+\n+The maximum size in bytes that a request body may be before a\n+:exc:`~django.core.exceptions.SuspiciousOperation` (``RequestDataTooBig``) is\n+raised. The check is done when accessing ``request.body`` or ``request.POST``\n+and is calculated against the total request size excluding any file upload\n+data. You can set this to ``None`` to disable the check. Applications that are\n+expected to receive unusually large form posts should tune this setting.\n+\n+The amount of request data is correlated to the amount of memory needed to\n+process the request and populate the GET and POST dictionaries. Large requests\n+could be used as a denial-of-service attack vector if left unchecked. Since web\n+servers don't typically perform deep request inspection, it's not possible to\n+perform a similar check at that level.\n+\n+See also :setting:`FILE_UPLOAD_MAX_MEMORY_SIZE`.\n+\n+.. setting:: DATA_UPLOAD_MAX_NUMBER_FIELDS\n+\n+``DATA_UPLOAD_MAX_NUMBER_FIELDS``\n+---------------------------------\n+\n+Default: ``1000``\n+\n+The maximum number of parameters that may be received via GET or POST before a\n+:exc:`~django.core.exceptions.SuspiciousOperation` (``TooManyFields``) is\n+raised. You can set this to ``None`` to disable the check. Applications that\n+are expected to receive an unusually large number of form fields should tune\n+this setting.\n+\n+The number of request parameters is correlated to the amount of time needed to\n+process the request and populate the GET and POST dictionaries. Large requests\n+could be used as a denial-of-service attack vector if left unchecked. Since web\n+servers don't typically perform deep request inspection, it's not possible to\n+perform a similar check at that level.\n+\n+.. setting:: DATA_UPLOAD_MAX_NUMBER_FILES\n+\n+``DATA_UPLOAD_MAX_NUMBER_FILES``\n+--------------------------------\n+\n+.. versionadded:: 3.2.18\n+\n+Default: ``100``\n+\n+The maximum number of files that may be received via POST in a\n+``multipart/form-data`` encoded request before a\n+:exc:`~django.core.exceptions.SuspiciousOperation` (``TooManyFiles``) is\n+raised. You can set this to ``None`` to disable the check. Applications that\n+are expected to receive an unusually large number of file fields should tune\n+this setting.\n+\n+The number of accepted files is correlated to the amount of time and memory\n+needed to process the request. Large requests could be used as a\n+denial-of-service attack vector if left unchecked. Since web servers don't\n+typically perform deep request inspection, it's not possible to perform a\n+similar check at that level.\n+\n+.. setting:: DATABASE_ROUTERS\n+\n+``DATABASE_ROUTERS``\n+--------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+The list of routers that will be used to determine which database\n+to use when performing a database query.\n+\n+See the documentation on :ref:`automatic database routing in multi\n+database configurations <topics-db-multi-db-routing>`.\n+\n+.. setting:: DATE_FORMAT\n+\n+``DATE_FORMAT``\n+---------------\n+\n+Default: ``'N j, Y'`` (e.g. ``Feb. 4, 2003``)\n+\n+The default formatting to use for displaying date fields in any part of the\n+system. Note that the locale-dictated format has higher precedence and will be\n+applied instead. See :tfilter:`allowed date format strings <date>`.\n+\n+See also :setting:`DATETIME_FORMAT`, :setting:`TIME_FORMAT` and :setting:`SHORT_DATE_FORMAT`.\n+\n+.. setting:: DATE_INPUT_FORMATS\n+\n+``DATE_INPUT_FORMATS``\n+----------------------\n+\n+Default::\n+\n+    [\n+        \"%Y-%m-%d\",  # '2006-10-25'\n+        \"%m/%d/%Y\",  # '10/25/2006'\n+        \"%m/%d/%y\",  # '10/25/06'\n+        \"%b %d %Y\",  # 'Oct 25 2006'\n+        \"%b %d, %Y\",  # 'Oct 25, 2006'\n+        \"%d %b %Y\",  # '25 Oct 2006'\n+        \"%d %b, %Y\",  # '25 Oct, 2006'\n+        \"%B %d %Y\",  # 'October 25 2006'\n+        \"%B %d, %Y\",  # 'October 25, 2006'\n+        \"%d %B %Y\",  # '25 October 2006'\n+        \"%d %B, %Y\",  # '25 October, 2006'\n+    ]\n+\n+A list of formats that will be accepted when inputting data on a date field.\n+Formats will be tried in order, using the first valid one. Note that these\n+format strings use Python's :ref:`datetime module syntax\n+<strftime-strptime-behavior>`, not the format strings from the :tfilter:`date`\n+template filter.\n+\n+The locale-dictated format has higher precedence and will be applied instead.\n+\n+See also :setting:`DATETIME_INPUT_FORMATS` and :setting:`TIME_INPUT_FORMATS`.\n+\n+.. setting:: DATETIME_FORMAT\n+\n+``DATETIME_FORMAT``\n+-------------------\n+\n+Default: ``'N j, Y, P'`` (e.g. ``Feb. 4, 2003, 4 p.m.``)\n+\n+The default formatting to use for displaying datetime fields in any part of the\n+system. Note that the locale-dictated format has higher precedence and will be\n+applied instead. See :tfilter:`allowed date format strings <date>`.\n+\n+See also :setting:`DATE_FORMAT`, :setting:`TIME_FORMAT` and :setting:`SHORT_DATETIME_FORMAT`.\n+\n+.. setting:: DATETIME_INPUT_FORMATS\n+\n+``DATETIME_INPUT_FORMATS``\n+--------------------------\n+\n+Default::\n+\n+    [\n+        \"%Y-%m-%d %H:%M:%S\",  # '2006-10-25 14:30:59'\n+        \"%Y-%m-%d %H:%M:%S.%f\",  # '2006-10-25 14:30:59.000200'\n+        \"%Y-%m-%d %H:%M\",  # '2006-10-25 14:30'\n+        \"%m/%d/%Y %H:%M:%S\",  # '10/25/2006 14:30:59'\n+        \"%m/%d/%Y %H:%M:%S.%f\",  # '10/25/2006 14:30:59.000200'\n+        \"%m/%d/%Y %H:%M\",  # '10/25/2006 14:30'\n+        \"%m/%d/%y %H:%M:%S\",  # '10/25/06 14:30:59'\n+        \"%m/%d/%y %H:%M:%S.%f\",  # '10/25/06 14:30:59.000200'\n+        \"%m/%d/%y %H:%M\",  # '10/25/06 14:30'\n+    ]\n+\n+A list of formats that will be accepted when inputting data on a datetime\n+field. Formats will be tried in order, using the first valid one. Note that\n+these format strings use Python's :ref:`datetime module syntax\n+<strftime-strptime-behavior>`, not the format strings from the :tfilter:`date`\n+template filter. Date-only formats are not included as datetime fields will\n+automatically try :setting:`DATE_INPUT_FORMATS` in last resort.\n+\n+The locale-dictated format has higher precedence and will be applied instead.\n+\n+See also :setting:`DATE_INPUT_FORMATS` and :setting:`TIME_INPUT_FORMATS`.\n+\n+.. setting:: DEBUG\n+\n+``DEBUG``\n+---------\n+\n+Default: ``False``\n+\n+A boolean that turns on/off debug mode.\n+\n+Never deploy a site into production with :setting:`DEBUG` turned on.\n+\n+One of the main features of debug mode is the display of detailed error pages.\n+If your app raises an exception when :setting:`DEBUG` is ``True``, Django will\n+display a detailed traceback, including a lot of metadata about your\n+environment, such as all the currently defined Django settings (from\n+``settings.py``).\n+\n+As a security measure, Django will *not* include settings that might be\n+sensitive, such as :setting:`SECRET_KEY`. Specifically, it will exclude any\n+setting whose name includes any of the following:\n+\n+* ``'API'``\n+* ``'KEY'``\n+* ``'PASS'``\n+* ``'SECRET'``\n+* ``'SIGNATURE'``\n+* ``'TOKEN'``\n+\n+Note that these are *partial* matches. ``'PASS'`` will also match PASSWORD,\n+just as ``'TOKEN'`` will also match TOKENIZED and so on.\n+\n+Still, note that there are always going to be sections of your debug output\n+that are inappropriate for public consumption. File paths, configuration\n+options and the like all give attackers extra information about your server.\n+\n+It is also important to remember that when running with :setting:`DEBUG`\n+turned on, Django will remember every SQL query it executes. This is useful\n+when you're debugging, but it'll rapidly consume memory on a production server.\n+\n+Finally, if :setting:`DEBUG` is ``False``, you also need to properly set\n+the :setting:`ALLOWED_HOSTS` setting. Failing to do so will result in all\n+requests being returned as \"Bad Request (400)\".\n+\n+.. note::\n+\n+    The default :file:`settings.py` file created by :djadmin:`django-admin\n+    startproject <startproject>` sets ``DEBUG = True`` for convenience.\n+\n+.. setting:: DEBUG_PROPAGATE_EXCEPTIONS\n+\n+``DEBUG_PROPAGATE_EXCEPTIONS``\n+------------------------------\n+\n+Default: ``False``\n+\n+If set to ``True``, Django's exception handling of view functions\n+(:data:`~django.conf.urls.handler500`, or the debug view if :setting:`DEBUG`\n+is ``True``) and logging of 500 responses (:ref:`django-request-logger`) is\n+skipped and exceptions propagate upward.\n+\n+This can be useful for some test setups. It shouldn't be used on a live site\n+unless you want your web server (instead of Django) to generate \"Internal\n+Server Error\" responses. In that case, make sure your server doesn't show the\n+stack trace or other sensitive information in the response.\n+\n+.. setting:: DECIMAL_SEPARATOR\n+\n+``DECIMAL_SEPARATOR``\n+---------------------\n+\n+Default: ``'.'`` (Dot)\n+\n+Default decimal separator used when formatting decimal numbers.\n+\n+Note that the locale-dictated format has higher precedence and will be applied\n+instead.\n+\n+See also :setting:`NUMBER_GROUPING`, :setting:`THOUSAND_SEPARATOR` and\n+:setting:`USE_THOUSAND_SEPARATOR`.\n+\n+.. setting:: DEFAULT_AUTO_FIELD\n+\n+``DEFAULT_AUTO_FIELD``\n+----------------------\n+\n+Default: ``'``:class:`django.db.models.AutoField`\\ ``'``\n+\n+Default primary key field type to use for models that don't have a field with\n+:attr:`primary_key=True <django.db.models.Field.primary_key>`.\n+\n+.. admonition:: Migrating auto-created through tables\n+\n+    The value of ``DEFAULT_AUTO_FIELD`` will be respected when creating new\n+    auto-created through tables for many-to-many relationships.\n+\n+    Unfortunately, the primary keys of existing auto-created through tables\n+    cannot currently be updated by the migrations framework.\n+\n+    This means that if you switch the value of ``DEFAULT_AUTO_FIELD`` and then\n+    generate migrations, the primary keys of the related models will be\n+    updated, as will the foreign keys from the through table, but the primary\n+    key of the auto-created through table will not be migrated.\n+\n+    In order to address this, you should add a\n+    :class:`~django.db.migrations.operations.RunSQL` operation to your\n+    migrations to perform the required ``ALTER TABLE`` step. You can check the\n+    existing table name through ``sqlmigrate``, ``dbshell``, or with the\n+    field\u2019s ``remote_field.through._meta.db_table`` property.\n+\n+    Explicitly defined through models are already handled by the migrations\n+    system.\n+\n+    Allowing automatic migrations for the primary key of existing auto-created\n+    through tables :ticket:`may be implemented at a later date <32674>`.\n+\n+.. setting:: DEFAULT_CHARSET\n+\n+``DEFAULT_CHARSET``\n+-------------------\n+\n+Default: ``'utf-8'``\n+\n+Default charset to use for all ``HttpResponse`` objects, if a MIME type isn't\n+manually specified. Used when constructing the ``Content-Type`` header.\n+\n+.. setting:: DEFAULT_EXCEPTION_REPORTER\n+\n+``DEFAULT_EXCEPTION_REPORTER``\n+------------------------------\n+\n+Default: ``'``:class:`django.views.debug.ExceptionReporter`\\ ``'``\n+\n+Default exception reporter class to be used if none has been assigned to the\n+:class:`~django.http.HttpRequest` instance yet. See\n+:ref:`custom-error-reports`.\n+\n+.. setting:: DEFAULT_EXCEPTION_REPORTER_FILTER\n+\n+``DEFAULT_EXCEPTION_REPORTER_FILTER``\n+-------------------------------------\n+\n+Default: ``'``:class:`django.views.debug.SafeExceptionReporterFilter`\\ ``'``\n+\n+Default exception reporter filter class to be used if none has been assigned to\n+the :class:`~django.http.HttpRequest` instance yet.\n+See :ref:`Filtering error reports<filtering-error-reports>`.\n+\n+.. setting:: DEFAULT_FILE_STORAGE\n+\n+``DEFAULT_FILE_STORAGE``\n+------------------------\n+\n+Default: ``'``:class:`django.core.files.storage.FileSystemStorage`\\ ``'``\n+\n+Default file storage class to be used for any file-related operations that don't\n+specify a particular storage system. See :doc:`/topics/files`.\n+\n+.. deprecated:: 4.2\n+\n+    This setting is deprecated. Starting with Django 4.2, default file storage\n+    engine can be configured with the :setting:`STORAGES` setting under the\n+    ``default`` key.\n+\n+.. setting:: DEFAULT_FROM_EMAIL\n+\n+``DEFAULT_FROM_EMAIL``\n+----------------------\n+\n+Default: ``'webmaster@localhost'``\n+\n+Default email address to use for various automated correspondence from the\n+site manager(s). This doesn't include error messages sent to :setting:`ADMINS`\n+and :setting:`MANAGERS`; for that, see :setting:`SERVER_EMAIL`.\n+\n+.. setting:: DEFAULT_INDEX_TABLESPACE\n+\n+``DEFAULT_INDEX_TABLESPACE``\n+----------------------------\n+\n+Default: ``''`` (Empty string)\n+\n+Default tablespace to use for indexes on fields that don't specify\n+one, if the backend supports it (see :doc:`/topics/db/tablespaces`).\n+\n+.. setting:: DEFAULT_TABLESPACE\n+\n+``DEFAULT_TABLESPACE``\n+----------------------\n+\n+Default: ``''`` (Empty string)\n+\n+Default tablespace to use for models that don't specify one, if the\n+backend supports it (see :doc:`/topics/db/tablespaces`).\n+\n+.. setting:: DISALLOWED_USER_AGENTS\n+\n+``DISALLOWED_USER_AGENTS``\n+--------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+List of compiled regular expression objects representing User-Agent strings\n+that are not allowed to visit any page, systemwide. Use this for bots/crawlers.\n+This is only used if ``CommonMiddleware`` is installed (see\n+:doc:`/topics/http/middleware`).\n+\n+.. setting:: EMAIL_BACKEND\n+\n+``EMAIL_BACKEND``\n+-----------------\n+\n+Default: ``'``:class:`django.core.mail.backends.smtp.EmailBackend`\\ ``'``\n+\n+The backend to use for sending emails. For the list of available backends see\n+:doc:`/topics/email`.\n+\n+.. setting:: EMAIL_FILE_PATH\n+\n+``EMAIL_FILE_PATH``\n+-------------------\n+\n+Default: Not defined\n+\n+The directory used by the :ref:`file email backend <topic-email-file-backend>`\n+to store output files.\n+\n+.. setting:: EMAIL_HOST\n+\n+``EMAIL_HOST``\n+--------------\n+\n+Default: ``'localhost'``\n+\n+The host to use for sending email.\n+\n+See also :setting:`EMAIL_PORT`.\n+\n+.. setting:: EMAIL_HOST_PASSWORD\n+\n+``EMAIL_HOST_PASSWORD``\n+-----------------------\n+\n+Default: ``''`` (Empty string)\n+\n+Password to use for the SMTP server defined in :setting:`EMAIL_HOST`. This\n+setting is used in conjunction with :setting:`EMAIL_HOST_USER` when\n+authenticating to the SMTP server. If either of these settings is empty,\n+Django won't attempt authentication.\n+\n+See also :setting:`EMAIL_HOST_USER`.\n+\n+.. setting:: EMAIL_HOST_USER\n+\n+``EMAIL_HOST_USER``\n+-------------------\n+\n+Default: ``''`` (Empty string)\n+\n+Username to use for the SMTP server defined in :setting:`EMAIL_HOST`.\n+If empty, Django won't attempt authentication.\n+\n+See also :setting:`EMAIL_HOST_PASSWORD`.\n+\n+.. setting:: EMAIL_PORT\n+\n+``EMAIL_PORT``\n+--------------\n+\n+Default: ``25``\n+\n+Port to use for the SMTP server defined in :setting:`EMAIL_HOST`.\n+\n+.. setting:: EMAIL_SUBJECT_PREFIX\n+\n+``EMAIL_SUBJECT_PREFIX``\n+------------------------\n+\n+Default: ``'[Django] '``\n+\n+Subject-line prefix for email messages sent with ``django.core.mail.mail_admins``\n+or ``django.core.mail.mail_managers``. You'll probably want to include the\n+trailing space.\n+\n+.. setting:: EMAIL_USE_LOCALTIME\n+\n+``EMAIL_USE_LOCALTIME``\n+-----------------------\n+\n+Default: ``False``\n+\n+Whether to send the SMTP ``Date`` header of email messages in the local time\n+zone (``True``) or in UTC (``False``).\n+\n+.. setting:: EMAIL_USE_TLS\n+\n+``EMAIL_USE_TLS``\n+-----------------\n+\n+Default: ``False``\n+\n+Whether to use a TLS (secure) connection when talking to the SMTP server.\n+This is used for explicit TLS connections, generally on port 587. If you are\n+experiencing hanging connections, see the implicit TLS setting\n+:setting:`EMAIL_USE_SSL`.\n+\n+.. setting:: EMAIL_USE_SSL\n+\n+``EMAIL_USE_SSL``\n+-----------------\n+\n+Default: ``False``\n+\n+Whether to use an implicit TLS (secure) connection when talking to the SMTP\n+server. In most email documentation this type of TLS connection is referred\n+to as SSL. It is generally used on port 465. If you are experiencing problems,\n+see the explicit TLS setting :setting:`EMAIL_USE_TLS`.\n+\n+Note that :setting:`EMAIL_USE_TLS`/:setting:`EMAIL_USE_SSL` are mutually\n+exclusive, so only set one of those settings to ``True``.\n+\n+.. setting:: EMAIL_SSL_CERTFILE\n+\n+``EMAIL_SSL_CERTFILE``\n+----------------------\n+\n+Default: ``None``\n+\n+If :setting:`EMAIL_USE_SSL` or :setting:`EMAIL_USE_TLS` is ``True``, you can\n+optionally specify the path to a PEM-formatted certificate chain file to use\n+for the SSL connection.\n+\n+.. setting:: EMAIL_SSL_KEYFILE\n+\n+``EMAIL_SSL_KEYFILE``\n+---------------------\n+\n+Default: ``None``\n+\n+If :setting:`EMAIL_USE_SSL` or :setting:`EMAIL_USE_TLS` is ``True``, you can\n+optionally specify the path to a PEM-formatted private key file to use for the\n+SSL connection.\n+\n+Note that setting :setting:`EMAIL_SSL_CERTFILE` and :setting:`EMAIL_SSL_KEYFILE`\n+doesn't result in any certificate checking. They're passed to the underlying SSL\n+connection. Please refer to the documentation of Python's\n+:func:`python:ssl.wrap_socket` function for details on how the certificate chain\n+file and private key file are handled.\n+\n+.. setting:: EMAIL_TIMEOUT\n+\n+``EMAIL_TIMEOUT``\n+-----------------\n+\n+Default: ``None``\n+\n+Specifies a timeout in seconds for blocking operations like the connection\n+attempt.\n+\n+.. setting:: FILE_UPLOAD_HANDLERS\n+\n+``FILE_UPLOAD_HANDLERS``\n+------------------------\n+\n+Default::\n+\n+    [\n+        \"django.core.files.uploadhandler.MemoryFileUploadHandler\",\n+        \"django.core.files.uploadhandler.TemporaryFileUploadHandler\",\n+    ]\n+\n+A list of handlers to use for uploading. Changing this setting allows complete\n+customization -- even replacement -- of Django's upload process.\n+\n+See :doc:`/topics/files` for details.\n+\n+.. setting:: FILE_UPLOAD_MAX_MEMORY_SIZE\n+\n+``FILE_UPLOAD_MAX_MEMORY_SIZE``\n+-------------------------------\n+\n+Default: ``2621440`` (i.e. 2.5 MB).\n+\n+The maximum size (in bytes) that an upload will be before it gets streamed to\n+the file system. See :doc:`/topics/files` for details.\n+\n+See also :setting:`DATA_UPLOAD_MAX_MEMORY_SIZE`.\n+\n+.. setting:: FILE_UPLOAD_DIRECTORY_PERMISSIONS\n+\n+``FILE_UPLOAD_DIRECTORY_PERMISSIONS``\n+-------------------------------------\n+\n+Default: ``None``\n+\n+The numeric mode to apply to directories created in the process of uploading\n+files.\n+\n+This setting also determines the default permissions for collected static\n+directories when using the :djadmin:`collectstatic` management command. See\n+:djadmin:`collectstatic` for details on overriding it.\n+\n+This value mirrors the functionality and caveats of the\n+:setting:`FILE_UPLOAD_PERMISSIONS` setting.\n+\n+.. setting:: FILE_UPLOAD_PERMISSIONS\n+\n+``FILE_UPLOAD_PERMISSIONS``\n+---------------------------\n+\n+Default: ``0o644``\n+\n+The numeric mode (i.e. ``0o644``) to set newly uploaded files to. For\n+more information about what these modes mean, see the documentation for\n+:func:`os.chmod`.\n+\n+If ``None``, you'll get operating-system dependent behavior. On most platforms,\n+temporary files will have a mode of ``0o600``, and files saved from memory will\n+be saved using the system's standard umask.\n+\n+For security reasons, these permissions aren't applied to the temporary files\n+that are stored in :setting:`FILE_UPLOAD_TEMP_DIR`.\n+\n+This setting also determines the default permissions for collected static files\n+when using the :djadmin:`collectstatic` management command. See\n+:djadmin:`collectstatic` for details on overriding it.\n+\n+.. warning::\n+\n+    **Always prefix the mode with** ``0o`` **.**\n+\n+    If you're not familiar with file modes, please note that the ``0o`` prefix\n+    is very important: it indicates an octal number, which is the way that\n+    modes must be specified. If you try to use ``644``, you'll get totally\n+    incorrect behavior.\n+\n+.. setting:: FILE_UPLOAD_TEMP_DIR\n+\n+``FILE_UPLOAD_TEMP_DIR``\n+------------------------\n+\n+Default: ``None``\n+\n+The directory to store data to (typically files larger than\n+:setting:`FILE_UPLOAD_MAX_MEMORY_SIZE`) temporarily while uploading files.\n+If ``None``, Django will use the standard temporary directory for the operating\n+system. For example, this will default to ``/tmp`` on \\*nix-style operating\n+systems.\n+\n+See :doc:`/topics/files` for details.\n+\n+.. setting:: FIRST_DAY_OF_WEEK\n+\n+``FIRST_DAY_OF_WEEK``\n+---------------------\n+\n+Default: ``0`` (Sunday)\n+\n+A number representing the first day of the week. This is especially useful\n+when displaying a calendar. This value is only used when not using\n+format internationalization, or when a format cannot be found for the\n+current locale.\n+\n+The value must be an integer from 0 to 6, where 0 means Sunday, 1 means\n+Monday and so on.\n+\n+.. setting:: FIXTURE_DIRS\n+\n+``FIXTURE_DIRS``\n+----------------\n+\n+Default: ``[]`` (Empty list)\n+\n+List of directories searched for :ref:`fixture <fixtures-explanation>` files,\n+in addition to the ``fixtures`` directory of each application, in search order.\n+\n+Note that these paths should use Unix-style forward slashes, even on Windows.\n+\n+See :ref:`initial-data-via-fixtures` and :ref:`topics-testing-fixtures`.\n+\n+.. setting:: FORCE_SCRIPT_NAME\n+\n+``FORCE_SCRIPT_NAME``\n+---------------------\n+\n+Default: ``None``\n+\n+If not ``None``, this will be used as the value of the ``SCRIPT_NAME``\n+environment variable in any HTTP request. This setting can be used to override\n+the server-provided value of ``SCRIPT_NAME``, which may be a rewritten version\n+of the preferred value or not supplied at all. It is also used by\n+:func:`django.setup()` to set the URL resolver script prefix outside of the\n+request/response cycle (e.g. in management commands and standalone scripts) to\n+generate correct URLs when ``SCRIPT_NAME`` is not ``/``.\n+\n+.. setting:: FORM_RENDERER\n+\n+``FORM_RENDERER``\n+-----------------\n+\n+Default: ``'``:class:`django.forms.renderers.DjangoTemplates`\\ ``'``\n+\n+The class that renders forms and form widgets. It must implement\n+:ref:`the low-level render API <low-level-widget-render-api>`. Included form\n+renderers are:\n+\n+* ``'``:class:`django.forms.renderers.DjangoTemplates`\\ ``'``\n+* ``'``:class:`django.forms.renderers.Jinja2`\\ ``'``\n+* ``'``:class:`django.forms.renderers.TemplatesSetting`\\ ``'``\n+\n+.. setting:: FORMAT_MODULE_PATH\n+\n+``FORMAT_MODULE_PATH``\n+----------------------\n+\n+Default: ``None``\n+\n+A full Python path to a Python package that contains custom format definitions\n+for project locales. If not ``None``, Django will check for a ``formats.py``\n+file, under the directory named as the current locale, and will use the\n+formats defined in this file.\n+\n+For example, if :setting:`FORMAT_MODULE_PATH` is set to ``mysite.formats``,\n+and current language is ``en`` (English), Django will expect a directory tree\n+like:\n+\n+.. code-block:: text\n+\n+    mysite/\n+        formats/\n+            __init__.py\n+            en/\n+                __init__.py\n+                formats.py\n+\n+You can also set this setting to a list of Python paths, for example::\n+\n+    FORMAT_MODULE_PATH = [\n+        \"mysite.formats\",\n+        \"some_app.formats\",\n+    ]\n+\n+When Django searches for a certain format, it will go through all given Python\n+paths until it finds a module that actually defines the given format. This\n+means that formats defined in packages farther up in the list will take\n+precedence over the same formats in packages farther down.\n+\n+Available formats are:\n+\n+* :setting:`DATE_FORMAT`\n+* :setting:`DATE_INPUT_FORMATS`\n+* :setting:`DATETIME_FORMAT`,\n+* :setting:`DATETIME_INPUT_FORMATS`\n+* :setting:`DECIMAL_SEPARATOR`\n+* :setting:`FIRST_DAY_OF_WEEK`\n+* :setting:`MONTH_DAY_FORMAT`\n+* :setting:`NUMBER_GROUPING`\n+* :setting:`SHORT_DATE_FORMAT`\n+* :setting:`SHORT_DATETIME_FORMAT`\n+* :setting:`THOUSAND_SEPARATOR`\n+* :setting:`TIME_FORMAT`\n+* :setting:`TIME_INPUT_FORMATS`\n+* :setting:`YEAR_MONTH_FORMAT`\n+\n+.. setting:: IGNORABLE_404_URLS\n+\n+``IGNORABLE_404_URLS``\n+----------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+List of compiled regular expression objects describing URLs that should be\n+ignored when reporting HTTP 404 errors via email (see\n+:doc:`/howto/error-reporting`). Regular expressions are matched against\n+:meth:`request's full paths <django.http.HttpRequest.get_full_path>` (including\n+query string, if any). Use this if your site does not provide a commonly\n+requested file such as ``favicon.ico`` or ``robots.txt``.\n+\n+This is only used if\n+:class:`~django.middleware.common.BrokenLinkEmailsMiddleware` is enabled (see\n+:doc:`/topics/http/middleware`).\n+\n+.. setting:: INSTALLED_APPS\n+\n+``INSTALLED_APPS``\n+------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of strings designating all applications that are enabled in this\n+Django installation. Each string should be a dotted Python path to:\n+\n+* an application configuration class (preferred), or\n+* a package containing an application.\n+\n+:doc:`Learn more about application configurations </ref/applications>`.\n+\n+.. admonition:: Use the application registry for introspection\n+\n+    Your code should never access :setting:`INSTALLED_APPS` directly. Use\n+    :attr:`django.apps.apps` instead.\n+\n+.. admonition:: Application names and labels must be unique in\n+                :setting:`INSTALLED_APPS`\n+\n+    Application :attr:`names <django.apps.AppConfig.name>` \u2014 the dotted Python\n+    path to the application package \u2014 must be unique. There is no way to\n+    include the same application twice, short of duplicating its code under\n+    another name.\n+\n+    Application :attr:`labels <django.apps.AppConfig.label>` \u2014 by default the\n+    final part of the name \u2014 must be unique too. For example, you can't\n+    include both ``django.contrib.auth`` and ``myproject.auth``. However, you\n+    can relabel an application with a custom configuration that defines a\n+    different :attr:`~django.apps.AppConfig.label`.\n+\n+    These rules apply regardless of whether :setting:`INSTALLED_APPS`\n+    references application configuration classes or application packages.\n+\n+When several applications provide different versions of the same resource\n+(template, static file, management command, translation), the application\n+listed first in :setting:`INSTALLED_APPS` has precedence.\n+\n+.. setting:: INTERNAL_IPS\n+\n+``INTERNAL_IPS``\n+----------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of IP addresses, as strings, that:\n+\n+* Allow the :func:`~django.template.context_processors.debug` context processor\n+  to add some variables to the template context.\n+* Can use the :ref:`admindocs bookmarklets <admindocs-bookmarklets>` even if\n+  not logged in as a staff user.\n+* Are marked as \"internal\" (as opposed to \"EXTERNAL\") in\n+  :class:`~django.utils.log.AdminEmailHandler` emails.\n+\n+.. setting:: LANGUAGE_CODE\n+\n+``LANGUAGE_CODE``\n+-----------------\n+\n+Default: ``'en-us'``\n+\n+A string representing the language code for this installation. This should be in\n+standard :term:`language ID format <language code>`. For example, U.S. English\n+is ``\"en-us\"``. See also the `list of language identifiers`_ and\n+:doc:`/topics/i18n/index`.\n+\n+:setting:`USE_I18N` must be active for this setting to have any effect.\n+\n+It serves two purposes:\n+\n+* If the locale middleware isn't in use, it decides which translation is served\n+  to all users.\n+* If the locale middleware is active, it provides a fallback language in case the\n+  user's preferred language can't be determined or is not supported by the\n+  website. It also provides the fallback translation when a translation for a\n+  given literal doesn't exist for the user's preferred language.\n+\n+See :ref:`how-django-discovers-language-preference` for more details.\n+\n+.. _list of language identifiers: http://www.i18nguy.com/unicode/language-identifiers.html\n+\n+.. setting:: LANGUAGE_COOKIE_AGE\n+\n+``LANGUAGE_COOKIE_AGE``\n+-----------------------\n+\n+Default: ``None`` (expires at browser close)\n+\n+The age of the language cookie, in seconds.\n+\n+.. setting:: LANGUAGE_COOKIE_DOMAIN\n+\n+``LANGUAGE_COOKIE_DOMAIN``\n+--------------------------\n+\n+Default: ``None``\n+\n+The domain to use for the language cookie. Set this to a string such as\n+``\"example.com\"`` for cross-domain cookies, or use ``None`` for a standard\n+domain cookie.\n+\n+Be cautious when updating this setting on a production site. If you update\n+this setting to enable cross-domain cookies on a site that previously used\n+standard domain cookies, existing user cookies that have the old domain\n+will not be updated. This will result in site users being unable to switch\n+the language as long as these cookies persist. The only safe and reliable\n+option to perform the switch is to change the language cookie name\n+permanently (via the :setting:`LANGUAGE_COOKIE_NAME` setting) and to add\n+a middleware that copies the value from the old cookie to a new one and then\n+deletes the old one.\n+\n+.. setting:: LANGUAGE_COOKIE_HTTPONLY\n+\n+``LANGUAGE_COOKIE_HTTPONLY``\n+----------------------------\n+\n+Default: ``False``\n+\n+Whether to use ``HttpOnly`` flag on the language cookie. If this is set to\n+``True``, client-side JavaScript will not be able to access the language\n+cookie.\n+\n+See :setting:`SESSION_COOKIE_HTTPONLY` for details on ``HttpOnly``.\n+\n+.. setting:: LANGUAGE_COOKIE_NAME\n+\n+``LANGUAGE_COOKIE_NAME``\n+------------------------\n+\n+Default: ``'django_language'``\n+\n+The name of the cookie to use for the language cookie. This can be whatever\n+you want (as long as it's different from the other cookie names in your\n+application). See :doc:`/topics/i18n/index`.\n+\n+.. setting:: LANGUAGE_COOKIE_PATH\n+\n+``LANGUAGE_COOKIE_PATH``\n+------------------------\n+\n+Default: ``'/'``\n+\n+The path set on the language cookie. This should either match the URL path of your\n+Django installation or be a parent of that path.\n+\n+This is useful if you have multiple Django instances running under the same\n+hostname. They can use different cookie paths and each instance will only see\n+its own language cookie.\n+\n+Be cautious when updating this setting on a production site. If you update this\n+setting to use a deeper path than it previously used, existing user cookies that\n+have the old path will not be updated. This will result in site users being\n+unable to switch the language as long as these cookies persist. The only safe\n+and reliable option to perform the switch is to change the language cookie name\n+permanently (via the :setting:`LANGUAGE_COOKIE_NAME` setting), and to add\n+a middleware that copies the value from the old cookie to a new one and then\n+deletes the one.\n+\n+.. setting:: LANGUAGE_COOKIE_SAMESITE\n+\n+``LANGUAGE_COOKIE_SAMESITE``\n+----------------------------\n+\n+Default: ``None``\n+\n+The value of the `SameSite`_ flag on the language cookie. This flag prevents the\n+cookie from being sent in cross-site requests.\n+\n+See :setting:`SESSION_COOKIE_SAMESITE` for details about ``SameSite``.\n+\n+.. setting:: LANGUAGE_COOKIE_SECURE\n+\n+``LANGUAGE_COOKIE_SECURE``\n+--------------------------\n+\n+Default: ``False``\n+\n+Whether to use a secure cookie for the language cookie. If this is set to\n+``True``, the cookie will be marked as \"secure\", which means browsers may\n+ensure that the cookie is only sent under an HTTPS connection.\n+\n+.. setting:: LANGUAGES\n+\n+``LANGUAGES``\n+-------------\n+\n+Default: A list of all available languages. This list is continually growing\n+and including a copy here would inevitably become rapidly out of date. You can\n+see the current list of translated languages by looking in\n+:source:`django/conf/global_settings.py`.\n+\n+The list is a list of two-tuples in the format\n+(:term:`language code<language code>`, ``language name``) -- for example,\n+``('ja', 'Japanese')``.\n+This specifies which languages are available for language selection. See\n+:doc:`/topics/i18n/index`.\n+\n+Generally, the default value should suffice. Only set this setting if you want\n+to restrict language selection to a subset of the Django-provided languages.\n+\n+If you define a custom :setting:`LANGUAGES` setting, you can mark the\n+language names as translation strings using the\n+:func:`~django.utils.translation.gettext_lazy` function.\n+\n+Here's a sample settings file::\n+\n+    from django.utils.translation import gettext_lazy as _\n+\n+    LANGUAGES = [\n+        (\"de\", _(\"German\")),\n+        (\"en\", _(\"English\")),\n+    ]\n+\n+.. setting:: LANGUAGES_BIDI\n+\n+``LANGUAGES_BIDI``\n+------------------\n+\n+Default: A list of all language codes that are written right-to-left. You can\n+see the current list of these languages by looking in\n+:source:`django/conf/global_settings.py`.\n+\n+The list contains :term:`language codes<language code>` for languages that are\n+written right-to-left.\n+\n+Generally, the default value should suffice. Only set this setting if you want\n+to restrict language selection to a subset of the Django-provided languages.\n+If you define a custom :setting:`LANGUAGES` setting, the list of bidirectional\n+languages may contain language codes which are not enabled on a given site.\n+\n+.. setting:: LOCALE_PATHS\n+\n+``LOCALE_PATHS``\n+----------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of directories where Django looks for translation files.\n+See :ref:`how-django-discovers-translations`.\n+\n+Example::\n+\n+    LOCALE_PATHS = [\n+        \"/home/www/project/common_files/locale\",\n+        \"/var/local/translations/locale\",\n+    ]\n+\n+Django will look within each of these paths for the ``<locale_code>/LC_MESSAGES``\n+directories containing the actual translation files.\n+\n+.. setting:: LOGGING\n+\n+``LOGGING``\n+-----------\n+\n+Default: A logging configuration dictionary.\n+\n+A data structure containing configuration information. When not-empty, the\n+contents of this data structure will be passed as the argument to the\n+configuration method described in :setting:`LOGGING_CONFIG`.\n+\n+Among other things, the default logging configuration passes HTTP 500 server\n+errors to an email log handler when :setting:`DEBUG` is ``False``. See also\n+:ref:`configuring-logging`.\n+\n+You can see the default logging configuration by looking in\n+:source:`django/utils/log.py`.\n+\n+.. setting:: LOGGING_CONFIG\n+\n+``LOGGING_CONFIG``\n+------------------\n+\n+Default: ``'logging.config.dictConfig'``\n+\n+A path to a callable that will be used to configure logging in the\n+Django project. Points at an instance of Python's :ref:`dictConfig\n+<logging-config-dictschema>` configuration method by default.\n+\n+If you set :setting:`LOGGING_CONFIG` to ``None``, the logging\n+configuration process will be skipped.\n+\n+.. setting:: MANAGERS\n+\n+``MANAGERS``\n+------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list in the same format as :setting:`ADMINS` that specifies who should get\n+broken link notifications when\n+:class:`~django.middleware.common.BrokenLinkEmailsMiddleware` is enabled.\n+\n+.. setting:: MEDIA_ROOT\n+\n+``MEDIA_ROOT``\n+--------------\n+\n+Default: ``''`` (Empty string)\n+\n+Absolute filesystem path to the directory that will hold :doc:`user-uploaded\n+files </topics/files>`.\n+\n+Example: ``\"/var/www/example.com/media/\"``\n+\n+See also :setting:`MEDIA_URL`.\n+\n+.. warning::\n+\n+    :setting:`MEDIA_ROOT` and :setting:`STATIC_ROOT` must have different\n+    values. Before :setting:`STATIC_ROOT` was introduced, it was common to\n+    rely or fallback on :setting:`MEDIA_ROOT` to also serve static files;\n+    however, since this can have serious security implications, there is a\n+    validation check to prevent it.\n+\n+.. setting:: MEDIA_URL\n+\n+``MEDIA_URL``\n+-------------\n+\n+Default: ``''`` (Empty string)\n+\n+URL that handles the media served from :setting:`MEDIA_ROOT`, used\n+for :doc:`managing stored files </topics/files>`. It must end in a slash if set\n+to a non-empty value. You will need to :ref:`configure these files to be served\n+<serving-uploaded-files-in-development>` in both development and production\n+environments.\n+\n+If you want to use ``{{ MEDIA_URL }}`` in your templates, add\n+``'django.template.context_processors.media'`` in the ``'context_processors'``\n+option of :setting:`TEMPLATES`.\n+\n+Example: ``\"http://media.example.com/\"``\n+\n+.. warning::\n+\n+    There are security risks if you are accepting uploaded content from\n+    untrusted users! See the security guide's topic on\n+    :ref:`user-uploaded-content-security` for mitigation details.\n+\n+.. warning::\n+\n+    :setting:`MEDIA_URL` and :setting:`STATIC_URL` must have different\n+    values. See :setting:`MEDIA_ROOT` for more details.\n+\n+.. note::\n+\n+    If :setting:`MEDIA_URL` is a relative path, then it will be prefixed by the\n+    server-provided value of ``SCRIPT_NAME`` (or ``/`` if not set). This makes\n+    it easier to serve a Django application in a subpath without adding an\n+    extra configuration to the settings.\n+\n+.. setting:: MIDDLEWARE\n+\n+``MIDDLEWARE``\n+--------------\n+\n+Default: ``None``\n+\n+A list of middleware to use. See :doc:`/topics/http/middleware`.\n+\n+.. setting:: MIGRATION_MODULES\n+\n+``MIGRATION_MODULES``\n+---------------------\n+\n+Default: ``{}`` (Empty dictionary)\n+\n+A dictionary specifying the package where migration modules can be found on a\n+per-app basis. The default value of this setting is an empty dictionary, but\n+the default package name for migration modules is ``migrations``.\n+\n+Example::\n+\n+    {\"blog\": \"blog.db_migrations\"}\n+\n+In this case, migrations pertaining to the ``blog`` app will be contained in\n+the ``blog.db_migrations`` package.\n+\n+If you provide the ``app_label`` argument, :djadmin:`makemigrations` will\n+automatically create the package if it doesn't already exist.\n+\n+When you supply ``None`` as a value for an app, Django will consider the app as\n+an app without migrations regardless of an existing ``migrations`` submodule.\n+This can be used, for example, in a test settings file to skip migrations while\n+testing (tables will still be created for the apps' models). To disable\n+migrations for all apps during tests, you can set the\n+:setting:`MIGRATE <TEST_MIGRATE>` to ``False`` instead. If\n+``MIGRATION_MODULES`` is used in your general project settings, remember to use\n+the :option:`migrate --run-syncdb` option if you want to create tables for the\n+app.\n+\n+.. setting:: MONTH_DAY_FORMAT\n+\n+``MONTH_DAY_FORMAT``\n+--------------------\n+\n+Default: ``'F j'``\n+\n+The default formatting to use for date fields on Django admin change-list\n+pages -- and, possibly, by other parts of the system -- in cases when only the\n+month and day are displayed.\n+\n+For example, when a Django admin change-list page is being filtered by a date\n+drilldown, the header for a given day displays the day and month. Different\n+locales have different formats. For example, U.S. English would say\n+\"January 1,\" whereas Spanish might say \"1 Enero.\"\n+\n+Note that the corresponding locale-dictated format has higher precedence and\n+will be applied instead.\n+\n+See :tfilter:`allowed date format strings <date>`. See also\n+:setting:`DATE_FORMAT`, :setting:`DATETIME_FORMAT`,\n+:setting:`TIME_FORMAT` and :setting:`YEAR_MONTH_FORMAT`.\n+\n+.. setting:: NUMBER_GROUPING\n+\n+``NUMBER_GROUPING``\n+-------------------\n+\n+Default: ``0``\n+\n+Number of digits grouped together on the integer part of a number.\n+\n+Common use is to display a thousand separator. If this setting is ``0``, then\n+no grouping will be applied to the number. If this setting is greater than\n+``0``, then :setting:`THOUSAND_SEPARATOR` will be used as the separator between\n+those groups.\n+\n+Some locales use non-uniform digit grouping, e.g. ``10,00,00,000`` in\n+``en_IN``. For this case, you can provide a sequence with the number of digit\n+group sizes to be applied. The first number defines the size of the group\n+preceding the decimal delimiter, and each number that follows defines the size\n+of preceding groups. If the sequence is terminated with ``-1``, no further\n+grouping is performed. If the sequence terminates with a ``0``, the last group\n+size is used for the remainder of the number.\n+\n+Example tuple for ``en_IN``::\n+\n+    NUMBER_GROUPING = (3, 2, 0)\n+\n+Note that the locale-dictated format has higher precedence and will be applied\n+instead.\n+\n+See also :setting:`DECIMAL_SEPARATOR`, :setting:`THOUSAND_SEPARATOR` and\n+:setting:`USE_THOUSAND_SEPARATOR`.\n+\n+.. setting:: PREPEND_WWW\n+\n+``PREPEND_WWW``\n+---------------\n+\n+Default: ``False``\n+\n+Whether to prepend the \"www.\" subdomain to URLs that don't have it. This is only\n+used if :class:`~django.middleware.common.CommonMiddleware` is installed\n+(see :doc:`/topics/http/middleware`). See also :setting:`APPEND_SLASH`.\n+\n+.. setting:: ROOT_URLCONF\n+\n+``ROOT_URLCONF``\n+----------------\n+\n+Default: Not defined\n+\n+A string representing the full Python import path to your root URLconf, for\n+example ``\"mydjangoapps.urls\"``. Can be overridden on a per-request basis by\n+setting the attribute ``urlconf`` on the incoming ``HttpRequest``\n+object. See :ref:`how-django-processes-a-request` for details.\n+\n+.. setting:: SECRET_KEY\n+\n+``SECRET_KEY``\n+--------------\n+\n+Default: ``''`` (Empty string)\n+\n+A secret key for a particular Django installation. This is used to provide\n+:doc:`cryptographic signing </topics/signing>`, and should be set to a unique,\n+unpredictable value.\n+\n+:djadmin:`django-admin startproject <startproject>` automatically adds a\n+randomly-generated ``SECRET_KEY`` to each new project.\n+\n+Uses of the key shouldn't assume that it's text or bytes. Every use should go\n+through :func:`~django.utils.encoding.force_str` or\n+:func:`~django.utils.encoding.force_bytes` to convert it to the desired type.\n+\n+Django will refuse to start if :setting:`SECRET_KEY` is not set.\n+\n+.. warning::\n+\n+    **Keep this value secret.**\n+\n+    Running Django with a known :setting:`SECRET_KEY` defeats many of Django's\n+    security protections, and can lead to privilege escalation and remote code\n+    execution vulnerabilities.\n+\n+The secret key is used for:\n+\n+* All :doc:`sessions </topics/http/sessions>` if you are using\n+  any other session backend than ``django.contrib.sessions.backends.cache``,\n+  or are using the default\n+  :meth:`~django.contrib.auth.models.AbstractBaseUser.get_session_auth_hash()`.\n+* All :doc:`messages </ref/contrib/messages>` if you are using\n+  :class:`~django.contrib.messages.storage.cookie.CookieStorage` or\n+  :class:`~django.contrib.messages.storage.fallback.FallbackStorage`.\n+* All :class:`~django.contrib.auth.views.PasswordResetView` tokens.\n+* Any usage of :doc:`cryptographic signing </topics/signing>`, unless a\n+  different key is provided.\n+\n+When a secret key is no longer set as :setting:`SECRET_KEY` or contained within\n+:setting:`SECRET_KEY_FALLBACKS` all of the above will be invalidated. When\n+rotating your secret key, you should move the old key to\n+:setting:`SECRET_KEY_FALLBACKS` temporarily. Secret keys are not used for\n+passwords of users and key rotation will not affect them.\n+\n+.. note::\n+\n+    The default :file:`settings.py` file created by :djadmin:`django-admin\n+    startproject <startproject>` creates a unique ``SECRET_KEY`` for\n+    convenience.\n+\n+.. setting:: SECRET_KEY_FALLBACKS\n+\n+``SECRET_KEY_FALLBACKS``\n+------------------------\n+\n+Default: ``[]``\n+\n+A list of fallback secret keys for a particular Django installation. These are\n+used to allow rotation of the ``SECRET_KEY``.\n+\n+\n+.. setting:: SECURE_CONTENT_TYPE_NOSNIFF\n+\n+``SECURE_CONTENT_TYPE_NOSNIFF``\n+-------------------------------\n+\n+Default: ``True``\n+\n+If ``True``, the :class:`~django.middleware.security.SecurityMiddleware`\n+sets the :ref:`x-content-type-options` header on all responses that do not\n+already have it.\n+\n+.. setting:: SECURE_CROSS_ORIGIN_OPENER_POLICY\n+\n+``SECURE_CROSS_ORIGIN_OPENER_POLICY``\n+-------------------------------------\n+\n+Default: ``'same-origin'``\n+\n+Unless set to ``None``, the\n+:class:`~django.middleware.security.SecurityMiddleware` sets the\n+:ref:`cross-origin-opener-policy` header on all responses that do not already\n+have it to the value provided.\n+\n+.. setting:: SECURE_HSTS_INCLUDE_SUBDOMAINS\n+\n+``SECURE_HSTS_INCLUDE_SUBDOMAINS``\n+----------------------------------\n+\n+Default: ``False``\n+\n+If ``True``, the :class:`~django.middleware.security.SecurityMiddleware` adds\n+the ``includeSubDomains`` directive to the :ref:`http-strict-transport-security`\n+header. It has no effect unless :setting:`SECURE_HSTS_SECONDS` is set to a\n+non-zero value.\n+\n+.. warning::\n+    Setting this incorrectly can irreversibly (for the value of\n+    :setting:`SECURE_HSTS_SECONDS`) break your site. Read the\n+    :ref:`http-strict-transport-security` documentation first.\n+\n+.. setting:: SECURE_HSTS_PRELOAD\n+\n+``SECURE_HSTS_PRELOAD``\n+-----------------------\n+\n+Default: ``False``\n+\n+If ``True``, the :class:`~django.middleware.security.SecurityMiddleware` adds\n+the ``preload`` directive to the :ref:`http-strict-transport-security`\n+header. It has no effect unless :setting:`SECURE_HSTS_SECONDS` is set to a\n+non-zero value.\n+\n+.. setting:: SECURE_HSTS_SECONDS\n+\n+``SECURE_HSTS_SECONDS``\n+-----------------------\n+\n+Default: ``0``\n+\n+If set to a non-zero integer value, the\n+:class:`~django.middleware.security.SecurityMiddleware` sets the\n+:ref:`http-strict-transport-security` header on all responses that do not\n+already have it.\n+\n+.. warning::\n+    Setting this incorrectly can irreversibly (for some time) break your site.\n+    Read the :ref:`http-strict-transport-security` documentation first.\n+\n+.. setting:: SECURE_PROXY_SSL_HEADER\n+\n+``SECURE_PROXY_SSL_HEADER``\n+---------------------------\n+\n+Default: ``None``\n+\n+A tuple representing an HTTP header/value combination that signifies a request\n+is secure. This controls the behavior of the request object's ``is_secure()``\n+method.\n+\n+By default, ``is_secure()`` determines if a request is secure by confirming\n+that a requested URL uses ``https://``. This method is important for Django's\n+CSRF protection, and it may be used by your own code or third-party apps.\n+\n+If your Django app is behind a proxy, though, the proxy may be \"swallowing\"\n+whether the original request uses HTTPS or not. If there is a non-HTTPS\n+connection between the proxy and Django then ``is_secure()`` would always\n+return ``False`` -- even for requests that were made via HTTPS by the end user.\n+In contrast, if there is an HTTPS connection between the proxy and Django then\n+``is_secure()`` would always return ``True`` -- even for requests that were\n+made originally via HTTP.\n+\n+In this situation, configure your proxy to set a custom HTTP header that tells\n+Django whether the request came in via HTTPS, and set\n+``SECURE_PROXY_SSL_HEADER`` so that Django knows what header to look for.\n+\n+Set a tuple with two elements -- the name of the header to look for and the\n+required value. For example::\n+\n+    SECURE_PROXY_SSL_HEADER = (\"HTTP_X_FORWARDED_PROTO\", \"https\")\n+\n+This tells Django to trust the ``X-Forwarded-Proto`` header that comes from our\n+proxy and that the request is guaranteed to be secure (i.e., it originally came\n+in via HTTPS) when:\n+\n+* the header value is ``'https'``, or\n+* its initial, leftmost value is ``'https'`` in the case of a comma-separated\n+  list of protocols (e.g. ``'https,http,http'``).\n+\n+You should *only* set this setting if you control your proxy or have some other\n+guarantee that it sets/strips this header appropriately.\n+\n+Note that the header needs to be in the format as used by ``request.META`` --\n+all caps and likely starting with ``HTTP_``. (Remember, Django automatically\n+adds ``'HTTP_'`` to the start of x-header names before making the header\n+available in ``request.META``.)\n+\n+.. warning::\n+\n+    **Modifying this setting can compromise your site's security. Ensure you\n+    fully understand your setup before changing it.**\n+\n+    Make sure ALL of the following are true before setting this (assuming the\n+    values from the example above):\n+\n+    * Your Django app is behind a proxy.\n+    * Your proxy strips the ``X-Forwarded-Proto`` header from all incoming\n+      requests, even when it contains a comma-separated list of protocols. In\n+      other words, if end users include that header in their requests, the\n+      proxy will discard it.\n+    * Your proxy sets the ``X-Forwarded-Proto`` header and sends it to Django,\n+      but only for requests that originally come in via HTTPS.\n+\n+    If any of those are not true, you should keep this setting set to ``None``\n+    and find another way of determining HTTPS, perhaps via custom middleware.\n+\n+.. setting:: SECURE_REDIRECT_EXEMPT\n+\n+``SECURE_REDIRECT_EXEMPT``\n+--------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+If a URL path matches a regular expression in this list, the request will not be\n+redirected to HTTPS. The\n+:class:`~django.middleware.security.SecurityMiddleware` strips leading slashes\n+from URL paths, so patterns shouldn't include them, e.g.\n+``SECURE_REDIRECT_EXEMPT = [r'^no-ssl/$', \u2026]``. If\n+:setting:`SECURE_SSL_REDIRECT` is ``False``, this setting has no effect.\n+\n+.. setting:: SECURE_REFERRER_POLICY\n+\n+``SECURE_REFERRER_POLICY``\n+--------------------------\n+\n+Default: ``'same-origin'``\n+\n+If configured, the :class:`~django.middleware.security.SecurityMiddleware` sets\n+the :ref:`referrer-policy` header on all responses that do not already have it\n+to the value provided.\n+\n+.. setting:: SECURE_SSL_HOST\n+\n+``SECURE_SSL_HOST``\n+-------------------\n+\n+Default: ``None``\n+\n+If a string (e.g. ``secure.example.com``), all SSL redirects will be directed\n+to this host rather than the originally-requested host\n+(e.g. ``www.example.com``). If :setting:`SECURE_SSL_REDIRECT` is ``False``, this\n+setting has no effect.\n+\n+.. setting:: SECURE_SSL_REDIRECT\n+\n+``SECURE_SSL_REDIRECT``\n+-----------------------\n+\n+Default: ``False``\n+\n+If ``True``, the :class:`~django.middleware.security.SecurityMiddleware`\n+:ref:`redirects <ssl-redirect>` all non-HTTPS requests to HTTPS (except for\n+those URLs matching a regular expression listed in\n+:setting:`SECURE_REDIRECT_EXEMPT`).\n+\n+.. note::\n+\n+   If turning this to ``True`` causes infinite redirects, it probably means\n+   your site is running behind a proxy and can't tell which requests are secure\n+   and which are not. Your proxy likely sets a header to indicate secure\n+   requests; you can correct the problem by finding out what that header is and\n+   configuring the :setting:`SECURE_PROXY_SSL_HEADER` setting accordingly.\n+\n+.. setting:: SERIALIZATION_MODULES\n+\n+``SERIALIZATION_MODULES``\n+-------------------------\n+\n+Default: Not defined\n+\n+A dictionary of modules containing serializer definitions (provided as\n+strings), keyed by a string identifier for that serialization type. For\n+example, to define a YAML serializer, use::\n+\n+    SERIALIZATION_MODULES = {\"yaml\": \"path.to.yaml_serializer\"}\n+\n+.. setting:: SERVER_EMAIL\n+\n+``SERVER_EMAIL``\n+----------------\n+\n+Default: ``'root@localhost'``\n+\n+The email address that error messages come from, such as those sent to\n+:setting:`ADMINS` and :setting:`MANAGERS`.\n+\n+.. admonition:: Why are my emails sent from a different address?\n+\n+    This address is used only for error messages. It is *not* the address that\n+    regular email messages sent with :meth:`~django.core.mail.send_mail()`\n+    come from; for that, see :setting:`DEFAULT_FROM_EMAIL`.\n+\n+.. setting:: SHORT_DATE_FORMAT\n+\n+``SHORT_DATE_FORMAT``\n+---------------------\n+\n+Default: ``'m/d/Y'`` (e.g. ``12/31/2003``)\n+\n+An available formatting that can be used for displaying date fields on\n+templates. Note that the corresponding locale-dictated format has higher\n+precedence and will be applied instead. See\n+:tfilter:`allowed date format strings <date>`.\n+\n+See also :setting:`DATE_FORMAT` and :setting:`SHORT_DATETIME_FORMAT`.\n+\n+.. setting:: SHORT_DATETIME_FORMAT\n+\n+``SHORT_DATETIME_FORMAT``\n+-------------------------\n+\n+Default: ``'m/d/Y P'`` (e.g. ``12/31/2003 4 p.m.``)\n+\n+An available formatting that can be used for displaying datetime fields on\n+templates. Note that the corresponding locale-dictated format has higher\n+precedence and will be applied instead. See\n+:tfilter:`allowed date format strings <date>`.\n+\n+See also :setting:`DATE_FORMAT` and :setting:`SHORT_DATE_FORMAT`.\n+\n+.. setting:: SIGNING_BACKEND\n+\n+``SIGNING_BACKEND``\n+-------------------\n+\n+Default: ``'django.core.signing.TimestampSigner'``\n+\n+The backend used for signing cookies and other data.\n+\n+See also the :doc:`/topics/signing` documentation.\n+\n+.. setting:: SILENCED_SYSTEM_CHECKS\n+\n+``SILENCED_SYSTEM_CHECKS``\n+--------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list of identifiers of messages generated by the system check framework\n+(i.e. ``[\"models.W001\"]``) that you wish to permanently acknowledge and ignore.\n+Silenced checks will not be output to the console.\n+\n+See also the :doc:`/ref/checks` documentation.\n+\n+.. setting:: STORAGES\n+\n+``STORAGES``\n+------------\n+\n+.. versionadded:: 4.2\n+\n+Default::\n+\n+    {\n+        \"default\": {\n+            \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n+        },\n+        \"staticfiles\": {\n+            \"BACKEND\": \"django.contrib.staticfiles.storage.StaticFilesStorage\",\n+        },\n+    }\n+\n+A dictionary containing the settings for all storages to be used with Django.\n+It is a nested dictionary whose contents map a storage alias to a dictionary\n+containing the options for an individual storage.\n+\n+Storages can have any alias you choose. However, there are two aliases with\n+special significance:\n+\n+* ``default`` for :doc:`managing files </topics/files>`.\n+  ``'``:class:`django.core.files.storage.FileSystemStorage`\\ ``'`` is the\n+  default storage engine.\n+* ``staticfiles`` for :doc:`managing static files </ref/contrib/staticfiles>`.\n+  ``'``:class:`django.contrib.staticfiles.storage.StaticFilesStorage`\\ ``'`` is\n+  the default storage engine.\n+\n+The following is an example ``settings.py`` snippet defining a custom file\n+storage called ``example``::\n+\n+    STORAGES = {\n+        # ...\n+        \"example\": {\n+            \"BACKEND\": \"django.core.files.storage.FileSystemStorage\",\n+            \"OPTIONS\": {\n+                \"location\": \"/example\",\n+                \"base_url\": \"/example/\",\n+            },\n+        },\n+    }\n+\n+``OPTIONS`` are passed to the ``BACKEND`` on initialization in ``**kwargs``.\n+\n+A ready-to-use instance of the storage backends can be retrieved from\n+:data:`django.core.files.storage.storages`. Use a key corresponding to the\n+backend definition in :setting:`STORAGES`.\n+\n+.. setting:: TEMPLATES\n+\n+``TEMPLATES``\n+-------------\n+\n+Default: ``[]`` (Empty list)\n+\n+A list containing the settings for all template engines to be used with\n+Django. Each item of the list is a dictionary containing the options for an\n+individual engine.\n+\n+Here's a setup that tells the Django template engine to load templates from the\n+``templates`` subdirectory inside each installed application::\n+\n+    TEMPLATES = [\n+        {\n+            \"BACKEND\": \"django.template.backends.django.DjangoTemplates\",\n+            \"APP_DIRS\": True,\n+        },\n+    ]\n+\n+The following options are available for all backends.\n+\n+.. setting:: TEMPLATES-BACKEND\n+\n+``BACKEND``\n+~~~~~~~~~~~\n+\n+Default: Not defined\n+\n+The template backend to use. The built-in template backends are:\n+\n+* ``'django.template.backends.django.DjangoTemplates'``\n+* ``'django.template.backends.jinja2.Jinja2'``\n+\n+You can use a template backend that doesn't ship with Django by setting\n+``BACKEND`` to a fully-qualified path (i.e. ``'mypackage.whatever.Backend'``).\n+\n+.. setting:: TEMPLATES-NAME\n+\n+``NAME``\n+~~~~~~~~\n+\n+Default: see below\n+\n+The alias for this particular template engine. It's an identifier that allows\n+selecting an engine for rendering. Aliases must be unique across all\n+configured template engines.\n+\n+It defaults to the name of the module defining the engine class, i.e. the\n+next to last piece of :setting:`BACKEND <TEMPLATES-BACKEND>`, when it isn't\n+provided. For example if the backend is ``'mypackage.whatever.Backend'`` then\n+its default name is ``'whatever'``.\n+\n+.. setting:: TEMPLATES-DIRS\n+\n+``DIRS``\n+~~~~~~~~\n+\n+Default: ``[]`` (Empty list)\n+\n+Directories where the engine should look for template source files, in search\n+order.\n+\n+.. setting:: TEMPLATES-APP_DIRS\n+\n+``APP_DIRS``\n+~~~~~~~~~~~~\n+\n+Default: ``False``\n+\n+Whether the engine should look for template source files inside installed\n+applications.\n+\n+.. note::\n+\n+    The default :file:`settings.py` file created by :djadmin:`django-admin\n+    startproject <startproject>` sets ``'APP_DIRS': True``.\n+\n+.. setting:: TEMPLATES-OPTIONS\n+\n+``OPTIONS``\n+~~~~~~~~~~~\n+\n+Default: ``{}`` (Empty dict)\n+\n+Extra parameters to pass to the template backend. Available parameters vary\n+depending on the template backend. See\n+:class:`~django.template.backends.django.DjangoTemplates` and\n+:class:`~django.template.backends.jinja2.Jinja2` for the options of the\n+built-in backends.\n+\n+.. setting:: TEST_RUNNER\n+\n+``TEST_RUNNER``\n+---------------\n+\n+Default: ``'django.test.runner.DiscoverRunner'``\n+\n+The name of the class to use for starting the test suite. See\n+:ref:`other-testing-frameworks`.\n+\n+.. setting:: TEST_NON_SERIALIZED_APPS\n+\n+``TEST_NON_SERIALIZED_APPS``\n+----------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+In order to restore the database state between tests for\n+``TransactionTestCase``\\s and database backends without transactions, Django\n+will :ref:`serialize the contents of all apps <test-case-serialized-rollback>`\n+when it starts the test run so it can then reload from that copy before running\n+tests that need it.\n+\n+This slows down the startup time of the test runner; if you have apps that\n+you know don't need this feature, you can add their full names in here (e.g.\n+``'django.contrib.contenttypes'``) to exclude them from this serialization\n+process.\n+\n+.. setting:: THOUSAND_SEPARATOR\n+\n+``THOUSAND_SEPARATOR``\n+----------------------\n+\n+Default: ``','`` (Comma)\n+\n+Default thousand separator used when formatting numbers. This setting is\n+used only when :setting:`USE_THOUSAND_SEPARATOR` is ``True`` and\n+:setting:`NUMBER_GROUPING` is greater than ``0``.\n+\n+Note that the locale-dictated format has higher precedence and will be applied\n+instead.\n+\n+See also :setting:`NUMBER_GROUPING`, :setting:`DECIMAL_SEPARATOR` and\n+:setting:`USE_THOUSAND_SEPARATOR`.\n+\n+.. setting:: TIME_FORMAT\n+\n+``TIME_FORMAT``\n+---------------\n+\n+Default: ``'P'`` (e.g. ``4 p.m.``)\n+\n+The default formatting to use for displaying time fields in any part of the\n+system. Note that the locale-dictated format has higher precedence and will be\n+applied instead. See :tfilter:`allowed date format strings <date>`.\n+\n+See also :setting:`DATE_FORMAT` and :setting:`DATETIME_FORMAT`.\n+\n+.. setting:: TIME_INPUT_FORMATS\n+\n+``TIME_INPUT_FORMATS``\n+----------------------\n+\n+Default::\n+\n+    [\n+        \"%H:%M:%S\",  # '14:30:59'\n+        \"%H:%M:%S.%f\",  # '14:30:59.000200'\n+        \"%H:%M\",  # '14:30'\n+    ]\n+\n+A list of formats that will be accepted when inputting data on a time field.\n+Formats will be tried in order, using the first valid one. Note that these\n+format strings use Python's :ref:`datetime module syntax\n+<strftime-strptime-behavior>`, not the format strings from the :tfilter:`date`\n+template filter.\n+\n+The locale-dictated format has higher precedence and will be applied instead.\n+\n+See also :setting:`DATE_INPUT_FORMATS` and :setting:`DATETIME_INPUT_FORMATS`.\n+\n+.. setting:: TIME_ZONE\n+\n+``TIME_ZONE``\n+-------------\n+\n+Default: ``'America/Chicago'``\n+\n+A string representing the time zone for this installation. See the `list of\n+time zones`_.\n+\n+.. note::\n+    Since Django was first released with the :setting:`TIME_ZONE` set to\n+    ``'America/Chicago'``, the global setting (used if nothing is defined in\n+    your project's ``settings.py``) remains ``'America/Chicago'`` for backwards\n+    compatibility. New project templates default to ``'UTC'``.\n+\n+Note that this isn't necessarily the time zone of the server. For example, one\n+server may serve multiple Django-powered sites, each with a separate time zone\n+setting.\n+\n+When :setting:`USE_TZ` is ``False``, this is the time zone in which Django\n+will store all datetimes. When :setting:`USE_TZ` is ``True``, this is the\n+default time zone that Django will use to display datetimes in templates and\n+to interpret datetimes entered in forms.\n+\n+On Unix environments (where :func:`time.tzset` is implemented), Django sets the\n+``os.environ['TZ']`` variable to the time zone you specify in the\n+:setting:`TIME_ZONE` setting. Thus, all your views and models will\n+automatically operate in this time zone. However, Django won't set the ``TZ``\n+environment variable if you're using the manual configuration option as\n+described in :ref:`manually configuring settings\n+<settings-without-django-settings-module>`. If Django doesn't set the ``TZ``\n+environment variable, it's up to you to ensure your processes are running in\n+the correct environment.\n+\n+.. note::\n+    Django cannot reliably use alternate time zones in a Windows environment.\n+    If you're running Django on Windows, :setting:`TIME_ZONE` must be set to\n+    match the system time zone.\n+\n+.. _list of time zones: https://en.wikipedia.org/wiki/List_of_tz_database_time_zones\n+\n+.. setting:: USE_I18N\n+\n+``USE_I18N``\n+------------\n+\n+Default: ``True``\n+\n+A boolean that specifies whether Django's translation system should be enabled.\n+This provides a way to turn it off, for performance. If this is set to\n+``False``, Django will make some optimizations so as not to load the\n+translation machinery.\n+\n+See also :setting:`LANGUAGE_CODE` and :setting:`USE_TZ`.\n+\n+.. note::\n+\n+    The default :file:`settings.py` file created by :djadmin:`django-admin\n+    startproject <startproject>` includes ``USE_I18N = True`` for convenience.\n+\n+.. setting:: USE_THOUSAND_SEPARATOR\n+\n+``USE_THOUSAND_SEPARATOR``\n+--------------------------\n+\n+Default: ``False``\n+\n+A boolean that specifies whether to display numbers using a thousand separator.\n+When set to ``True``, Django will format numbers using the\n+:setting:`NUMBER_GROUPING` and :setting:`THOUSAND_SEPARATOR` settings. The\n+latter two settings may also be dictated by the locale, which takes precedence.\n+\n+See also :setting:`DECIMAL_SEPARATOR`, :setting:`NUMBER_GROUPING` and\n+:setting:`THOUSAND_SEPARATOR`.\n+\n+.. setting:: USE_TZ\n+\n+``USE_TZ``\n+----------\n+\n+Default: ``True``\n+\n+A boolean that specifies if datetimes will be timezone-aware by default or not.\n+If this is set to ``True``, Django will use timezone-aware datetimes internally.\n+\n+When ``USE_TZ`` is False, Django will use naive datetimes in local time, except\n+when parsing ISO 8601 formatted strings, where timezone information will always\n+be retained if present.\n+\n+See also :setting:`TIME_ZONE` and :setting:`USE_I18N`.\n+\n+.. versionchanged:: 5.0\n+\n+    In older versions, the default value is ``False``.\n+\n+.. setting:: USE_X_FORWARDED_HOST\n+\n+``USE_X_FORWARDED_HOST``\n+------------------------\n+\n+Default: ``False``\n+\n+A boolean that specifies whether to use the ``X-Forwarded-Host`` header in\n+preference to the ``Host`` header. This should only be enabled if a proxy\n+which sets this header is in use.\n+\n+This setting takes priority over :setting:`USE_X_FORWARDED_PORT`. Per\n+:rfc:`7239#section-5.3`, the ``X-Forwarded-Host`` header can include the port\n+number, in which case you shouldn't use :setting:`USE_X_FORWARDED_PORT`.\n+\n+.. setting:: USE_X_FORWARDED_PORT\n+\n+``USE_X_FORWARDED_PORT``\n+------------------------\n+\n+Default: ``False``\n+\n+A boolean that specifies whether to use the ``X-Forwarded-Port`` header in\n+preference to the ``SERVER_PORT`` ``META`` variable. This should only be\n+enabled if a proxy which sets this header is in use.\n+\n+:setting:`USE_X_FORWARDED_HOST` takes priority over this setting.\n+\n+.. setting:: WSGI_APPLICATION\n+\n+``WSGI_APPLICATION``\n+--------------------\n+\n+Default: ``None``\n+\n+The full Python path of the WSGI application object that Django's built-in\n+servers (e.g. :djadmin:`runserver`) will use. The :djadmin:`django-admin\n+startproject <startproject>` management command will create a standard\n+``wsgi.py`` file with an ``application`` callable in it, and point this setting\n+to that ``application``.\n+\n+If not set, the return value of ``django.core.wsgi.get_wsgi_application()``\n+will be used. In this case, the behavior of :djadmin:`runserver` will be\n+identical to previous Django versions.\n+\n+.. setting:: YEAR_MONTH_FORMAT\n+\n+``YEAR_MONTH_FORMAT``\n+---------------------\n+\n+Default: ``'F Y'``\n+\n+The default formatting to use for date fields on Django admin change-list\n+pages -- and, possibly, by other parts of the system -- in cases when only the\n+year and month are displayed.\n+\n+For example, when a Django admin change-list page is being filtered by a date\n+drilldown, the header for a given month displays the month and the year.\n+Different locales have different formats. For example, U.S. English would say\n+\"January 2006,\" whereas another locale might say \"2006/January.\"\n+\n+Note that the corresponding locale-dictated format has higher precedence and\n+will be applied instead.\n+\n+See :tfilter:`allowed date format strings <date>`. See also\n+:setting:`DATE_FORMAT`, :setting:`DATETIME_FORMAT`, :setting:`TIME_FORMAT`\n+and :setting:`MONTH_DAY_FORMAT`.\n+\n+.. setting:: X_FRAME_OPTIONS\n+\n+``X_FRAME_OPTIONS``\n+-------------------\n+\n+Default: ``'DENY'``\n+\n+The default value for the X-Frame-Options header used by\n+:class:`~django.middleware.clickjacking.XFrameOptionsMiddleware`. See the\n+:doc:`clickjacking protection </ref/clickjacking/>` documentation.\n+\n+Auth\n+====\n+\n+Settings for :mod:`django.contrib.auth`.\n+\n+.. setting:: AUTHENTICATION_BACKENDS\n+\n+``AUTHENTICATION_BACKENDS``\n+---------------------------\n+\n+Default: ``['django.contrib.auth.backends.ModelBackend']``\n+\n+A list of authentication backend classes (as strings) to use when attempting to\n+authenticate a user. See the :ref:`authentication backends documentation\n+<authentication-backends>` for details.\n+\n+.. setting:: AUTH_USER_MODEL\n+\n+``AUTH_USER_MODEL``\n+-------------------\n+\n+Default: ``'auth.User'``\n+\n+The model to use to represent a User. See :ref:`auth-custom-user`.\n+\n+.. warning::\n+    You cannot change the AUTH_USER_MODEL setting during the lifetime of\n+    a project (i.e. once you have made and migrated models that depend on it)\n+    without serious effort. It is intended to be set at the project start,\n+    and the model it refers to must be available in the first migration of\n+    the app that it lives in.\n+    See :ref:`auth-custom-user` for more details.\n+\n+.. setting:: LOGIN_REDIRECT_URL\n+\n+``LOGIN_REDIRECT_URL``\n+----------------------\n+\n+Default: ``'/accounts/profile/'``\n+\n+The URL or :ref:`named URL pattern <naming-url-patterns>` where requests are\n+redirected after login when the :class:`~django.contrib.auth.views.LoginView`\n+doesn't get a ``next`` GET parameter.\n+\n+.. setting:: LOGIN_URL\n+\n+``LOGIN_URL``\n+-------------\n+\n+Default: ``'/accounts/login/'``\n+\n+The URL or :ref:`named URL pattern <naming-url-patterns>` where requests are\n+redirected for login when using the\n+:func:`~django.contrib.auth.decorators.login_required` decorator,\n+:class:`~django.contrib.auth.mixins.LoginRequiredMixin`, or\n+:class:`~django.contrib.auth.mixins.AccessMixin`.\n+\n+.. setting:: LOGOUT_REDIRECT_URL\n+\n+``LOGOUT_REDIRECT_URL``\n+-----------------------\n+\n+Default: ``None``\n+\n+The URL or :ref:`named URL pattern <naming-url-patterns>` where requests are\n+redirected after logout if :class:`~django.contrib.auth.views.LogoutView`\n+doesn't have a ``next_page`` attribute.\n+\n+If ``None``, no redirect will be performed and the logout view will be\n+rendered.\n+\n+.. setting:: PASSWORD_RESET_TIMEOUT\n+\n+``PASSWORD_RESET_TIMEOUT``\n+--------------------------\n+\n+Default: ``259200`` (3 days, in seconds)\n+\n+The number of seconds a password reset link is valid for.\n+\n+Used by the :class:`~django.contrib.auth.views.PasswordResetConfirmView`.\n+\n+.. note::\n+\n+   Reducing the value of this timeout doesn't make any difference to the\n+   ability of an attacker to brute-force a password reset token. Tokens are\n+   designed to be safe from brute-forcing without any timeout.\n+\n+   This timeout exists to protect against some unlikely attack scenarios, such\n+   as someone gaining access to email archives that may contain old, unused\n+   password reset tokens.\n+\n+.. setting:: PASSWORD_HASHERS\n+\n+``PASSWORD_HASHERS``\n+--------------------\n+\n+See :ref:`auth_password_storage`.\n+\n+Default::\n+\n+    [\n+        \"django.contrib.auth.hashers.PBKDF2PasswordHasher\",\n+        \"django.contrib.auth.hashers.PBKDF2SHA1PasswordHasher\",\n+        \"django.contrib.auth.hashers.Argon2PasswordHasher\",\n+        \"django.contrib.auth.hashers.BCryptSHA256PasswordHasher\",\n+    ]\n+\n+.. setting:: AUTH_PASSWORD_VALIDATORS\n+\n+``AUTH_PASSWORD_VALIDATORS``\n+----------------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+The list of validators that are used to check the strength of user's passwords.\n+See :ref:`password-validation` for more details. By default, no validation is\n+performed and all passwords are accepted.\n+\n+.. _settings-messages:\n+\n+Messages\n+========\n+\n+Settings for :mod:`django.contrib.messages`.\n+\n+.. setting:: MESSAGE_LEVEL\n+\n+``MESSAGE_LEVEL``\n+-----------------\n+\n+Default: ``messages.INFO``\n+\n+Sets the minimum message level that will be recorded by the messages\n+framework. See :ref:`message levels <message-level>` for more details.\n+\n+.. admonition:: Important\n+\n+   If you override ``MESSAGE_LEVEL`` in your settings file and rely on any of\n+   the built-in constants, you must import the constants module directly to\n+   avoid the potential for circular imports, e.g.::\n+\n+       from django.contrib.messages import constants as message_constants\n+\n+       MESSAGE_LEVEL = message_constants.DEBUG\n+\n+   If desired, you may specify the numeric values for the constants directly\n+   according to the values in the above :ref:`constants table\n+   <message-level-constants>`.\n+\n+.. setting:: MESSAGE_STORAGE\n+\n+``MESSAGE_STORAGE``\n+-------------------\n+\n+Default: ``'django.contrib.messages.storage.fallback.FallbackStorage'``\n+\n+Controls where Django stores message data. Valid values are:\n+\n+* ``'django.contrib.messages.storage.fallback.FallbackStorage'``\n+* ``'django.contrib.messages.storage.session.SessionStorage'``\n+* ``'django.contrib.messages.storage.cookie.CookieStorage'``\n+\n+See :ref:`message storage backends <message-storage-backends>` for more details.\n+\n+The backends that use cookies --\n+:class:`~django.contrib.messages.storage.cookie.CookieStorage` and\n+:class:`~django.contrib.messages.storage.fallback.FallbackStorage` --\n+use the value of :setting:`SESSION_COOKIE_DOMAIN`, :setting:`SESSION_COOKIE_SECURE`\n+and :setting:`SESSION_COOKIE_HTTPONLY` when setting their cookies.\n+\n+.. setting:: MESSAGE_TAGS\n+\n+``MESSAGE_TAGS``\n+----------------\n+\n+Default::\n+\n+    {\n+        messages.DEBUG: \"debug\",\n+        messages.INFO: \"info\",\n+        messages.SUCCESS: \"success\",\n+        messages.WARNING: \"warning\",\n+        messages.ERROR: \"error\",\n+    }\n+\n+This sets the mapping of message level to message tag, which is typically\n+rendered as a CSS class in HTML. If you specify a value, it will extend\n+the default. This means you only have to specify those values which you need\n+to override. See :ref:`message-displaying` above for more details.\n+\n+.. admonition:: Important\n+\n+   If you override ``MESSAGE_TAGS`` in your settings file and rely on any of\n+   the built-in constants, you must import the ``constants`` module directly to\n+   avoid the potential for circular imports, e.g.::\n+\n+       from django.contrib.messages import constants as message_constants\n+\n+       MESSAGE_TAGS = {message_constants.INFO: \"\"}\n+\n+   If desired, you may specify the numeric values for the constants directly\n+   according to the values in the above :ref:`constants table\n+   <message-level-constants>`.\n+\n+.. _settings-sessions:\n+\n+Sessions\n+========\n+\n+Settings for :mod:`django.contrib.sessions`.\n+\n+.. setting:: SESSION_CACHE_ALIAS\n+\n+``SESSION_CACHE_ALIAS``\n+-----------------------\n+\n+Default: ``'default'``\n+\n+If you're using :ref:`cache-based session storage <cached-sessions-backend>`,\n+this selects the cache to use.\n+\n+.. setting:: SESSION_COOKIE_AGE\n+\n+``SESSION_COOKIE_AGE``\n+----------------------\n+\n+Default: ``1209600`` (2 weeks, in seconds)\n+\n+The age of session cookies, in seconds.\n+\n+.. setting:: SESSION_COOKIE_DOMAIN\n+\n+``SESSION_COOKIE_DOMAIN``\n+-------------------------\n+\n+Default: ``None``\n+\n+The domain to use for session cookies. Set this to a string such as\n+``\"example.com\"`` for cross-domain cookies, or use ``None`` for a standard\n+domain cookie.\n+\n+To use cross-domain cookies with :setting:`CSRF_USE_SESSIONS`, you must include\n+a leading dot (e.g. ``\".example.com\"``) to accommodate the CSRF middleware's\n+referer checking.\n+\n+Be cautious when updating this setting on a production site. If you update\n+this setting to enable cross-domain cookies on a site that previously used\n+standard domain cookies, existing user cookies will be set to the old\n+domain. This may result in them being unable to log in as long as these cookies\n+persist.\n+\n+This setting also affects cookies set by :mod:`django.contrib.messages`.\n+\n+.. setting:: SESSION_COOKIE_HTTPONLY\n+\n+``SESSION_COOKIE_HTTPONLY``\n+---------------------------\n+\n+Default: ``True``\n+\n+Whether to use ``HttpOnly`` flag on the session cookie. If this is set to\n+``True``, client-side JavaScript will not be able to access the session\n+cookie.\n+\n+HttpOnly_ is a flag included in a Set-Cookie HTTP response header. It's part of\n+the :rfc:`6265#section-4.1.2.6` standard for cookies and can be a useful way to\n+mitigate the risk of a client-side script accessing the protected cookie data.\n+\n+This makes it less trivial for an attacker to escalate a cross-site scripting\n+vulnerability into full hijacking of a user's session. There aren't many good\n+reasons for turning this off. Your code shouldn't read session cookies from\n+JavaScript.\n+\n+.. _HttpOnly: https://owasp.org/www-community/HttpOnly\n+\n+.. setting:: SESSION_COOKIE_NAME\n+\n+``SESSION_COOKIE_NAME``\n+-----------------------\n+\n+Default: ``'sessionid'``\n+\n+The name of the cookie to use for sessions. This can be whatever you want\n+(as long as it's different from the other cookie names in your application).\n+\n+.. setting:: SESSION_COOKIE_PATH\n+\n+``SESSION_COOKIE_PATH``\n+-----------------------\n+\n+Default: ``'/'``\n+\n+The path set on the session cookie. This should either match the URL path of your\n+Django installation or be parent of that path.\n+\n+This is useful if you have multiple Django instances running under the same\n+hostname. They can use different cookie paths, and each instance will only see\n+its own session cookie.\n+\n+.. setting:: SESSION_COOKIE_SAMESITE\n+\n+``SESSION_COOKIE_SAMESITE``\n+---------------------------\n+\n+Default: ``'Lax'``\n+\n+The value of the `SameSite`_ flag on the session cookie. This flag prevents the\n+cookie from being sent in cross-site requests thus preventing CSRF attacks and\n+making some methods of stealing session cookie impossible.\n+\n+Possible values for the setting are:\n+\n+* ``'Strict'``: prevents the cookie from being sent by the browser to the\n+  target site in all cross-site browsing context, even when following a regular\n+  link.\n+\n+  For example, for a GitHub-like website this would mean that if a logged-in\n+  user follows a link to a private GitHub project posted on a corporate\n+  discussion forum or email, GitHub will not receive the session cookie and the\n+  user won't be able to access the project. A bank website, however, most\n+  likely doesn't want to allow any transactional pages to be linked from\n+  external sites so the ``'Strict'`` flag would be appropriate.\n+\n+* ``'Lax'`` (default): provides a balance between security and usability for\n+  websites that want to maintain user's logged-in session after the user\n+  arrives from an external link.\n+\n+  In the GitHub scenario, the session cookie would be allowed when following a\n+  regular link from an external website and be blocked in CSRF-prone request\n+  methods (e.g. ``POST``).\n+\n+* ``'None'`` (string): the session cookie will be sent with all same-site and\n+  cross-site requests.\n+\n+* ``False``: disables the flag.\n+\n+.. note::\n+\n+    Modern browsers provide a more secure default policy for the ``SameSite``\n+    flag and will assume ``Lax`` for cookies without an explicit value set.\n+\n+.. _SameSite: https://developer.mozilla.org/en-US/docs/Web/HTTP/Headers/Set-Cookie/SameSite\n+\n+.. setting:: SESSION_COOKIE_SECURE\n+\n+``SESSION_COOKIE_SECURE``\n+-------------------------\n+\n+Default: ``False``\n+\n+Whether to use a secure cookie for the session cookie. If this is set to\n+``True``, the cookie will be marked as \"secure\", which means browsers may\n+ensure that the cookie is only sent under an HTTPS connection.\n+\n+Leaving this setting off isn't a good idea because an attacker could capture an\n+unencrypted session cookie with a packet sniffer and use the cookie to hijack\n+the user's session.\n+\n+.. setting:: SESSION_ENGINE\n+\n+``SESSION_ENGINE``\n+------------------\n+\n+Default: ``'django.contrib.sessions.backends.db'``\n+\n+Controls where Django stores session data. Included engines are:\n+\n+* ``'django.contrib.sessions.backends.db'``\n+* ``'django.contrib.sessions.backends.file'``\n+* ``'django.contrib.sessions.backends.cache'``\n+* ``'django.contrib.sessions.backends.cached_db'``\n+* ``'django.contrib.sessions.backends.signed_cookies'``\n+\n+See :ref:`configuring-sessions` for more details.\n+\n+.. setting:: SESSION_EXPIRE_AT_BROWSER_CLOSE\n+\n+``SESSION_EXPIRE_AT_BROWSER_CLOSE``\n+-----------------------------------\n+\n+Default: ``False``\n+\n+Whether to expire the session when the user closes their browser. See\n+:ref:`browser-length-vs-persistent-sessions`.\n+\n+.. setting:: SESSION_FILE_PATH\n+\n+``SESSION_FILE_PATH``\n+---------------------\n+\n+Default: ``None``\n+\n+If you're using file-based session storage, this sets the directory in\n+which Django will store session data. When the default value (``None``) is\n+used, Django will use the standard temporary directory for the system.\n+\n+\n+.. setting:: SESSION_SAVE_EVERY_REQUEST\n+\n+``SESSION_SAVE_EVERY_REQUEST``\n+------------------------------\n+\n+Default: ``False``\n+\n+Whether to save the session data on every request. If this is ``False``\n+(default), then the session data will only be saved if it has been modified --\n+that is, if any of its dictionary values have been assigned or deleted. Empty\n+sessions won't be created, even if this setting is active.\n+\n+.. setting:: SESSION_SERIALIZER\n+\n+``SESSION_SERIALIZER``\n+----------------------\n+\n+Default: ``'django.contrib.sessions.serializers.JSONSerializer'``\n+\n+Full import path of a serializer class to use for serializing session data.\n+Included serializer is:\n+\n+* ``'django.contrib.sessions.serializers.JSONSerializer'``\n+\n+See :ref:`session_serialization` for details.\n+\n+Sites\n+=====\n+\n+Settings for :mod:`django.contrib.sites`.\n+\n+.. setting:: SITE_ID\n+\n+``SITE_ID``\n+-----------\n+\n+Default: Not defined\n+\n+The ID, as an integer, of the current site in the ``django_site`` database\n+table. This is used so that application data can hook into specific sites\n+and a single database can manage content for multiple sites.\n+\n+\n+.. _settings-staticfiles:\n+\n+Static Files\n+============\n+\n+Settings for :mod:`django.contrib.staticfiles`.\n+\n+.. setting:: STATIC_ROOT\n+\n+``STATIC_ROOT``\n+---------------\n+\n+Default: ``None``\n+\n+The absolute path to the directory where :djadmin:`collectstatic` will collect\n+static files for deployment.\n+\n+Example: ``\"/var/www/example.com/static/\"``\n+\n+If the :doc:`staticfiles</ref/contrib/staticfiles>` contrib app is enabled\n+(as in the default project template), the :djadmin:`collectstatic` management\n+command will collect static files into this directory. See the how-to on\n+:doc:`managing static files</howto/static-files/index>` for more details about\n+usage.\n+\n+.. warning::\n+\n+    This should be an initially empty destination directory for collecting\n+    your static files from their permanent locations into one directory for\n+    ease of deployment; it is **not** a place to store your static files\n+    permanently. You should do that in directories that will be found by\n+    :doc:`staticfiles</ref/contrib/staticfiles>`\u2019s\n+    :setting:`finders<STATICFILES_FINDERS>`, which by default, are\n+    ``'static/'`` app sub-directories and any directories you include in\n+    :setting:`STATICFILES_DIRS`).\n+\n+.. setting:: STATIC_URL\n+\n+``STATIC_URL``\n+--------------\n+\n+Default: ``None``\n+\n+URL to use when referring to static files located in :setting:`STATIC_ROOT`.\n+\n+Example: ``\"static/\"`` or ``\"http://static.example.com/\"``\n+\n+If not ``None``, this will be used as the base path for\n+:ref:`asset definitions<form-asset-paths>` (the ``Media`` class) and the\n+:doc:`staticfiles app</ref/contrib/staticfiles>`.\n+\n+It must end in a slash if set to a non-empty value.\n+\n+You may need to :ref:`configure these files to be served in development\n+<serving-static-files-in-development>` and will definitely need to do so\n+:doc:`in production </howto/static-files/deployment>`.\n+\n+.. note::\n+\n+    If :setting:`STATIC_URL` is a relative path, then it will be prefixed by\n+    the server-provided value of ``SCRIPT_NAME`` (or ``/`` if not set). This\n+    makes it easier to serve a Django application in a subpath without adding\n+    an extra configuration to the settings.\n+\n+.. setting:: STATICFILES_DIRS\n+\n+``STATICFILES_DIRS``\n+--------------------\n+\n+Default: ``[]`` (Empty list)\n+\n+This setting defines the additional locations the staticfiles app will traverse\n+if the ``FileSystemFinder`` finder is enabled, e.g. if you use the\n+:djadmin:`collectstatic` or :djadmin:`findstatic` management command or use the\n+static file serving view.\n+\n+This should be set to a list of strings that contain full paths to\n+your additional files directory(ies) e.g.::\n+\n+    STATICFILES_DIRS = [\n+        \"/home/special.polls.com/polls/static\",\n+        \"/home/polls.com/polls/static\",\n+        \"/opt/webfiles/common\",\n+    ]\n+\n+Note that these paths should use Unix-style forward slashes, even on Windows\n+(e.g. ``\"C:/Users/user/mysite/extra_static_content\"``).\n+\n+.. _staticfiles-dirs-prefixes:\n+\n+Prefixes (optional)\n+~~~~~~~~~~~~~~~~~~~\n+\n+In case you want to refer to files in one of the locations with an additional\n+namespace, you can **optionally** provide a prefix as ``(prefix, path)``\n+tuples, e.g.::\n+\n+    STATICFILES_DIRS = [\n+        # ...\n+        (\"downloads\", \"/opt/webfiles/stats\"),\n+    ]\n+\n+For example, assuming you have :setting:`STATIC_URL` set to ``'static/'``, the\n+:djadmin:`collectstatic` management command would collect the \"stats\" files\n+in a ``'downloads'`` subdirectory of :setting:`STATIC_ROOT`.\n+\n+This would allow you to refer to the local file\n+``'/opt/webfiles/stats/polls_20101022.tar.gz'`` with\n+``'/static/downloads/polls_20101022.tar.gz'`` in your templates, e.g.:\n+\n+.. code-block:: html+django\n+\n+    <a href=\"{% static 'downloads/polls_20101022.tar.gz' %}\">\n+\n+.. setting:: STATICFILES_STORAGE\n+\n+``STATICFILES_STORAGE``\n+-----------------------\n+\n+Default: ``'django.contrib.staticfiles.storage.StaticFilesStorage'``\n+\n+The file storage engine to use when collecting static files with the\n+:djadmin:`collectstatic` management command.\n+\n+A ready-to-use instance of the storage backend defined in this setting\n+can be found under ``staticfiles`` key in ``django.core.files.storage.storages``.\n+\n+For an example, see :ref:`staticfiles-from-cdn`.\n+\n+.. deprecated:: 4.2\n+\n+    This setting is deprecated. Starting with Django 4.2, static files storage\n+    engine can be configured with the :setting:`STORAGES` setting under the\n+    ``staticfiles`` key.\n+\n+.. setting:: STATICFILES_FINDERS\n+\n+``STATICFILES_FINDERS``\n+-----------------------\n+\n+Default::\n+\n+    [\n+        \"django.contrib.staticfiles.finders.FileSystemFinder\",\n+        \"django.contrib.staticfiles.finders.AppDirectoriesFinder\",\n+    ]\n+\n+The list of finder backends that know how to find static files in\n+various locations.\n+\n+The default will find files stored in the :setting:`STATICFILES_DIRS` setting\n+(using ``django.contrib.staticfiles.finders.FileSystemFinder``) and in a\n+``static`` subdirectory of each app (using\n+``django.contrib.staticfiles.finders.AppDirectoriesFinder``). If multiple\n+files with the same name are present, the first file that is found will be\n+used.\n+\n+One finder is disabled by default:\n+``django.contrib.staticfiles.finders.DefaultStorageFinder``. If added to\n+your :setting:`STATICFILES_FINDERS` setting, it will look for static files in\n+the default file storage as defined by the ``default`` key in the\n+:setting:`STORAGES` setting.\n+\n+.. note::\n+\n+    When using the ``AppDirectoriesFinder`` finder, make sure your apps\n+    can be found by staticfiles by adding the app to the\n+    :setting:`INSTALLED_APPS` setting of your site.\n+\n+Static file finders are currently considered a private interface, and this\n+interface is thus undocumented.\n+\n+Core Settings Topical Index\n+===========================\n+\n+Cache\n+-----\n+* :setting:`CACHES`\n+* :setting:`CACHE_MIDDLEWARE_ALIAS`\n+* :setting:`CACHE_MIDDLEWARE_KEY_PREFIX`\n+* :setting:`CACHE_MIDDLEWARE_SECONDS`\n+\n+Database\n+--------\n+* :setting:`DATABASES`\n+* :setting:`DATABASE_ROUTERS`\n+* :setting:`DEFAULT_INDEX_TABLESPACE`\n+* :setting:`DEFAULT_TABLESPACE`\n+\n+Debugging\n+---------\n+* :setting:`DEBUG`\n+* :setting:`DEBUG_PROPAGATE_EXCEPTIONS`\n+\n+Email\n+-----\n+* :setting:`ADMINS`\n+* :setting:`DEFAULT_CHARSET`\n+* :setting:`DEFAULT_FROM_EMAIL`\n+* :setting:`EMAIL_BACKEND`\n+* :setting:`EMAIL_FILE_PATH`\n+* :setting:`EMAIL_HOST`\n+* :setting:`EMAIL_HOST_PASSWORD`\n+* :setting:`EMAIL_HOST_USER`\n+* :setting:`EMAIL_PORT`\n+* :setting:`EMAIL_SSL_CERTFILE`\n+* :setting:`EMAIL_SSL_KEYFILE`\n+* :setting:`EMAIL_SUBJECT_PREFIX`\n+* :setting:`EMAIL_TIMEOUT`\n+* :setting:`EMAIL_USE_LOCALTIME`\n+* :setting:`EMAIL_USE_TLS`\n+* :setting:`MANAGERS`\n+* :setting:`SERVER_EMAIL`\n+\n+Error reporting\n+---------------\n+* :setting:`DEFAULT_EXCEPTION_REPORTER`\n+* :setting:`DEFAULT_EXCEPTION_REPORTER_FILTER`\n+* :setting:`IGNORABLE_404_URLS`\n+* :setting:`MANAGERS`\n+* :setting:`SILENCED_SYSTEM_CHECKS`\n+\n+.. _file-upload-settings:\n+\n+File uploads\n+------------\n+* :setting:`DEFAULT_FILE_STORAGE`\n+* :setting:`FILE_UPLOAD_HANDLERS`\n+* :setting:`FILE_UPLOAD_MAX_MEMORY_SIZE`\n+* :setting:`FILE_UPLOAD_PERMISSIONS`\n+* :setting:`FILE_UPLOAD_TEMP_DIR`\n+* :setting:`MEDIA_ROOT`\n+* :setting:`MEDIA_URL`\n+* :setting:`STORAGES`\n+\n+Forms\n+-----\n+* :setting:`FORM_RENDERER`\n+\n+Globalization (``i18n``/``l10n``)\n+---------------------------------\n+* :setting:`DATE_FORMAT`\n+* :setting:`DATE_INPUT_FORMATS`\n+* :setting:`DATETIME_FORMAT`\n+* :setting:`DATETIME_INPUT_FORMATS`\n+* :setting:`DECIMAL_SEPARATOR`\n+* :setting:`FIRST_DAY_OF_WEEK`\n+* :setting:`FORMAT_MODULE_PATH`\n+* :setting:`LANGUAGE_CODE`\n+* :setting:`LANGUAGE_COOKIE_AGE`\n+* :setting:`LANGUAGE_COOKIE_DOMAIN`\n+* :setting:`LANGUAGE_COOKIE_HTTPONLY`\n+* :setting:`LANGUAGE_COOKIE_NAME`\n+* :setting:`LANGUAGE_COOKIE_PATH`\n+* :setting:`LANGUAGE_COOKIE_SAMESITE`\n+* :setting:`LANGUAGE_COOKIE_SECURE`\n+* :setting:`LANGUAGES`\n+* :setting:`LANGUAGES_BIDI`\n+* :setting:`LOCALE_PATHS`\n+* :setting:`MONTH_DAY_FORMAT`\n+* :setting:`NUMBER_GROUPING`\n+* :setting:`SHORT_DATE_FORMAT`\n+* :setting:`SHORT_DATETIME_FORMAT`\n+* :setting:`THOUSAND_SEPARATOR`\n+* :setting:`TIME_FORMAT`\n+* :setting:`TIME_INPUT_FORMATS`\n+* :setting:`TIME_ZONE`\n+* :setting:`USE_I18N`\n+* :setting:`USE_THOUSAND_SEPARATOR`\n+* :setting:`USE_TZ`\n+* :setting:`YEAR_MONTH_FORMAT`\n+\n+HTTP\n+----\n+* :setting:`DATA_UPLOAD_MAX_MEMORY_SIZE`\n+* :setting:`DATA_UPLOAD_MAX_NUMBER_FIELDS`\n+* :setting:`DATA_UPLOAD_MAX_NUMBER_FILES`\n+* :setting:`DEFAULT_CHARSET`\n+* :setting:`DISALLOWED_USER_AGENTS`\n+* :setting:`FORCE_SCRIPT_NAME`\n+* :setting:`INTERNAL_IPS`\n+* :setting:`MIDDLEWARE`\n+* Security\n+\n+  * :setting:`SECURE_CONTENT_TYPE_NOSNIFF`\n+  * :setting:`SECURE_CROSS_ORIGIN_OPENER_POLICY`\n+  * :setting:`SECURE_HSTS_INCLUDE_SUBDOMAINS`\n+  * :setting:`SECURE_HSTS_PRELOAD`\n+  * :setting:`SECURE_HSTS_SECONDS`\n+  * :setting:`SECURE_PROXY_SSL_HEADER`\n+  * :setting:`SECURE_REDIRECT_EXEMPT`\n+  * :setting:`SECURE_REFERRER_POLICY`\n+  * :setting:`SECURE_SSL_HOST`\n+  * :setting:`SECURE_SSL_REDIRECT`\n+* :setting:`SIGNING_BACKEND`\n+* :setting:`USE_X_FORWARDED_HOST`\n+* :setting:`USE_X_FORWARDED_PORT`\n+* :setting:`WSGI_APPLICATION`\n+\n+Logging\n+-------\n+* :setting:`LOGGING`\n+* :setting:`LOGGING_CONFIG`\n+\n+Models\n+------\n+* :setting:`ABSOLUTE_URL_OVERRIDES`\n+* :setting:`FIXTURE_DIRS`\n+* :setting:`INSTALLED_APPS`\n+\n+Security\n+--------\n+* Cross Site Request Forgery Protection\n+\n+  * :setting:`CSRF_COOKIE_DOMAIN`\n+  * :setting:`CSRF_COOKIE_NAME`\n+  * :setting:`CSRF_COOKIE_PATH`\n+  * :setting:`CSRF_COOKIE_SAMESITE`\n+  * :setting:`CSRF_COOKIE_SECURE`\n+  * :setting:`CSRF_FAILURE_VIEW`\n+  * :setting:`CSRF_HEADER_NAME`\n+  * :setting:`CSRF_TRUSTED_ORIGINS`\n+  * :setting:`CSRF_USE_SESSIONS`\n+\n+* :setting:`SECRET_KEY`\n+* :setting:`SECRET_KEY_FALLBACKS`\n+* :setting:`X_FRAME_OPTIONS`\n+\n+Serialization\n+-------------\n+* :setting:`DEFAULT_CHARSET`\n+* :setting:`SERIALIZATION_MODULES`\n+\n+Templates\n+---------\n+* :setting:`TEMPLATES`\n+\n+Testing\n+-------\n+* Database: :setting:`TEST <DATABASE-TEST>`\n+* :setting:`TEST_NON_SERIALIZED_APPS`\n+* :setting:`TEST_RUNNER`\n+\n+URLs\n+----\n+* :setting:`APPEND_SLASH`\n+* :setting:`PREPEND_WWW`\n+* :setting:`ROOT_URLCONF`\n"
  },
  "psf__requests-1921": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "psf__requests-1921",
    "model_patch": ""
  },
  "sphinx-doc__sphinx-7590": {
    "model_name_or_path": "gpt-5-mini",
    "instance_id": "sphinx-doc__sphinx-7590",
    "model_patch": "diff --git a/sphinx/domains/cpp.py b/sphinx/domains/cpp.py\nindex fe52d881d..b747c3c4e 100644\n--- a/sphinx/domains/cpp.py\n+++ b/sphinx/domains/cpp.py\n@@ -882,6 +882,40 @@ class ASTCharLiteral(ASTLiteral):\n         signode.append(nodes.Text(txt, txt))\n \n \n+\n+class ASTUserDefinedLiteral(ASTLiteral):\n+    \"\"\"\n+    Represents a user-defined literal in the form:\n+      <literal><identifier-suffix>\n+    where there is no whitespace between literal and suffix.\n+    \"\"\"\n+    def __init__(self, literal: ASTLiteral, identifier: ASTIdentifier) -> None:\n+        self.literal = literal\n+        self.identifier = identifier\n+\n+    def _stringify(self, transform: StringifyTransform) -> str:\n+        return transform(self.literal) + transform(self.identifier)\n+\n+    def get_id(self, version: int) -> str:\n+        # For id purposes, encode as literal id concatenated with 'li' + suffix id\n+        # Older id schemes may not support this.\n+        try:\n+            lit_id = self.literal.get_id(version)\n+        except Exception:\n+            # fallback: stringify literal\n+            lit_id = str(self.literal)\n+        if version == 1:\n+            # no good mapping for UDLs in old scheme\n+            raise NoOldIdError()\n+        return lit_id + ('li' + self.identifier.get_id(version))\n+\n+    def describe_signature(self, signode: TextElement, mode: str,\n+                           env: \"BuildEnvironment\", symbol: \"Symbol\") -> None:\n+        # Render as the literal followed immediately by the identifier\n+        self.literal.describe_signature(signode, mode, env, symbol)\n+        signode.append(nodes.Text(str(self.identifier)))\n+\n+\n class ASTThisLiteral(ASTExpression):\n     def _stringify(self, transform: StringifyTransform) -> str:\n         return \"this\"\n@@ -4662,12 +4696,33 @@ class DefinitionParser(BaseParser):\n                       integer_literal_re, octal_literal_re]:\n             pos = self.pos\n             if self.match(regex):\n+                # consume builtin integer/float suffix characters (uUlLfF...)\n                 while self.current_char in 'uUlLfF':\n                     self.pos += 1\n+                # If an identifier immediately follows (no intervening space),\n+                # treat this as a user-defined literal suffix.\n+                id_pos = self.pos\n+                m = identifier_re.match(self.definition, id_pos)\n+                if m:\n+                    num_txt = self.definition[pos:self.pos]\n+                    ident_txt = m.group()\n+                    self.pos = m.end()\n+                    num_node = ASTNumberLiteral(num_txt)\n+                    ident_node = ASTIdentifier(ident_txt)\n+                    return ASTUserDefinedLiteral(num_node, ident_node)\n                 return ASTNumberLiteral(self.definition[pos:self.pos])\n \n         string = self._parse_string()\n         if string is not None:\n+            # Check for user-defined literal suffix attached to string literal\n+            id_pos = self.pos\n+            m = identifier_re.match(self.definition, id_pos)\n+            if m:\n+                ident_txt = m.group()\n+                self.pos = m.end()\n+                str_node = ASTStringLiteral(string)\n+                ident_node = ASTIdentifier(ident_txt)\n+                return ASTUserDefinedLiteral(str_node, ident_node)\n             return ASTStringLiteral(string)\n \n         # character-literal\n@@ -4675,6 +4730,15 @@ class DefinitionParser(BaseParser):\n             prefix = self.last_match.group(1)  # may be None when no prefix\n             data = self.last_match.group(2)\n             try:\n+                # After a char literal, a UDL suffix may follow directly\n+                id_pos = self.pos\n+                m = identifier_re.match(self.definition, id_pos)\n+                if m:\n+                    ident_txt = m.group()\n+                    self.pos = m.end()\n+                    char_node = ASTCharLiteral(prefix, data)\n+                    ident_node = ASTIdentifier(ident_txt)\n+                    return ASTUserDefinedLiteral(char_node, ident_node)\n                 return ASTCharLiteral(prefix, data)\n             except UnicodeDecodeError as e:\n                 self.fail(\"Can not handle character literal. Internal error was: %s\" % e)\n"
  }
}